{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiRPSHY0sCQi"
      },
      "source": [
        "#**Language Modeling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QdB9C9msIw8"
      },
      "source": [
        "Language Modeling aims at defining a probability distribution for the different logical units of a sentence. \n",
        "\n",
        "It assigns a conditional probability to each single word, providing a distribution over the whole vocabulary.\n",
        "\n",
        "**$P(w_{1:n})=\\prod_{i=1}^{n}P(w_i|w_{<i})$**\n",
        "\n",
        "There are two classic approaches for estimating these probabilities, one based on a statistical analysis and the other one on neural models.\n",
        "\n",
        "In this project we take into account the second one, and specifically we deal with recurrent neural networks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ghay9pfTvsv8"
      },
      "source": [
        "#**Preparation of the data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1gV4EZdvuul"
      },
      "source": [
        "We start by importing the different libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBXILJXgTNjs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.utils.data as data\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import pickle\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import math\n",
        "import timeit\n",
        "import warnings\n",
        "import rarfile\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils import weight_norm\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from collections import Counter\n",
        "from scipy.stats import pearsonr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhWbQWIHv1NL"
      },
      "source": [
        "We set the device we are going to use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjPDtcFUTRBl"
      },
      "outputs": [],
      "source": [
        "device = 'cuda:0'\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "PAD_TOKEN = 0\n",
        "save='model.tar'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNo42SY-v7P5"
      },
      "source": [
        "We mount the drive so that to access to our dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZCkPrggU8As",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3316465-1d64-4054-ceb5-699594b99ef7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rar_file = rarfile.RarFile(\"/content/gdrive/MyDrive/dataset/ptbdataset.rar\")\n",
        "rar_file.extractall(\"/content/gdrive/MyDrive/dataset/\")\n",
        "fold_path = \"/content/gdrive/MyDrive/dataset/ptbdataset/\""
      ],
      "metadata": {
        "id": "sXq4pP7KHLwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We set the general parameters of the model, valid for both the regularized and the non-regularized model"
      ],
      "metadata": {
        "id": "YeZG5UlZMnNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_txt=str(\"PTB\")\n",
        "save=str(\"model.tar\")\n",
        "bptt=60\n",
        "epochs=100\n",
        "lr_sgd=0.1\n",
        "log=100"
      ],
      "metadata": {
        "id": "J3bilPYJsMX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nruV5c7cg6-j"
      },
      "source": [
        "#**Data pre-processing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyQm6Jebv9q6"
      },
      "source": [
        "**Dataset**\n",
        "\n",
        "PTB dataset was developed by researchers at the University of Pennsylvania and consists of approximately $4.5$ million words of newspaper and magazine articles from the Wall Street Journal and Associated Press. It is heavily pre-processed, in that it has no capital letters, numbers (substituted with $N$) or various punctuation. It also contains the token \"unk\", standing for the large number of out of vocabulary tokens which results for the relatively reduced size of the vocabulary, equal to $10 000$ words."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def list_sents(path):\n",
        "    \"\"\"\n",
        "    Returns a list of sentences and words from the input text file.\n",
        "    Each sentence is ending with the special token <eos>.\n",
        "    The returned list of words also includes the <eos> token.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    path : str\n",
        "        The path to the input text file.\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    sents : list of list of str\n",
        "        A list of sentences, each one ending with <eos>.\n",
        "    words : list of str\n",
        "        A list of words, including the <eos> token.\n",
        "    \"\"\"\n",
        "    sents = []\n",
        "    words = []\n",
        "    with open(path) as file:\n",
        "        for line in file:\n",
        "          sent = line.split()\n",
        "          sent.insert(len(sent), '<eos>')\n",
        "          sents.append(sent)\n",
        "          for word in line.split():\n",
        "            words.append(word)\n",
        "          words.append('<eos>')\n",
        "    file.close()\n",
        "    return sents, words\n"
      ],
      "metadata": {
        "id": "RGKZecmfv6cV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script is loading data from three different text files, each text file contains sentences which are splitted into words. A vocabulary of unique words is then created."
      ],
      "metadata": {
        "id": "GDzCKjjwr2xW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentences and words in the training set \n",
        "train_data, train_words_list = list_sents(fold_path+\"ptb.train.txt\")\n",
        "train_words = set(train_words_list)\n",
        "\n",
        "# Sentences and words in the test set \n",
        "test_data, test_words_list = list_sents(fold_path+\"ptb.test.txt\")\n",
        "test_words = set(test_words_list)\n",
        "\n",
        "# Sentences and words in the validation set \n",
        "valid_data, valid_words_list = list_sents(fold_path+\"ptb.valid.txt\")\n",
        "valid_words = set(valid_words_list)"
      ],
      "metadata": {
        "id": "l2idR-4Vv0eC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The frequency of the lengths of the sentences is visualized using bar plots for training, validation and test sets."
      ],
      "metadata": {
        "id": "oww1Cu-z-Axq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Senteces length in training set \n",
        "n_words_train = len(max(train_data, key=len))\n",
        "lengths = list(range(1, n_words_train+1))\n",
        "frequencies = [0 for i in lengths]\n",
        "for sent in train_data:\n",
        "  frequencies[len(sent)-1] += 1\n",
        "\n",
        "print('Number of unit lenght sentences in the training set: {:d}'.format(frequencies[1]))\n",
        "\n",
        "pos = np.arange(1.75, n_words_train+3, 5)\n",
        "lens = list(range(1, n_words_train+3, 5))\n",
        "\n",
        "ax = plt.axes()\n",
        "ax.set_xticks(pos)\n",
        "ax.set_xticklabels(lens)\n",
        "ax.set_xlabel(r'Sentence length')\n",
        "ax.set_ylabel(r'Frequence in the training set')\n",
        "\n",
        "plt.bar(lengths, frequencies, 0.5,color='y')\n",
        "plt.show()\n",
        "\n",
        "# Senteces length in test set\n",
        "n_words_test = len(max(test_data, key=len))\n",
        "lengths = list(range(1, n_words_test+1))\n",
        "frequencies = [0 for i in lengths]\n",
        "for sent in test_data:\n",
        "  frequencies[len(sent)-1] += 1\n",
        "\n",
        "pos = np.arange(1.75, n_words_test+3, 5)\n",
        "lens = list(range(1, n_words_test+3, 5))\n",
        "\n",
        "ax = plt.axes()\n",
        "ax.set_xticks(pos)\n",
        "ax.set_xticklabels(lens)\n",
        "ax.set_xlabel(r'Sentence length')\n",
        "ax.set_ylabel(r'Frequence in the test set')\n",
        "\n",
        "plt.bar(lengths, frequencies, 0.5, color='tab:pink')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Senteces length in validation set \n",
        "n_words_valid = len(max(valid_data, key=len))\n",
        "lengths = list(range(1, n_words_valid+1))\n",
        "frequencies = [0 for i in lengths]\n",
        "for sent in valid_data:\n",
        "  frequencies[len(sent)-1] += 1\n",
        "\n",
        "pos = np.arange(1.75, n_words_valid+3, 5)\n",
        "lens = list(range(1, n_words_valid+3, 5))\n",
        "\n",
        "ax = plt.axes()\n",
        "ax.set_xticks(pos)\n",
        "ax.set_xticklabels(lens)\n",
        "ax.set_xlabel(r'Sentence length')\n",
        "ax.set_ylabel(r'Frequence in the validation set')\n",
        "\n",
        "plt.bar(lengths, frequencies, 0.5, color='tab:orange')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 820
        },
        "id": "jvKy7bNRoaZB",
        "outputId": "ec66d721-0a72-44d2-84e0-4b64ee5b8583"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unit lenght sentences in the training set: 137\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The class called Vocab is used to create a vocabulary of words from a list of words. The class has two methods, the init() method, and the w2id() method.\n",
        "\n",
        "When an object of the class is created, it takes a list of wordsand an optional argument, cutoff, as inputs. The init() method calls the w2id() method which aims at creating an unique corrispondence between words and integer numbers. There is also the inverse mapping method.\n",
        "\n",
        "Words that appear less than cutoff times are discarded."
      ],
      "metadata": {
        "id": "w2dYL0OQtIGM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4c04f608"
      },
      "outputs": [],
      "source": [
        "class Vocab():\n",
        "    def __init__(self, words, cutoff=0):\n",
        "        self.word2id = self.w2id(words, cutoff=cutoff)\n",
        "        self.id2word = {v:k for k, v in self.word2id.items()}\n",
        "        \n",
        "    def w2id(self, elements, cutoff=None):\n",
        "        \"\"\"\n",
        "        This function creates a vocabulary of words from a list of words,\n",
        "        and assigns an id to each word. Words that appear less than 'cutoff' \n",
        "        times are discarded.\n",
        "        \"\"\"\n",
        "        vocab = {'pad': PAD_TOKEN, '<eos>': 1}\n",
        "        count = Counter(elements)\n",
        "        for k, v in count.items():\n",
        "          if v > cutoff:\n",
        "              if k not in vocab:\n",
        "                vocab[k] = len(vocab)\n",
        "        return vocab\n",
        "\n",
        "vocab = Vocab(train_words_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines a Pytorch Dataset class called SentsDataset, which is designed to load and manipulate the sentences of the datasets. It takes two arguments, the dataset (a list of sentences) and the vocab (a Vocab object containing the vocabulary of the dataset). The class has three methods init, len, and getitem. The class is then used to create three datasets: train_dataset, valid_dataset, and test_dataset, by passing the corresponding train_data, valid_data, and test_data, and vocab to the class."
      ],
      "metadata": {
        "id": "cxfDCdq7uUZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SentsDataset(data.Dataset):\n",
        "    \"\"\"\n",
        "    A Pytorch Dataset class for loading and manipulating sentence datasets.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    dataset : list\n",
        "        A list of sentences\n",
        "    vocab : Vocab\n",
        "        A Vocab object containing the vocabulary of the dataset\n",
        "        \n",
        "    \"\"\"\n",
        "    def __init__(self, dataset, vocab):\n",
        "        \"\"\"\n",
        "        Initialize the SentsDataset class\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        dataset : list\n",
        "            A list of sentences\n",
        "        vocab : Vocab\n",
        "            A Vocab object containing the vocabulary of the dataset\n",
        "        \n",
        "        \"\"\"\n",
        "        self.dataset = dataset\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Return the number of sentences in the dataset\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        int\n",
        "            The number of sentences in the dataset\n",
        "        \n",
        "        \"\"\"\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Return the idx-th sentence in the dataset\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        idx : int\n",
        "            Index of the sentence to be returned\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            A tensor containing the id's of the words in the sentence\n",
        "        \n",
        "        \"\"\"\n",
        "        sent = [self.vocab.word2id[word] for word in self.dataset[idx]]\n",
        "        return torch.Tensor(sent)\n",
        "\n",
        "# Training dataset\n",
        "train_dataset = SentsDataset(train_data, vocab)\n",
        "\n",
        "# Validation dataset\n",
        "valid_dataset = SentsDataset(valid_data, vocab)\n",
        "\n",
        "# Test dataset\n",
        "test_dataset = SentsDataset(test_data, vocab)"
      ],
      "metadata": {
        "id": "HAYf3prdM0kK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collate_fn is used to merge a batch of sentences into a single tensor.\n",
        "\n",
        "The function first sorts the sentences in descending order of length, then it splits them into two lists, one for input and one for target. The input list contains all the words except the last one and on the other hand the target list contains all words except the first.\n",
        "\n",
        "Merge() function is used to take a list of sequences as input and returns a tuple composed by the padded sequences, a tensor of shape (batch_size, max_len), and a list of the lengths of the original sequences.\n",
        "\n",
        "A matrix full of PAD_TOKEN with the shape batch_size x maximum length of a sequence is created.\n",
        "\n",
        "The scope is to pad the sequences and create fixed-length inputs and targets, so that all of them will have the same length within a certain batch."
      ],
      "metadata": {
        "id": "T7MBfE0SuZYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(data):\n",
        "    \n",
        "    \"\"\"\n",
        "    A function to merge a batch of sentences into a single tensor.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    data : list\n",
        "        A list of sentences\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    tuple\n",
        "        A tuple containing:\n",
        "            - input_pad : torch.Tensor\n",
        "                A tensor containing the padded inputs of the sentences.\n",
        "            - target_pad : torch.Tensor\n",
        "                A tensor containing the padded targets of the sentences.\n",
        "            - y_lengths : torch.Tensor\n",
        "                A tensor containing the lengths of the targets.\n",
        "    \"\"\"\n",
        "    \n",
        "    def merge(sequences):\n",
        "       \n",
        "        \"\"\"\n",
        "        Merge a list of sequences into a single tensor.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        sequences : list\n",
        "            A list of sequences\n",
        "            \n",
        "        Returns\n",
        "        -------\n",
        "        tuple\n",
        "            A tuple containing:\n",
        "                - padded_seqs : torch.Tensor\n",
        "                    A tensor of shape (batch_size, max_len) containing the padded sequences\n",
        "                - lengths : list\n",
        "                    A list of the lengths of the original sequences\n",
        "        \"\"\"\n",
        "        lengths = [len(seq) for seq in sequences]\n",
        "        max_len = 1 if max(lengths)==0 else max(lengths)\n",
        "        \n",
        "        # Pad token is zero in our case\n",
        "        # So we create a matrix full of PAD_TOKEN (i.e. 0) with the shape \n",
        "        # batch_size X maximum length of a sequence\n",
        "        padded_seqs = torch.LongTensor(len(sequences),max_len).fill_(PAD_TOKEN)\n",
        "        for i, seq in enumerate(sequences):\n",
        "            end = lengths[i]\n",
        "           \n",
        "            # We copy each sequence into the matrix\n",
        "            padded_seqs[i, :end] = seq\n",
        "       \n",
        "        # We remove these tensors from the computational graph\n",
        "        padded_seqs = padded_seqs.detach()\n",
        "        return padded_seqs, lengths\n",
        "\n",
        "    # Sort data by seq lengths\n",
        "    data.sort(key=lambda x: len(x), reverse=True) \n",
        "\n",
        "    # Input sequences\n",
        "    input = [s[:-1] for s in data]\n",
        "   \n",
        "    # Target sequences\n",
        "    target = [s[1:] for s in data]\n",
        "    input_pad, _ = merge(input)\n",
        "    target_pad, y_lengths = merge(target)\n",
        "\n",
        "    y_lengths = torch.LongTensor(y_lengths).to(device)\n",
        "    \n",
        "    return (input_pad, target_pad, y_lengths)"
      ],
      "metadata": {
        "id": "obbF5mNQ7ePW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here three Pytorch DataLoader objects are created, respectively for the training, validation and test data. \n",
        "\n",
        "The DataLoader class is used to load a dataset in batches. It takes several arguments, such as the dataset, the batch size, the collate function and the shuffle flag. The collate function is used to merge a batch of sentences into a single tensor.\n",
        "\n",
        "The first line sets the batch size to 64, which means that the DataLoader will return 64 sentences at a time.\n",
        "\n",
        "The next three lines create the training, validation and test DataLoader objects by passing the corresponding dataset, the batch size, the collate function and the shuffle flag to the DataLoader class.\n",
        "\n",
        "The training dataloader uses the collate_fn function to collate the data and shuffle the data, so that the model sees different sentences in each iteration. The validation and test dataloader uses the collate_fn function to collate the data but do not shuffle the data, so that the validation and test data are processed in the order they are given.\n",
        "\n",
        "These DataLoader objects will be used to train, validate and test the model, they will be used to iterate over the data and feed it to the model one batch at a time."
      ],
      "metadata": {
        "id": "M9ytc5trwFwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definition of the dataloaders\n",
        "batch_size = 64\n",
        "\n",
        "# Training dataloader\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn,  shuffle=True)\n",
        "\n",
        "\n",
        "# Validation dataloader\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
        "\n",
        "# Test dataloader\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "7GhqP_EtfpfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cig-V_0yhE5I"
      },
      "source": [
        "#**Functions valid both for the regularized and the no-regularized model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZdVAyP7SF6Y"
      },
      "source": [
        "##**Weights initialization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvZMn_z5Yts0"
      },
      "source": [
        "The code defines a function called init_weights() which takes a Pytorch model as input. The function loops over all the modules in the model and checks their types. If the module is an LSTM, it loops over all the parameters of the LSTM and assigns different initialization methods to them based on the parameter name. If the parameter name contains weight_ih, it initializes the parameter with the Xavier initialization method. If the parameter name contains weight_hh, it initializes the parameter with the Orthogonal initialization method. If the parameter name contains 'bias', it initializes the parameter with zero.\n",
        "\n",
        "If the module is not an LSTM, the function checks if it is a Linear or Embedding layer, if it is, it initializes the weights with the uniform initialization method and the biases with zero. Finally, if the module is an Embedding layer, the function also initializes the weights with the uniform initialization method.\n",
        "\n",
        "The function is used to initialize the weights of a Pytorch model, it is important to initialize the weights of a neural network model before training it, because if the weights are not initialized properly the model may not converge or may take a long time to converge."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(mat):\n",
        "    \n",
        "    \"\"\"\n",
        "    A function to initialize the weights of a Pytorch model\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    mat : nn.Module\n",
        "        A Pytorch model\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "    \"\"\"\n",
        "    \n",
        "    for m in mat.modules():\n",
        "        if type(m) in [nn.LSTM]:\n",
        "            for name, param in m.named_parameters():\n",
        "                \n",
        "                # Xavier initialization for input weights\n",
        "                if 'weight_ih' in name:\n",
        "                    for idx in range(4):\n",
        "                        mul = param.shape[0]//4\n",
        "                        torch.nn.init.xavier_uniform_(param[idx*mul:(idx+1)*mul])\n",
        "               \n",
        "                # Orthogonal initialization for recurrent weights\n",
        "                elif 'weight_hh' in name:\n",
        "                    for idx in range(4):\n",
        "                        mul = param.shape[0]//4\n",
        "                        torch.nn.init.orthogonal_(param[idx*mul:(idx+1)*mul])\n",
        "                \n",
        "                # Zero initialization for biases\n",
        "                elif 'bias' in name:\n",
        "                    param.data.fill_(0.00)\n",
        "        else:\n",
        "            \n",
        "            # Uniform initialization for Linear and Embedding layers\n",
        "            if type(m) in [nn.Linear]:\n",
        "                torch.nn.init.uniform_(m.weight, -0.1, 0.1)\n",
        "               \n",
        "                # Zero initialization for biases\n",
        "                if m.bias != None:\n",
        "                    m.bias.data.fill_(0.01)   \n",
        "            if type(m) in [nn.Embedding]:\n",
        "                torch.nn.init.uniform_(m.weight, -0.1, 0.1)"
      ],
      "metadata": {
        "id": "18uY9IuksyZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8r4ON97ilHJ"
      },
      "source": [
        "##**Optimizers (for the regularized version)**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The traditional SGD has been replaced with a slightly different version of it: Non-monotone Target-Adaptive Stochastic Gradient Descent (NT-ASGD). It is designed specifically to improve the convergence rate of SGD in non-convex optimization problems ad introduces a target update rule that adaptively adjusts the learning rate based on the performance of the current iteration compared to the previous iteration. Taking the average is a method which helps the stabilization of the training and improves the final performance of the model. To ensure that the averaging process does not significantly influence the stability of the neural network weights, it is important to perform averaging at a point during training when the weights have reached a consistent value. One strategy for determining when to average the weights is to monitor the model's performance on a validation set. When the validation metric fails to improve for multiple cycles, averaging can be initiated. This technique requires the use of two additional hyperparameters: L, which represents the logging interval (typically set to the number of iterations in an epoch), and n, a non-monotone interval (commonly set to 5). These hyperparameters are used to help ensure that the randomness of the training process does not significantly impact the decision to average the weights."
      ],
      "metadata": {
        "id": "UvErxxJ-sP4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NTASGD(optim.Optimizer):\n",
        "    \n",
        "    \"\"\"\n",
        "    A custom Pytorch optimizer called NTASGD (non-monotonically triggered adaptive SGD)\n",
        "    that inherits from the built-in Pytorch optimizer class optim.Optimizer.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, params, lr=1, n=5, weight_decay=0, fine_tuning=False):\n",
        "    \n",
        "        \"\"\"\n",
        "        Initializes the optimizer with the given parameters.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        params : list of torch.Tensor\n",
        "            The parameters of the model to optimize\n",
        "        lr : float, optional\n",
        "            The learning rate of the optimizer, by default 1\n",
        "        n : int, optional\n",
        "            The number of previous steps to consider when checking the non-monotonic condition, by default 5\n",
        "        weight_decay : float, optional\n",
        "            The weight decay of the optimizer, by default 0\n",
        "        fine_tuning : bool, optional\n",
        "            Whether the optimizer is in fine-tuning mode or not, by default False\n",
        "        \"\"\"\n",
        "    \n",
        "        # Set the default values for the hyperparameters\n",
        "        self.defaults = dict(\n",
        "            lr=lr, \n",
        "            n=n,  \n",
        "            weight_decay=weight_decay,  \n",
        "            fine_tuning=fine_tuning,  \n",
        "            t0=10e7 if not fine_tuning else 0,  \n",
        "            t=0,  \n",
        "            logs=[]  \n",
        "        )\n",
        "\n",
        "        # Initialize the optimizer\n",
        "        super().__init__(params, self.defaults)\n",
        "\n",
        "\n",
        "    # Check if the current iteration satisfies the non-monotonic condition.\n",
        "    def check(self, v):\n",
        "     \n",
        "        \"\"\"\n",
        "        Check if the current iteration satisfies the non-monotonic condition.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        v : float\n",
        "            The current iteration's value\n",
        "            \n",
        "        Returns\n",
        "        -------\n",
        "        bool\n",
        "            True if the non-monotonic condition is triggered, False otherwise\n",
        "        \"\"\"\n",
        "     \n",
        "        for group in self.param_groups:\n",
        "            \n",
        "            # Check if the current group is in fine-tuning mode or if it is not in fine-tuning mode\n",
        "            # and the non-monotonic condition has not been triggered yet\n",
        "            if group[\"fine_tuning\"] or (not group[\"fine_tuning\"] and group[\"t0\"] == 10e7):\n",
        "            \n",
        "                # Check if the current iteration is past the n-th iteration and if the current value is\n",
        "                # greater than the minimum of the last n logs\n",
        "                if group[\"t\"] > group[\"n\"] and v > min(group[\"logs\"][:-group[\"n\"]]):\n",
        "                \n",
        "                    group[\"t0\"] = self.state[next(iter(group[\"params\"]))][\"step\"]\n",
        "                \n",
        "                    print(\"Non-monotonic condition is triggered!\")\n",
        "                \n",
        "                    return True\n",
        "                \n",
        "                group[\"logs\"].append(v)\n",
        "                \n",
        "                group[\"t\"] += 1\n",
        "\n",
        "    # Set the learning rate for the optimizer\n",
        "    def lr(self, lr):\n",
        "   \n",
        "        \"\"\"\n",
        "        Set the learning rate for the optimizer.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        lr : float\n",
        "            The new learning rate\n",
        "        \"\"\"\n",
        "      \n",
        "        for group in self.param_groups:\n",
        "            group[\"lr\"] = lr\n",
        "\n",
        "\n",
        "    # Perform a single optimization step\n",
        "    def step(self):\n",
        "     \n",
        "        \"\"\"\n",
        "        Perform a single optimization step for all parameters in the groups. \n",
        "        \"\"\"\n",
        "\n",
        "        for group in self.param_groups:\n",
        "\n",
        "            for p in group[\"params\"]:\n",
        "\n",
        "                # Get the gradient of the parameter\n",
        "                grad = p.grad.data\n",
        "\n",
        "                # Get the state of the parameter\n",
        "                state = self.state[p]\n",
        "\n",
        "                # Initialize the state if it is empty\n",
        "                if len(state) == 0:\n",
        "\n",
        "                    state[\"step\"] = 0\n",
        "\n",
        "                    state[\"mu\"] = 1\n",
        "\n",
        "                    state[\"ax\"] = torch.zeros_like(p.data)\n",
        "\n",
        "                # Increment the step counter\n",
        "                state[\"step\"] += 1\n",
        "\n",
        "                # Apply weight decay if specified\n",
        "                if group[\"weight_decay\"] != 0:\n",
        "\n",
        "                    grad = grad.add(group[\"weight_decay\"], p.data)\n",
        "\n",
        "                # Update the parameter\n",
        "                p.data.add_(-group[\"lr\"], grad)\n",
        "\n",
        "                # Update the moving average parameter\n",
        "                if state[\"mu\"] != 1:\n",
        "\n",
        "                    state[\"ax\"].add_(p.data.sub(state[\"ax\"]).mul(state[\"mu\"]))\n",
        "\n",
        "                else:\n",
        "\n",
        "                    state[\"ax\"].copy_(p.data)\n",
        "\n",
        "                state[\"mu\"] = 1 / max(1, state[\"step\"] - group[\"t0\"])"
      ],
      "metadata": {
        "id": "gzrU0roprc-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWL5HvHlRuJu"
      },
      "source": [
        "#**Neural Network, baseline**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmVh4Rh96iYP"
      },
      "source": [
        "In this section the baseline of the LSTM is taken into account.\n",
        "\n",
        "LSTM serves as a fundamental building block for many model languaging tasks.\n",
        "\n",
        "The specific model we have chosen is composed of three hidden layers.\n",
        "\n",
        "In init we can find the instantiation of the layers used, while in forward we have the achitecture of the neural \n",
        "network.\n",
        "\n",
        "Let's recall the mathematical formulation of the LSTM\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "*   $i_t = \\sigma(W^ix_t +U^ih_{t-1})$\n",
        "*   $ft = \\sigma(W^fx_t +U^fh_{t-1})$\n",
        "*   $ot =\\sigma(W^ox_t +U^oh_{t-1})$\n",
        "*   $c_t = tanh(W^cx_t + U^ch_{t-1})$\n",
        "*   $c_t=i_t \\circ  {c}_t + f_t \\circ +  {c}_{t-1}$\n",
        "*   $h_t = o_t \\circ tanh(c_t)$\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brYG6uu3YT7S"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRQn6sfKYb67"
      },
      "source": [
        "where $[W^i,W^f,W^o, U^i, U^f, U^o]$ are weight matrices, $x_t$ is the vector input to the timestep $t$, $h_t$ is the current exposed hidden state, $c_t$ is the memory cell state, and $\\circ$ is element-wise multiplication.\n",
        "\n",
        "Word embedding is a term used for the representation of words for text analysis. Typically it is in the form of real-valued vectors which encode the meaning of the word such that the words that are closer in the vector space are expected to be similar in meaning.\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First let's set some parameters of the non-regularized model"
      ],
      "metadata": {
        "id": "5yC3PaRRpw7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layer_num_reg=1\n",
        "embed_size_reg=400\n",
        "hidden_size_reg=400\n",
        "weight_decay=1.2e-6\n",
        "lr_reg=0.001\n",
        "max_grad_norm=0.25\n",
        "log=float(100)\n",
        "winit = 0.1\n",
        "vocab_size = len(vocab.word2id)"
      ],
      "metadata": {
        "id": "Q53fRhgT5lOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines a class called BaseModel that inherits from PyTorch's nn.Module class. The class has an init method that takes in several parameters, including the size of the vocabulary, the size of the embeddings, the size of the hidden layer, and the number of layers in the LSTM. It also has an optional parameter for the index of the padding token in the vocabulary.\n",
        "\n",
        "The class initializes several layers in the init method, including an embedding layer using PyTorch's nn.Embedding class, an LSTM layer using PyTorch's nn.LSTM class, and a linear output layer using PyTorch's nn.Linear class.\n",
        "\n",
        "The class also has a forward method that takes in inputs x and seq_lengths and applies the initialized layers to them. The method starts by passing the input x through the embedding layer, permutes the dimensions, and packs the sequence using pack_padded_sequence.\n",
        "\n",
        "Then it passes the packed sequence through the LSTM layer and gets the output. After that, it pads the output sequence and applies the linear output layer and returns the scores."
      ],
      "metadata": {
        "id": "gc8-taujqVCu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJbtlJXA1_hp"
      },
      "outputs": [],
      "source": [
        "class BaseModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, layer_num, pad_index = PAD_TOKEN):\n",
        "        super().__init__()\n",
        "\n",
        "        \"\"\"\n",
        "        Initialize the BaseModel with the given parameters\n",
        "        Parameters:\n",
        "        vocab_size (int): size of the vocabulary\n",
        "        embed_size (int): size of the embeddings\n",
        "        hidden_size (int): size of the hidden layer\n",
        "        layer_num (int): number of layers in the LSTM\n",
        "        pad_index (int): index of the padding token in the vocabulary (default: PAD_TOKEN)\n",
        "        \"\"\"\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embed_size = embed_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.layer_num = layer_num\n",
        "        self.pad_index = pad_index\n",
        "        \n",
        "        # Initialize the embedding layer with the given parameters\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size, padding_idx=pad_index)\n",
        "        \n",
        "        self.rnns = nn.LSTM(embed_size, hidden_size, num_layers = layer_num, bidirectional=False)\n",
        "        \n",
        "        # Decoder layer: linear output layer\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, seq_lengths):\n",
        "        \n",
        "        x_emb = self.embed(x)\n",
        "        x_emb = x_emb.permute(1,0,2)\n",
        "\n",
        "        packed_x = pack_padded_sequence(x_emb, seq_lengths.cpu().numpy())\n",
        "\n",
        "        packed_y, _ = self.rnns(packed_x)\n",
        "\n",
        "        padded_y, _ = pad_packed_sequence(packed_y)\n",
        "\n",
        "        scores = self.fc(padded_y)\n",
        "\n",
        "        scores = scores.permute(1,0,2).contiguous()\n",
        "\n",
        "        scores = scores.view(-1, scores.shape[-1])\n",
        "\n",
        "        return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here the model is instantiated and both the optimizer and the loss criterion are set."
      ],
      "metadata": {
        "id": "fv3RbpGHeuSo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c144536-d25e-41e6-dc4a-2c26c71c028a",
        "id": "_x4geNNgydRR"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BaseModel(\n",
            "  (embed): Embedding(10001, 400, padding_idx=0)\n",
            "  (rnns): LSTM(400, 400)\n",
            "  (fc): Linear(in_features=400, out_features=10001, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Instantiation of model\n",
        "vocab_size = len(vocab.word2id)\n",
        "model = BaseModel(vocab_size, embed_size_reg, hidden_size_reg, layer_num_reg, pad_index=PAD_TOKEN).to(device)\n",
        "model.apply(init_weights)\n",
        "\n",
        "# Optimizers          \n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr_reg)\n",
        "\n",
        "# Loss criterion\n",
        "loss_function = nn.CrossEntropyLoss(ignore_index = PAD_TOKEN, reduction = 'sum')\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Launch Baseline Model**"
      ],
      "metadata": {
        "id": "-1W6D-lSqq3F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BaseTrain is used to train the model on the provided dataset using the SGD optimizer and CE loss function."
      ],
      "metadata": {
        "id": "3EvI_DoqevVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def BaseTrain(data, model, optimizer, loss_predict, tic):\n",
        "    \n",
        "    # Set the training data to a local variable\n",
        "    trn = data\n",
        "    \n",
        "    try:\n",
        "        # Calculate the sequence length and number of batches\n",
        "        num_batch = len(trn.dataset)// batch_size -1\n",
        "        \n",
        "        # Set the model to training mode\n",
        "        model.train()\n",
        "\n",
        "        # Cumulative loss values obtained for each batch\n",
        "        loss_sum_array = []\n",
        "        \n",
        "        # Total number of words in the batch\n",
        "        num_words_array = []\n",
        "\n",
        "        # Iterate over the training set\n",
        "        for i, (x, y, length) in enumerate(data):\n",
        "          if x.size(0) == batch_size:\n",
        "            \n",
        "            # Move the input and target data to the specified device\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            \n",
        "            # Run the input data and the hidden states through the model\n",
        "            scores = model(x, length)\n",
        "\n",
        "            # Zero the gradients of the optimizer\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Calculate the cross-entropy loss between the predicted scores and the target data      \n",
        "            loss = loss_predict(scores, y.reshape(-1))\n",
        "            loss_sum_array.append(loss.item())\n",
        "            num_words_array.append((torch.sum(length)).item())\n",
        "                        \n",
        "            # Calculate the gradients of the loss with respect to the model's parameters\n",
        "            loss.backward()\n",
        "            \n",
        "            # Perform a single optimization step\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Print training statistics every \"log\" number of batches\n",
        "            if i % (log) == 0:\n",
        "                \n",
        "                # Calculate the elapsed time\n",
        "                toc = timeit.default_timer()\n",
        "                \n",
        "                # Print the training statistics\n",
        "                print(\"batch no = {:d} / {:d}, \".format(i, num_batch) +\n",
        "                      \"train loss = {:.3f}, \".format(loss.item()) +\n",
        "                      \"wps = {:d}, \".format(round(num_words_array[-1]/(toc-tic))) +\n",
        "                      \"since beginning = {:d} mins, \".format(round((toc-tic)/60)) +\n",
        "                      \"cuda memory = {:.3f} GBs\".format(torch.cuda.max_memory_allocated()/1024/1024/1024))\n",
        "    \n",
        "    except KeyboardInterrupt:\n",
        "        \n",
        "        # Print a message if the training is interrupted early\n",
        "        print(\"Finishing training early.\")\n",
        "        \n",
        "    # Return the trained model and the total number of words seen in the epoch\n",
        "    return model, num_words_array, loss_sum_array"
      ],
      "metadata": {
        "id": "ZN-L3TWL83du"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The BaseEvaluate function evaluates the performance of the model on the given dataset. It calculates the loss and returns the results."
      ],
      "metadata": {
        "id": "lGCudm25ewsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def BaseEvaluate(data_loader, loss_predict, model):\n",
        "    \n",
        "    # set the model to evaluation mode\n",
        "    model.eval()\n",
        "    \n",
        "    # Cumulative loss values obtained for each batch\n",
        "    lossess = []\n",
        "        \n",
        "    # Total number of words in the batch\n",
        "    num_words_array = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        # Iterate over the test set\n",
        "        for i, (x, y, lengths) in enumerate(data_loader):\n",
        "          if x.size(0) == batch_size:\n",
        "            # move the minibatch to the device\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "    \n",
        "            scores = model(x, lengths)\n",
        "\n",
        "            loss = loss_predict(scores, y.reshape(-1))\n",
        "\n",
        "            # Update of the loss array\n",
        "            lossess.append(loss.data.item())\n",
        "            num_words_array.append((torch.sum(lengths)).item())\n",
        "    \n",
        "    return lossess, num_words_array"
      ],
      "metadata": {
        "id": "cWkHCbFW-GIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model ran for fifty epochs and we can see the values obtained in each epoch."
      ],
      "metadata": {
        "id": "vT2DLn5ZDwLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the best validation perplexity to a large value\n",
        "best_val = 1e10\n",
        "epochs = 50\n",
        "\n",
        "# Vectors filled during training that allow to plot the loss variations across epochs\n",
        "loss_BaseTrain = []\n",
        "loss_BaseValid = []\n",
        "sampled_BaseEpochs = []\n",
        "\n",
        "# Start a timer\n",
        "tic = timeit.default_timer() \n",
        "\n",
        "# Loop over the number of epochs\n",
        "for epoch in range(epochs):  \n",
        "\n",
        "    print(\"Epoch : {:d}\".format(epoch+1))\n",
        "\n",
        "    # Train the model on the training data and update the total number of words processed\n",
        "    model, words_train, loss_train = BaseTrain(train_loader, model, optimizer, loss_function, tic)\n",
        "    \n",
        "    # Cross entropy\n",
        "    ce_train = np.asarray(loss_train).sum() / np.asarray(words_train).sum()\n",
        "    print('CE on the training set: ', ce_train, '\\n')\n",
        "\n",
        "    # Perplexity\n",
        "    perplexity_train = np.exp(ce_train)\n",
        "    print('Perplexity on the training set: ', perplexity_train, '\\n')\n",
        "\n",
        "    loss_BaseTrain.append(ce_train)\n",
        "    sampled_BaseEpochs.append(epoch)\n",
        "    loss_valid, words_valid = BaseEvaluate(valid_loader, loss_function, model)\n",
        "    \n",
        "    # Cross entropy\n",
        "    ce_valid = np.asarray(loss_valid).sum() / np.asarray(words_valid).sum()\n",
        "    print('CE on the validation set: ', ce_valid, '\\n')\n",
        "\n",
        "    # Perplexity\n",
        "    perplexity_valid = np.exp(ce_valid)\n",
        "    print('Perplexity on the validation set: ', perplexity_valid, '\\n')\n",
        "\n",
        "    loss_BaseValid.append(ce_valid)\n",
        "    \n",
        "    if ce_valid < best_val:\n",
        " \n",
        "       # Update the best validation perplexity\n",
        "        best_val = ce_valid \n",
        "\n",
        "        print(\"CE on the validation set: {:.3f}\".format(best_val))\n",
        "\n",
        "        # Save the model (solo per verificare il calcolo degli errori)\n",
        "        torch.save({'model_state_dict': model.state_dict()},fold_path+\"BaseModel\")\n",
        "        \n",
        "    print(\"*************************************************\\n\")\n",
        "\n",
        "old_name = fold_path+\"BaseModel\"\n",
        "new_name = fold_path+\"BaseModel\" + \"{:.3f}\".format(best_val)\n",
        "\n",
        "os.rename(old_name, new_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1895fba8-5709-4a87-dc22-825aef81100f",
        "id": "OLsC96dGyy6s"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 1\n",
            "batch no = 0 / 656, train loss = 11585.467, wps = 957, since beginning = 0 mins, cuda memory = 0.523 GBs\n",
            "batch no = 100 / 656, train loss = 9100.637, wps = 232, since beginning = 0 mins, cuda memory = 0.867 GBs\n",
            "batch no = 200 / 656, train loss = 9092.630, wps = 123, since beginning = 0 mins, cuda memory = 0.867 GBs\n",
            "batch no = 300 / 656, train loss = 9245.778, wps = 91, since beginning = 0 mins, cuda memory = 0.867 GBs\n",
            "batch no = 400 / 656, train loss = 9493.389, wps = 73, since beginning = 0 mins, cuda memory = 0.878 GBs\n",
            "batch no = 500 / 656, train loss = 8737.149, wps = 57, since beginning = 0 mins, cuda memory = 0.878 GBs\n",
            "batch no = 600 / 656, train loss = 8227.327, wps = 45, since beginning = 0 mins, cuda memory = 0.878 GBs\n",
            "CE on the training set:  6.633010323611019 \n",
            "\n",
            "Perplexity on the training set:  759.7658727215797 \n",
            "\n",
            "CE on the validation set:  6.242041370032889 \n",
            "\n",
            "Perplexity on the validation set:  513.9065142555224 \n",
            "\n",
            "CE on the validation set: 6.242\n",
            "*************************************************\n",
            "\n",
            "Epoch : 2\n",
            "batch no = 0 / 656, train loss = 8076.442, wps = 42, since beginning = 1 mins, cuda memory = 0.878 GBs\n",
            "batch no = 100 / 656, train loss = 7853.966, wps = 35, since beginning = 1 mins, cuda memory = 0.878 GBs\n",
            "batch no = 200 / 656, train loss = 8483.537, wps = 34, since beginning = 1 mins, cuda memory = 0.878 GBs\n",
            "batch no = 300 / 656, train loss = 8304.021, wps = 31, since beginning = 1 mins, cuda memory = 0.878 GBs\n",
            "batch no = 400 / 656, train loss = 7620.449, wps = 25, since beginning = 1 mins, cuda memory = 0.878 GBs\n",
            "batch no = 500 / 656, train loss = 8452.625, wps = 26, since beginning = 1 mins, cuda memory = 0.878 GBs\n",
            "batch no = 600 / 656, train loss = 7935.901, wps = 23, since beginning = 1 mins, cuda memory = 0.878 GBs\n",
            "CE on the training set:  6.088576720560339 \n",
            "\n",
            "Perplexity on the training set:  440.7935920142739 \n",
            "\n",
            "CE on the validation set:  5.921913724687401 \n",
            "\n",
            "Perplexity on the validation set:  373.1250897516976 \n",
            "\n",
            "CE on the validation set: 5.922\n",
            "*************************************************\n",
            "\n",
            "Epoch : 3\n",
            "batch no = 0 / 656, train loss = 8079.806, wps = 22, since beginning = 1 mins, cuda memory = 0.878 GBs\n",
            "batch no = 100 / 656, train loss = 8721.139, wps = 23, since beginning = 1 mins, cuda memory = 0.878 GBs\n",
            "batch no = 200 / 656, train loss = 7969.608, wps = 20, since beginning = 1 mins, cuda memory = 0.878 GBs\n",
            "batch no = 300 / 656, train loss = 8436.366, wps = 19, since beginning = 1 mins, cuda memory = 0.878 GBs\n",
            "batch no = 400 / 656, train loss = 8240.520, wps = 18, since beginning = 1 mins, cuda memory = 0.878 GBs\n",
            "batch no = 500 / 656, train loss = 8269.823, wps = 17, since beginning = 1 mins, cuda memory = 0.878 GBs\n",
            "batch no = 600 / 656, train loss = 8127.107, wps = 16, since beginning = 1 mins, cuda memory = 0.878 GBs\n",
            "CE on the training set:  5.814481688355868 \n",
            "\n",
            "Perplexity on the training set:  335.1176580948137 \n",
            "\n",
            "CE on the validation set:  5.700936896944037 \n",
            "\n",
            "Perplexity on the validation set:  299.14754013197063 \n",
            "\n",
            "CE on the validation set: 5.701\n",
            "*************************************************\n",
            "\n",
            "Epoch : 4\n",
            "batch no = 0 / 656, train loss = 7589.670, wps = 14, since beginning = 2 mins, cuda memory = 0.878 GBs\n",
            "batch no = 100 / 656, train loss = 7367.366, wps = 14, since beginning = 2 mins, cuda memory = 0.878 GBs\n",
            "batch no = 200 / 656, train loss = 7602.697, wps = 13, since beginning = 2 mins, cuda memory = 0.878 GBs\n",
            "batch no = 300 / 656, train loss = 7656.146, wps = 13, since beginning = 2 mins, cuda memory = 0.878 GBs\n",
            "batch no = 400 / 656, train loss = 7582.633, wps = 12, since beginning = 2 mins, cuda memory = 0.878 GBs\n",
            "batch no = 500 / 656, train loss = 7988.833, wps = 12, since beginning = 2 mins, cuda memory = 0.878 GBs\n",
            "batch no = 600 / 656, train loss = 7307.854, wps = 11, since beginning = 2 mins, cuda memory = 0.878 GBs\n",
            "CE on the training set:  5.616915552644814 \n",
            "\n",
            "Perplexity on the training set:  275.0397279884005 \n",
            "\n",
            "CE on the validation set:  5.558802562346536 \n",
            "\n",
            "Perplexity on the validation set:  259.51190087556716 \n",
            "\n",
            "CE on the validation set: 5.559\n",
            "*************************************************\n",
            "\n",
            "Epoch : 5\n",
            "batch no = 0 / 656, train loss = 6505.117, wps = 10, since beginning = 2 mins, cuda memory = 0.878 GBs\n",
            "batch no = 100 / 656, train loss = 7714.304, wps = 11, since beginning = 2 mins, cuda memory = 0.878 GBs\n",
            "batch no = 200 / 656, train loss = 6485.815, wps = 10, since beginning = 2 mins, cuda memory = 0.878 GBs\n",
            "batch no = 300 / 656, train loss = 6833.254, wps = 9, since beginning = 2 mins, cuda memory = 0.878 GBs\n",
            "batch no = 400 / 656, train loss = 7337.615, wps = 9, since beginning = 2 mins, cuda memory = 0.878 GBs\n",
            "batch no = 500 / 656, train loss = 7398.222, wps = 9, since beginning = 2 mins, cuda memory = 0.878 GBs\n",
            "batch no = 600 / 656, train loss = 6648.780, wps = 8, since beginning = 2 mins, cuda memory = 0.878 GBs\n",
            "CE on the training set:  5.465568385999606 \n",
            "\n",
            "Perplexity on the training set:  236.41018917013287 \n",
            "\n",
            "CE on the validation set:  5.438986628584132 \n",
            "\n",
            "Perplexity on the validation set:  230.20877822198557 \n",
            "\n",
            "CE on the validation set: 5.439\n",
            "*************************************************\n",
            "\n",
            "Epoch : 6\n",
            "batch no = 0 / 656, train loss = 7078.534, wps = 9, since beginning = 3 mins, cuda memory = 0.878 GBs\n",
            "batch no = 100 / 656, train loss = 6562.050, wps = 8, since beginning = 3 mins, cuda memory = 0.878 GBs\n",
            "batch no = 200 / 656, train loss = 7011.334, wps = 8, since beginning = 3 mins, cuda memory = 0.878 GBs\n",
            "batch no = 300 / 656, train loss = 6980.106, wps = 8, since beginning = 3 mins, cuda memory = 0.878 GBs\n",
            "batch no = 400 / 656, train loss = 7284.174, wps = 8, since beginning = 3 mins, cuda memory = 0.878 GBs\n",
            "batch no = 500 / 656, train loss = 6647.878, wps = 7, since beginning = 3 mins, cuda memory = 0.878 GBs\n",
            "batch no = 600 / 656, train loss = 6343.090, wps = 6, since beginning = 3 mins, cuda memory = 0.878 GBs\n",
            "CE on the training set:  5.343378122222976 \n",
            "\n",
            "Perplexity on the training set:  209.2182827935933 \n",
            "\n",
            "CE on the validation set:  5.344810256204275 \n",
            "\n",
            "Perplexity on the validation set:  209.51812606250994 \n",
            "\n",
            "CE on the validation set: 5.345\n",
            "*************************************************\n",
            "\n",
            "Epoch : 7\n",
            "batch no = 0 / 656, train loss = 7914.439, wps = 8, since beginning = 3 mins, cuda memory = 0.878 GBs\n",
            "batch no = 100 / 656, train loss = 6972.178, wps = 7, since beginning = 3 mins, cuda memory = 0.878 GBs\n",
            "batch no = 200 / 656, train loss = 8450.031, wps = 8, since beginning = 3 mins, cuda memory = 0.878 GBs\n",
            "batch no = 300 / 656, train loss = 6746.381, wps = 7, since beginning = 3 mins, cuda memory = 0.878 GBs\n",
            "batch no = 400 / 656, train loss = 7322.847, wps = 7, since beginning = 3 mins, cuda memory = 0.878 GBs\n",
            "batch no = 500 / 656, train loss = 6997.331, wps = 6, since beginning = 3 mins, cuda memory = 0.878 GBs\n",
            "batch no = 600 / 656, train loss = 6320.815, wps = 6, since beginning = 4 mins, cuda memory = 0.878 GBs\n",
            "CE on the training set:  5.240364123836594 \n",
            "\n",
            "Perplexity on the training set:  188.7388141962457 \n",
            "\n",
            "CE on the validation set:  5.291737738105846 \n",
            "\n",
            "Perplexity on the validation set:  198.68839398346793 \n",
            "\n",
            "CE on the validation set: 5.292\n",
            "*************************************************\n",
            "\n",
            "Epoch : 8\n",
            "batch no = 0 / 656, train loss = 7342.929, wps = 6, since beginning = 4 mins, cuda memory = 0.878 GBs\n",
            "batch no = 100 / 656, train loss = 7914.752, wps = 7, since beginning = 4 mins, cuda memory = 0.878 GBs\n",
            "batch no = 200 / 656, train loss = 6589.053, wps = 6, since beginning = 4 mins, cuda memory = 0.878 GBs\n",
            "batch no = 300 / 656, train loss = 6000.636, wps = 5, since beginning = 4 mins, cuda memory = 0.878 GBs\n",
            "batch no = 400 / 656, train loss = 5957.123, wps = 5, since beginning = 4 mins, cuda memory = 0.878 GBs\n",
            "batch no = 500 / 656, train loss = 6863.207, wps = 6, since beginning = 4 mins, cuda memory = 0.878 GBs\n",
            "batch no = 600 / 656, train loss = 6680.816, wps = 5, since beginning = 4 mins, cuda memory = 0.878 GBs\n",
            "CE on the training set:  5.1498940598294185 \n",
            "\n",
            "Perplexity on the training set:  172.4132238629492 \n",
            "\n",
            "CE on the validation set:  5.202871760093086 \n",
            "\n",
            "Perplexity on the validation set:  181.79356045802714 \n",
            "\n",
            "CE on the validation set: 5.203\n",
            "*************************************************\n",
            "\n",
            "Epoch : 9\n",
            "batch no = 0 / 656, train loss = 7121.309, wps = 6, since beginning = 4 mins, cuda memory = 0.878 GBs\n",
            "batch no = 100 / 656, train loss = 6750.949, wps = 5, since beginning = 4 mins, cuda memory = 0.878 GBs\n",
            "batch no = 200 / 656, train loss = 6443.392, wps = 5, since beginning = 4 mins, cuda memory = 0.878 GBs\n",
            "batch no = 300 / 656, train loss = 7100.391, wps = 6, since beginning = 4 mins, cuda memory = 0.878 GBs\n",
            "batch no = 400 / 656, train loss = 6975.278, wps = 5, since beginning = 4 mins, cuda memory = 0.878 GBs\n",
            "batch no = 500 / 656, train loss = 6973.352, wps = 5, since beginning = 4 mins, cuda memory = 0.878 GBs\n",
            "batch no = 600 / 656, train loss = 6797.096, wps = 5, since beginning = 5 mins, cuda memory = 0.878 GBs\n",
            "CE on the training set:  5.068839397919015 \n",
            "\n",
            "Perplexity on the training set:  158.9896964499673 \n",
            "\n",
            "CE on the validation set:  5.165029537787982 \n",
            "\n",
            "Perplexity on the validation set:  175.04262886749697 \n",
            "\n",
            "CE on the validation set: 5.165\n",
            "*************************************************\n",
            "\n",
            "Epoch : 10\n",
            "batch no = 0 / 656, train loss = 7561.302, wps = 5, since beginning = 5 mins, cuda memory = 0.878 GBs\n",
            "batch no = 100 / 656, train loss = 6607.777, wps = 5, since beginning = 5 mins, cuda memory = 0.878 GBs\n",
            "batch no = 200 / 656, train loss = 6447.989, wps = 5, since beginning = 5 mins, cuda memory = 0.878 GBs\n",
            "batch no = 300 / 656, train loss = 6640.144, wps = 5, since beginning = 5 mins, cuda memory = 0.878 GBs\n",
            "batch no = 400 / 656, train loss = 6883.242, wps = 5, since beginning = 5 mins, cuda memory = 0.878 GBs\n",
            "batch no = 500 / 656, train loss = 6719.434, wps = 4, since beginning = 5 mins, cuda memory = 0.878 GBs\n",
            "batch no = 600 / 656, train loss = 7117.640, wps = 5, since beginning = 5 mins, cuda memory = 0.878 GBs\n",
            "CE on the training set:  4.997637222695615 \n",
            "\n",
            "Perplexity on the training set:  148.06290580679303 \n",
            "\n",
            "CE on the validation set:  5.116719642910987 \n",
            "\n",
            "Perplexity on the validation set:  166.7873491978905 \n",
            "\n",
            "CE on the validation set: 5.117\n",
            "*************************************************\n",
            "\n",
            "Epoch : 11\n",
            "batch no = 0 / 656, train loss = 6786.692, wps = 5, since beginning = 5 mins, cuda memory = 0.878 GBs\n",
            "batch no = 100 / 656, train loss = 5599.345, wps = 4, since beginning = 5 mins, cuda memory = 0.878 GBs\n",
            "batch no = 200 / 656, train loss = 6455.575, wps = 4, since beginning = 5 mins, cuda memory = 0.878 GBs\n",
            "batch no = 300 / 656, train loss = 6396.093, wps = 4, since beginning = 5 mins, cuda memory = 0.878 GBs\n",
            "batch no = 400 / 656, train loss = 6613.059, wps = 4, since beginning = 5 mins, cuda memory = 0.878 GBs\n",
            "batch no = 500 / 656, train loss = 7461.333, wps = 4, since beginning = 5 mins, cuda memory = 0.878 GBs\n",
            "batch no = 600 / 656, train loss = 5857.103, wps = 4, since beginning = 6 mins, cuda memory = 0.878 GBs\n",
            "CE on the training set:  4.93009315776873 \n",
            "\n",
            "Perplexity on the training set:  138.39240406704113 \n",
            "\n",
            "CE on the validation set:  5.076949617487003 \n",
            "\n",
            "Perplexity on the validation set:  160.28438078779774 \n",
            "\n",
            "CE on the validation set: 5.077\n",
            "*************************************************\n",
            "\n",
            "Epoch : 12\n",
            "batch no = 0 / 656, train loss = 6122.484, wps = 4, since beginning = 6 mins, cuda memory = 0.878 GBs\n",
            "batch no = 100 / 656, train loss = 6066.215, wps = 4, since beginning = 6 mins, cuda memory = 0.878 GBs\n",
            "batch no = 200 / 656, train loss = 6888.521, wps = 4, since beginning = 6 mins, cuda memory = 0.878 GBs\n",
            "batch no = 300 / 656, train loss = 5816.372, wps = 4, since beginning = 6 mins, cuda memory = 0.878 GBs\n",
            "batch no = 400 / 656, train loss = 6018.798, wps = 3, since beginning = 6 mins, cuda memory = 0.878 GBs\n",
            "batch no = 500 / 656, train loss = 6708.427, wps = 4, since beginning = 6 mins, cuda memory = 0.878 GBs\n",
            "batch no = 600 / 656, train loss = 7050.861, wps = 4, since beginning = 6 mins, cuda memory = 0.878 GBs\n",
            "CE on the training set:  4.869177557451062 \n",
            "\n",
            "Perplexity on the training set:  130.2137794926054 \n",
            "\n",
            "CE on the validation set:  5.039623825137848 \n",
            "\n",
            "Perplexity on the validation set:  154.41191821724638 \n",
            "\n",
            "CE on the validation set: 5.040\n",
            "*************************************************\n",
            "\n",
            "Epoch : 13\n",
            "batch no = 0 / 656, train loss = 6534.631, wps = 4, since beginning = 6 mins, cuda memory = 0.878 GBs\n",
            "batch no = 100 / 656, train loss = 7480.584, wps = 4, since beginning = 6 mins, cuda memory = 0.878 GBs\n",
            "batch no = 200 / 656, train loss = 6133.473, wps = 4, since beginning = 6 mins, cuda memory = 0.878 GBs\n",
            "batch no = 300 / 656, train loss = 7016.295, wps = 4, since beginning = 6 mins, cuda memory = 0.878 GBs\n",
            "batch no = 400 / 656, train loss = 6773.904, wps = 4, since beginning = 6 mins, cuda memory = 0.878 GBs\n",
            "batch no = 500 / 656, train loss = 5943.575, wps = 3, since beginning = 7 mins, cuda memory = 0.878 GBs\n",
            "batch no = 600 / 656, train loss = 6326.085, wps = 4, since beginning = 7 mins, cuda memory = 0.878 GBs\n",
            "CE on the training set:  4.8113636008945875 \n",
            "\n",
            "Perplexity on the training set:  122.8990886166266 \n",
            "\n",
            "CE on the validation set:  5.021052535315902 \n",
            "\n",
            "Perplexity on the validation set:  151.57075343699265 \n",
            "\n",
            "CE on the validation set: 5.021\n",
            "*************************************************\n",
            "\n",
            "Epoch : 14\n",
            "batch no = 0 / 656, train loss = 5587.741, wps = 3, since beginning = 7 mins, cuda memory = 0.878 GBs\n",
            "batch no = 100 / 656, train loss = 7446.456, wps = 4, since beginning = 7 mins, cuda memory = 0.878 GBs\n",
            "batch no = 200 / 656, train loss = 7016.007, wps = 3, since beginning = 7 mins, cuda memory = 0.878 GBs\n",
            "batch no = 300 / 656, train loss = 6392.210, wps = 3, since beginning = 7 mins, cuda memory = 0.878 GBs\n",
            "batch no = 400 / 656, train loss = 6387.268, wps = 3, since beginning = 7 mins, cuda memory = 0.878 GBs\n",
            "batch no = 500 / 656, train loss = 7021.981, wps = 4, since beginning = 7 mins, cuda memory = 0.878 GBs\n",
            "batch no = 600 / 656, train loss = 6331.680, wps = 3, since beginning = 7 mins, cuda memory = 0.878 GBs\n",
            "CE on the training set:  4.756812466668381 \n",
            "\n",
            "Perplexity on the training set:  116.37438683002048 \n",
            "\n",
            "CE on the validation set:  4.993017753219374 \n",
            "\n",
            "Perplexity on the validation set:  147.38051109711736 \n",
            "\n",
            "CE on the validation set: 4.993\n",
            "*************************************************\n",
            "\n",
            "Epoch : 15\n",
            "batch no = 0 / 656, train loss = 6167.868, wps = 3, since beginning = 7 mins, cuda memory = 0.878 GBs\n",
            "batch no = 100 / 656, train loss = 6203.638, wps = 3, since beginning = 7 mins, cuda memory = 0.878 GBs\n",
            "batch no = 200 / 656, train loss = 5600.089, wps = 3, since beginning = 7 mins, cuda memory = 0.878 GBs\n",
            "batch no = 300 / 656, train loss = 5508.847, wps = 3, since beginning = 7 mins, cuda memory = 0.878 GBs\n",
            "batch no = 400 / 656, train loss = 5560.419, wps = 3, since beginning = 7 mins, cuda memory = 0.878 GBs\n",
            "batch no = 500 / 656, train loss = 6359.396, wps = 3, since beginning = 8 mins, cuda memory = 0.878 GBs\n",
            "batch no = 600 / 656, train loss = 6027.953, wps = 3, since beginning = 8 mins, cuda memory = 0.878 GBs\n",
            "CE on the training set:  4.704944998018268 \n",
            "\n",
            "Perplexity on the training set:  110.4922074899595 \n",
            "\n",
            "CE on the validation set:  4.9666626289482165 \n",
            "\n",
            "Perplexity on the validation set:  143.5470174037989 \n",
            "\n",
            "CE on the validation set: 4.967\n",
            "*************************************************\n",
            "\n",
            "Epoch : 16\n",
            "batch no = 0 / 656, train loss = 5854.226, wps = 3, since beginning = 8 mins, cuda memory = 0.878 GBs\n",
            "batch no = 100 / 656, train loss = 5922.647, wps = 3, since beginning = 8 mins, cuda memory = 0.878 GBs\n",
            "batch no = 200 / 656, train loss = 6130.790, wps = 3, since beginning = 8 mins, cuda memory = 0.878 GBs\n",
            "batch no = 300 / 656, train loss = 6263.169, wps = 3, since beginning = 8 mins, cuda memory = 0.878 GBs\n",
            "batch no = 400 / 656, train loss = 6391.011, wps = 3, since beginning = 8 mins, cuda memory = 0.878 GBs\n",
            "batch no = 500 / 656, train loss = 5980.342, wps = 3, since beginning = 8 mins, cuda memory = 0.878 GBs\n",
            "batch no = 600 / 656, train loss = 5606.390, wps = 2, since beginning = 8 mins, cuda memory = 0.878 GBs\n",
            "CE on the training set:  4.655298584751083 \n",
            "\n",
            "Perplexity on the training set:  105.14060869504938 \n",
            "\n",
            "CE on the validation set:  4.9315292642751 \n",
            "\n",
            "Perplexity on the validation set:  138.59129307767265 \n",
            "\n",
            "CE on the validation set: 4.932\n",
            "*************************************************\n",
            "\n",
            "Epoch : 17\n",
            "batch no = 0 / 656, train loss = 6549.442, wps = 3, since beginning = 8 mins, cuda memory = 0.878 GBs\n",
            "batch no = 100 / 656, train loss = 6430.968, wps = 3, since beginning = 8 mins, cuda memory = 0.878 GBs\n",
            "batch no = 200 / 656, train loss = 6445.318, wps = 3, since beginning = 8 mins, cuda memory = 0.878 GBs\n",
            "batch no = 300 / 656, train loss = 6416.598, wps = 3, since beginning = 8 mins, cuda memory = 0.878 GBs\n",
            "batch no = 400 / 656, train loss = 5825.288, wps = 3, since beginning = 8 mins, cuda memory = 0.878 GBs\n",
            "batch no = 500 / 656, train loss = 5612.446, wps = 2, since beginning = 9 mins, cuda memory = 0.878 GBs\n",
            "batch no = 600 / 656, train loss = 5533.794, wps = 2, since beginning = 9 mins, cuda memory = 0.878 GBs\n",
            "CE on the training set:  4.607878399507191 \n",
            "\n",
            "Perplexity on the training set:  100.27118840421045 \n",
            "\n",
            "CE on the validation set:  4.923458521169515 \n",
            "\n",
            "Perplexity on the validation set:  137.47725993925587 \n",
            "\n",
            "CE on the validation set: 4.923\n",
            "*************************************************\n",
            "\n",
            "Epoch : 18\n",
            "batch no = 0 / 656, train loss = 5868.904, wps = 2, since beginning = 9 mins, cuda memory = 0.878 GBs\n",
            "batch no = 100 / 656, train loss = 6042.202, wps = 3, since beginning = 9 mins, cuda memory = 0.878 GBs\n",
            "batch no = 200 / 656, train loss = 5643.332, wps = 2, since beginning = 9 mins, cuda memory = 0.878 GBs\n",
            "batch no = 300 / 656, train loss = 6244.360, wps = 2, since beginning = 9 mins, cuda memory = 0.878 GBs\n",
            "batch no = 400 / 656, train loss = 6832.799, wps = 3, since beginning = 9 mins, cuda memory = 0.878 GBs\n",
            "batch no = 500 / 656, train loss = 5551.776, wps = 2, since beginning = 9 mins, cuda memory = 0.878 GBs\n",
            "batch no = 600 / 656, train loss = 6436.088, wps = 3, since beginning = 9 mins, cuda memory = 0.878 GBs\n",
            "CE on the training set:  4.562063942950736 \n",
            "\n",
            "Perplexity on the training set:  95.78096240574936 \n",
            "\n",
            "CE on the validation set:  4.902606859182157 \n",
            "\n",
            "Perplexity on the validation set:  134.64031092445802 \n",
            "\n",
            "CE on the validation set: 4.903\n",
            "*************************************************\n",
            "\n",
            "Epoch : 19\n",
            "batch no = 0 / 656, train loss = 5867.115, wps = 2, since beginning = 9 mins, cuda memory = 0.878 GBs\n",
            "batch no = 100 / 656, train loss = 6384.188, wps = 3, since beginning = 9 mins, cuda memory = 0.878 GBs\n",
            "batch no = 200 / 656, train loss = 5661.371, wps = 2, since beginning = 9 mins, cuda memory = 0.878 GBs\n",
            "batch no = 300 / 656, train loss = 5755.859, wps = 2, since beginning = 9 mins, cuda memory = 0.878 GBs\n",
            "batch no = 400 / 656, train loss = 6139.272, wps = 2, since beginning = 10 mins, cuda memory = 0.878 GBs\n",
            "batch no = 500 / 656, train loss = 5052.625, wps = 2, since beginning = 10 mins, cuda memory = 0.878 GBs\n",
            "batch no = 600 / 656, train loss = 5877.373, wps = 2, since beginning = 10 mins, cuda memory = 0.878 GBs\n",
            "CE on the training set:  4.517310671241069 \n",
            "\n",
            "Perplexity on the training set:  91.58895366589594 \n",
            "\n",
            "CE on the validation set:  4.89524027321923 \n",
            "\n",
            "Perplexity on the validation set:  133.65211578102145 \n",
            "\n",
            "CE on the validation set: 4.895\n",
            "*************************************************\n",
            "\n",
            "Epoch : 20\n",
            "batch no = 0 / 656, train loss = 5780.514, wps = 2, since beginning = 10 mins, cuda memory = 0.878 GBs\n",
            "batch no = 100 / 656, train loss = 6145.121, wps = 2, since beginning = 10 mins, cuda memory = 0.878 GBs\n",
            "batch no = 200 / 656, train loss = 5827.606, wps = 2, since beginning = 10 mins, cuda memory = 0.878 GBs\n",
            "batch no = 300 / 656, train loss = 6162.212, wps = 2, since beginning = 10 mins, cuda memory = 0.878 GBs\n",
            "batch no = 400 / 656, train loss = 5697.740, wps = 2, since beginning = 10 mins, cuda memory = 0.878 GBs\n",
            "batch no = 500 / 656, train loss = 6743.350, wps = 3, since beginning = 10 mins, cuda memory = 0.878 GBs\n",
            "batch no = 600 / 656, train loss = 5617.832, wps = 2, since beginning = 10 mins, cuda memory = 0.878 GBs\n",
            "CE on the training set:  4.474987309760118 \n",
            "\n",
            "Perplexity on the training set:  87.7934862822845 \n",
            "\n",
            "CE on the validation set:  4.8804083768973765 \n",
            "\n",
            "Perplexity on the validation set:  131.68442978557596 \n",
            "\n",
            "CE on the validation set: 4.880\n",
            "*************************************************\n",
            "\n",
            "Epoch : 21\n",
            "batch no = 0 / 656, train loss = 6370.883, wps = 2, since beginning = 10 mins, cuda memory = 0.878 GBs\n",
            "batch no = 100 / 656, train loss = 5791.357, wps = 2, since beginning = 10 mins, cuda memory = 0.878 GBs\n",
            "batch no = 200 / 656, train loss = 6057.272, wps = 2, since beginning = 10 mins, cuda memory = 0.878 GBs\n",
            "batch no = 300 / 656, train loss = 5628.683, wps = 2, since beginning = 10 mins, cuda memory = 0.878 GBs\n",
            "batch no = 400 / 656, train loss = 6612.486, wps = 2, since beginning = 11 mins, cuda memory = 0.878 GBs\n",
            "batch no = 500 / 656, train loss = 5392.429, wps = 2, since beginning = 11 mins, cuda memory = 0.878 GBs\n",
            "batch no = 600 / 656, train loss = 7098.033, wps = 2, since beginning = 11 mins, cuda memory = 0.878 GBs\n",
            "CE on the training set:  4.433311796538859 \n",
            "\n",
            "Perplexity on the training set:  84.20984147456804 \n",
            "\n",
            "CE on the validation set:  4.876574207414573 \n",
            "\n",
            "Perplexity on the validation set:  131.18049606453366 \n",
            "\n",
            "CE on the validation set: 4.877\n",
            "*************************************************\n",
            "\n",
            "Epoch : 22\n",
            "batch no = 0 / 656, train loss = 5496.066, wps = 2, since beginning = 11 mins, cuda memory = 0.878 GBs\n",
            "batch no = 100 / 656, train loss = 5577.208, wps = 2, since beginning = 11 mins, cuda memory = 0.878 GBs\n",
            "batch no = 200 / 656, train loss = 6024.982, wps = 2, since beginning = 11 mins, cuda memory = 0.878 GBs\n",
            "batch no = 300 / 656, train loss = 5818.364, wps = 2, since beginning = 11 mins, cuda memory = 0.878 GBs\n",
            "batch no = 400 / 656, train loss = 5199.446, wps = 2, since beginning = 11 mins, cuda memory = 0.878 GBs\n",
            "batch no = 500 / 656, train loss = 6145.403, wps = 2, since beginning = 11 mins, cuda memory = 0.878 GBs\n",
            "batch no = 600 / 656, train loss = 6043.203, wps = 2, since beginning = 11 mins, cuda memory = 0.878 GBs\n",
            "CE on the training set:  4.39321019614699 \n",
            "\n",
            "Perplexity on the training set:  80.8997065020102 \n",
            "\n",
            "CE on the validation set:  4.864894030916069 \n",
            "\n",
            "Perplexity on the validation set:  129.65719822927264 \n",
            "\n",
            "CE on the validation set: 4.865\n",
            "*************************************************\n",
            "\n",
            "Epoch : 23\n",
            "batch no = 0 / 656, train loss = 5188.675, wps = 2, since beginning = 11 mins, cuda memory = 0.878 GBs\n",
            "batch no = 100 / 656, train loss = 6134.170, wps = 2, since beginning = 11 mins, cuda memory = 0.878 GBs\n",
            "batch no = 200 / 656, train loss = 5769.314, wps = 2, since beginning = 11 mins, cuda memory = 0.878 GBs\n",
            "batch no = 300 / 656, train loss = 5506.447, wps = 2, since beginning = 12 mins, cuda memory = 0.878 GBs\n",
            "batch no = 400 / 656, train loss = 6458.228, wps = 2, since beginning = 12 mins, cuda memory = 0.878 GBs\n",
            "batch no = 500 / 656, train loss = 5458.066, wps = 2, since beginning = 12 mins, cuda memory = 0.878 GBs\n",
            "batch no = 600 / 656, train loss = 5476.788, wps = 2, since beginning = 12 mins, cuda memory = 0.878 GBs\n",
            "CE on the training set:  4.352730259574911 \n",
            "\n",
            "Perplexity on the training set:  77.69028827802632 \n",
            "\n",
            "CE on the validation set:  4.86219472963966 \n",
            "\n",
            "Perplexity on the validation set:  129.30768631978702 \n",
            "\n",
            "CE on the validation set: 4.862\n",
            "*************************************************\n",
            "\n",
            "Epoch : 24\n",
            "batch no = 0 / 656, train loss = 5305.746, wps = 2, since beginning = 12 mins, cuda memory = 0.878 GBs\n",
            "batch no = 100 / 656, train loss = 6062.561, wps = 2, since beginning = 12 mins, cuda memory = 0.878 GBs\n",
            "batch no = 200 / 656, train loss = 5926.890, wps = 2, since beginning = 12 mins, cuda memory = 0.878 GBs\n",
            "batch no = 300 / 656, train loss = 6142.030, wps = 2, since beginning = 12 mins, cuda memory = 0.878 GBs\n",
            "batch no = 400 / 656, train loss = 6041.320, wps = 2, since beginning = 12 mins, cuda memory = 0.878 GBs\n",
            "batch no = 500 / 656, train loss = 6023.338, wps = 2, since beginning = 12 mins, cuda memory = 0.878 GBs\n",
            "batch no = 600 / 656, train loss = 6651.354, wps = 2, since beginning = 12 mins, cuda memory = 0.878 GBs\n",
            "CE on the training set:  4.313832006283324 \n",
            "\n",
            "Perplexity on the training set:  74.72629261269785 \n",
            "\n",
            "CE on the validation set:  4.856378449654503 \n",
            "\n",
            "Perplexity on the validation set:  128.55777955981316 \n",
            "\n",
            "CE on the validation set: 4.856\n",
            "*************************************************\n",
            "\n",
            "Epoch : 25\n",
            "batch no = 0 / 656, train loss = 6260.313, wps = 2, since beginning = 12 mins, cuda memory = 0.878 GBs\n",
            "batch no = 100 / 656, train loss = 5693.381, wps = 2, since beginning = 12 mins, cuda memory = 0.878 GBs\n",
            "batch no = 200 / 656, train loss = 5627.901, wps = 2, since beginning = 12 mins, cuda memory = 0.878 GBs\n",
            "batch no = 300 / 656, train loss = 5798.041, wps = 2, since beginning = 13 mins, cuda memory = 0.878 GBs\n",
            "batch no = 400 / 656, train loss = 5119.325, wps = 2, since beginning = 13 mins, cuda memory = 0.878 GBs\n",
            "batch no = 500 / 656, train loss = 5698.484, wps = 2, since beginning = 13 mins, cuda memory = 0.878 GBs\n",
            "batch no = 600 / 656, train loss = 6279.960, wps = 2, since beginning = 13 mins, cuda memory = 0.878 GBs\n",
            "CE on the training set:  4.275983298746208 \n",
            "\n",
            "Perplexity on the training set:  71.9508537215181 \n",
            "\n",
            "CE on the validation set:  4.859732027272756 \n",
            "\n",
            "Perplexity on the validation set:  128.98963177221935 \n",
            "\n",
            "*************************************************\n",
            "\n",
            "Epoch : 26\n",
            "batch no = 0 / 656, train loss = 5724.129, wps = 2, since beginning = 13 mins, cuda memory = 0.878 GBs\n",
            "batch no = 100 / 656, train loss = 5377.050, wps = 2, since beginning = 13 mins, cuda memory = 0.878 GBs\n",
            "batch no = 200 / 656, train loss = 5052.278, wps = 2, since beginning = 13 mins, cuda memory = 0.878 GBs\n",
            "batch no = 300 / 656, train loss = 5916.188, wps = 2, since beginning = 13 mins, cuda memory = 0.878 GBs\n",
            "batch no = 400 / 656, train loss = 5291.309, wps = 2, since beginning = 13 mins, cuda memory = 0.878 GBs\n",
            "batch no = 500 / 656, train loss = 6252.410, wps = 2, since beginning = 13 mins, cuda memory = 0.878 GBs\n",
            "batch no = 600 / 656, train loss = 5880.360, wps = 2, since beginning = 13 mins, cuda memory = 0.878 GBs\n",
            "CE on the training set:  4.2379488917147885 \n",
            "\n",
            "Perplexity on the training set:  69.26563472000475 \n",
            "\n",
            "CE on the validation set:  4.852334180986996 \n",
            "\n",
            "Perplexity on the validation set:  128.03890729662075 \n",
            "\n",
            "CE on the validation set: 4.852\n",
            "*************************************************\n",
            "\n",
            "Epoch : 27\n",
            "batch no = 0 / 656, train loss = 5784.579, wps = 2, since beginning = 13 mins, cuda memory = 0.878 GBs\n",
            "batch no = 100 / 656, train loss = 5965.985, wps = 2, since beginning = 13 mins, cuda memory = 0.878 GBs\n",
            "batch no = 200 / 656, train loss = 6191.877, wps = 2, since beginning = 13 mins, cuda memory = 0.878 GBs\n",
            "batch no = 300 / 656, train loss = 5450.688, wps = 2, since beginning = 14 mins, cuda memory = 0.878 GBs\n",
            "batch no = 400 / 656, train loss = 4618.169, wps = 1, since beginning = 14 mins, cuda memory = 0.878 GBs\n",
            "batch no = 500 / 656, train loss = 5099.894, wps = 1, since beginning = 14 mins, cuda memory = 0.878 GBs\n",
            "batch no = 600 / 656, train loss = 6617.411, wps = 2, since beginning = 14 mins, cuda memory = 0.878 GBs\n",
            "CE on the training set:  4.200992940130813 \n",
            "\n",
            "Perplexity on the training set:  66.7525794601063 \n",
            "\n",
            "CE on the validation set:  4.861759187658865 \n",
            "\n",
            "Perplexity on the validation set:  129.25137965680304 \n",
            "\n",
            "*************************************************\n",
            "\n",
            "Epoch : 28\n",
            "batch no = 0 / 656, train loss = 5457.326, wps = 2, since beginning = 14 mins, cuda memory = 0.878 GBs\n",
            "batch no = 100 / 656, train loss = 6078.124, wps = 2, since beginning = 14 mins, cuda memory = 0.878 GBs\n",
            "batch no = 200 / 656, train loss = 5634.182, wps = 2, since beginning = 14 mins, cuda memory = 0.878 GBs\n",
            "batch no = 300 / 656, train loss = 5448.082, wps = 2, since beginning = 14 mins, cuda memory = 0.878 GBs\n",
            "batch no = 400 / 656, train loss = 5768.938, wps = 2, since beginning = 14 mins, cuda memory = 0.878 GBs\n",
            "batch no = 500 / 656, train loss = 5611.139, wps = 2, since beginning = 14 mins, cuda memory = 0.878 GBs\n",
            "batch no = 600 / 656, train loss = 5937.826, wps = 2, since beginning = 14 mins, cuda memory = 0.878 GBs\n",
            "CE on the training set:  4.1645898037675515 \n",
            "\n",
            "Perplexity on the training set:  64.36627422437311 \n",
            "\n",
            "CE on the validation set:  4.864352257983291 \n",
            "\n",
            "Perplexity on the validation set:  129.58697249365122 \n",
            "\n",
            "*************************************************\n",
            "\n",
            "Epoch : 29\n",
            "batch no = 0 / 656, train loss = 5443.436, wps = 2, since beginning = 14 mins, cuda memory = 0.878 GBs\n",
            "batch no = 100 / 656, train loss = 6280.485, wps = 2, since beginning = 14 mins, cuda memory = 0.878 GBs\n",
            "batch no = 200 / 656, train loss = 6263.476, wps = 2, since beginning = 15 mins, cuda memory = 0.878 GBs\n",
            "batch no = 300 / 656, train loss = 5267.695, wps = 2, since beginning = 15 mins, cuda memory = 0.878 GBs\n",
            "batch no = 400 / 656, train loss = 5277.012, wps = 1, since beginning = 15 mins, cuda memory = 0.878 GBs\n",
            "batch no = 500 / 656, train loss = 5116.768, wps = 1, since beginning = 15 mins, cuda memory = 0.878 GBs\n",
            "batch no = 600 / 656, train loss = 5839.705, wps = 2, since beginning = 15 mins, cuda memory = 0.878 GBs\n",
            "CE on the training set:  4.129143663583886 \n",
            "\n",
            "Perplexity on the training set:  62.124700506477545 \n",
            "\n",
            "CE on the validation set:  4.867011322713248 \n",
            "\n",
            "Perplexity on the validation set:  129.93201117846635 \n",
            "\n",
            "*************************************************\n",
            "\n",
            "Epoch : 30\n",
            "batch no = 0 / 656, train loss = 4958.498, wps = 1, since beginning = 15 mins, cuda memory = 0.878 GBs\n",
            "batch no = 100 / 656, train loss = 5507.721, wps = 2, since beginning = 15 mins, cuda memory = 0.878 GBs\n",
            "batch no = 200 / 656, train loss = 6106.153, wps = 2, since beginning = 15 mins, cuda memory = 0.878 GBs\n",
            "batch no = 300 / 656, train loss = 6562.948, wps = 2, since beginning = 15 mins, cuda memory = 0.878 GBs\n",
            "batch no = 400 / 656, train loss = 5224.977, wps = 1, since beginning = 15 mins, cuda memory = 0.878 GBs\n",
            "batch no = 500 / 656, train loss = 5735.284, wps = 1, since beginning = 15 mins, cuda memory = 0.878 GBs\n",
            "batch no = 600 / 656, train loss = 5824.043, wps = 2, since beginning = 15 mins, cuda memory = 0.878 GBs\n",
            "CE on the training set:  4.093439076165429 \n",
            "\n",
            "Perplexity on the training set:  59.9456954263272 \n",
            "\n",
            "CE on the validation set:  4.8768186433794805 \n",
            "\n",
            "Perplexity on the validation set:  131.21256521493336 \n",
            "\n",
            "*************************************************\n",
            "\n",
            "Epoch : 31\n",
            "batch no = 0 / 656, train loss = 5083.884, wps = 1, since beginning = 15 mins, cuda memory = 0.878 GBs\n",
            "batch no = 100 / 656, train loss = 4979.659, wps = 1, since beginning = 15 mins, cuda memory = 0.878 GBs\n",
            "batch no = 200 / 656, train loss = 5511.750, wps = 1, since beginning = 16 mins, cuda memory = 0.878 GBs\n",
            "batch no = 300 / 656, train loss = 5909.701, wps = 2, since beginning = 16 mins, cuda memory = 0.878 GBs\n",
            "batch no = 400 / 656, train loss = 5900.027, wps = 2, since beginning = 16 mins, cuda memory = 0.878 GBs\n",
            "batch no = 500 / 656, train loss = 5762.868, wps = 1, since beginning = 16 mins, cuda memory = 0.878 GBs\n",
            "batch no = 600 / 656, train loss = 6067.625, wps = 2, since beginning = 16 mins, cuda memory = 0.878 GBs\n",
            "CE on the training set:  4.0590569285913825 \n",
            "\n",
            "Perplexity on the training set:  57.9196629363471 \n",
            "\n",
            "CE on the validation set:  4.869826509743854 \n",
            "\n",
            "Perplexity on the validation set:  130.29830944834796 \n",
            "\n",
            "*************************************************\n",
            "\n",
            "Epoch : 32\n",
            "batch no = 0 / 656, train loss = 5347.936, wps = 1, since beginning = 16 mins, cuda memory = 0.878 GBs\n",
            "batch no = 100 / 656, train loss = 4901.224, wps = 1, since beginning = 16 mins, cuda memory = 0.878 GBs\n",
            "batch no = 200 / 656, train loss = 5253.319, wps = 1, since beginning = 16 mins, cuda memory = 0.878 GBs\n",
            "batch no = 300 / 656, train loss = 5662.811, wps = 1, since beginning = 16 mins, cuda memory = 0.878 GBs\n",
            "batch no = 400 / 656, train loss = 5221.902, wps = 1, since beginning = 16 mins, cuda memory = 0.878 GBs\n",
            "batch no = 500 / 656, train loss = 6356.266, wps = 2, since beginning = 16 mins, cuda memory = 0.878 GBs\n",
            "batch no = 600 / 656, train loss = 4713.526, wps = 1, since beginning = 16 mins, cuda memory = 0.878 GBs\n",
            "CE on the training set:  4.024448814040985 \n",
            "\n",
            "Perplexity on the training set:  55.94946172347601 \n",
            "\n",
            "CE on the validation set:  4.881128605453336 \n",
            "\n",
            "Perplexity on the validation set:  131.77930683476154 \n",
            "\n",
            "*************************************************\n",
            "\n",
            "Epoch : 33\n",
            "batch no = 0 / 656, train loss = 5113.882, wps = 1, since beginning = 16 mins, cuda memory = 0.878 GBs\n",
            "batch no = 100 / 656, train loss = 4862.410, wps = 1, since beginning = 16 mins, cuda memory = 0.878 GBs\n",
            "batch no = 200 / 656, train loss = 5235.388, wps = 1, since beginning = 17 mins, cuda memory = 0.878 GBs\n",
            "batch no = 300 / 656, train loss = 5676.169, wps = 1, since beginning = 17 mins, cuda memory = 0.878 GBs\n",
            "batch no = 400 / 656, train loss = 5563.599, wps = 1, since beginning = 17 mins, cuda memory = 0.878 GBs\n",
            "batch no = 500 / 656, train loss = 5536.313, wps = 1, since beginning = 17 mins, cuda memory = 0.878 GBs\n",
            "batch no = 600 / 656, train loss = 5878.850, wps = 1, since beginning = 17 mins, cuda memory = 0.878 GBs\n",
            "CE on the training set:  3.9910155280122064 \n",
            "\n",
            "Perplexity on the training set:  54.10981150043604 \n",
            "\n",
            "CE on the validation set:  4.894552085527287 \n",
            "\n",
            "Perplexity on the validation set:  133.56016968165457 \n",
            "\n",
            "*************************************************\n",
            "\n",
            "Epoch : 34\n",
            "batch no = 0 / 656, train loss = 5248.817, wps = 1, since beginning = 17 mins, cuda memory = 0.878 GBs\n",
            "batch no = 100 / 656, train loss = 5525.069, wps = 1, since beginning = 17 mins, cuda memory = 0.878 GBs\n",
            "batch no = 200 / 656, train loss = 5285.524, wps = 1, since beginning = 17 mins, cuda memory = 0.878 GBs\n",
            "batch no = 300 / 656, train loss = 5359.188, wps = 1, since beginning = 17 mins, cuda memory = 0.878 GBs\n",
            "batch no = 400 / 656, train loss = 5384.312, wps = 1, since beginning = 17 mins, cuda memory = 0.878 GBs\n",
            "batch no = 500 / 656, train loss = 5716.838, wps = 1, since beginning = 17 mins, cuda memory = 0.878 GBs\n",
            "batch no = 600 / 656, train loss = 5198.545, wps = 1, since beginning = 17 mins, cuda memory = 0.878 GBs\n",
            "CE on the training set:  3.957378921734724 \n",
            "\n",
            "Perplexity on the training set:  52.32001122712925 \n",
            "\n",
            "CE on the validation set:  4.894850399929614 \n",
            "\n",
            "Perplexity on the validation set:  133.60001854729762 \n",
            "\n",
            "*************************************************\n",
            "\n",
            "Epoch : 35\n",
            "batch no = 0 / 656, train loss = 5382.254, wps = 1, since beginning = 17 mins, cuda memory = 0.878 GBs\n",
            "batch no = 100 / 656, train loss = 5128.224, wps = 1, since beginning = 18 mins, cuda memory = 0.878 GBs\n",
            "batch no = 200 / 656, train loss = 5356.475, wps = 1, since beginning = 18 mins, cuda memory = 0.878 GBs\n",
            "batch no = 300 / 656, train loss = 5317.015, wps = 1, since beginning = 18 mins, cuda memory = 0.878 GBs\n",
            "batch no = 400 / 656, train loss = 4711.957, wps = 1, since beginning = 18 mins, cuda memory = 0.878 GBs\n",
            "batch no = 500 / 656, train loss = 4992.096, wps = 1, since beginning = 18 mins, cuda memory = 0.878 GBs\n",
            "batch no = 600 / 656, train loss = 5133.417, wps = 1, since beginning = 18 mins, cuda memory = 0.878 GBs\n",
            "CE on the training set:  3.9244553713484915 \n",
            "\n",
            "Perplexity on the training set:  50.62549845224439 \n",
            "\n",
            "CE on the validation set:  4.905643224159952 \n",
            "\n",
            "Perplexity on the validation set:  135.04974933680793 \n",
            "\n",
            "*************************************************\n",
            "\n",
            "Epoch : 36\n",
            "batch no = 0 / 656, train loss = 4767.042, wps = 1, since beginning = 18 mins, cuda memory = 0.878 GBs\n",
            "batch no = 100 / 656, train loss = 4971.126, wps = 1, since beginning = 18 mins, cuda memory = 0.878 GBs\n",
            "batch no = 200 / 656, train loss = 4889.248, wps = 1, since beginning = 18 mins, cuda memory = 0.878 GBs\n",
            "batch no = 300 / 656, train loss = 5686.480, wps = 1, since beginning = 18 mins, cuda memory = 0.878 GBs\n",
            "batch no = 400 / 656, train loss = 5451.549, wps = 1, since beginning = 18 mins, cuda memory = 0.878 GBs\n",
            "batch no = 500 / 656, train loss = 5262.331, wps = 1, since beginning = 18 mins, cuda memory = 0.878 GBs\n",
            "batch no = 600 / 656, train loss = 5895.113, wps = 1, since beginning = 18 mins, cuda memory = 0.878 GBs\n",
            "CE on the training set:  3.892301793802097 \n",
            "\n",
            "Perplexity on the training set:  49.023598969827276 \n",
            "\n",
            "CE on the validation set:  4.920457452826622 \n",
            "\n",
            "Perplexity on the validation set:  137.06529975610684 \n",
            "\n",
            "*************************************************\n",
            "\n",
            "Epoch : 37\n",
            "batch no = 0 / 656, train loss = 4580.892, wps = 1, since beginning = 18 mins, cuda memory = 0.878 GBs\n",
            "batch no = 100 / 656, train loss = 5494.778, wps = 1, since beginning = 19 mins, cuda memory = 0.878 GBs\n",
            "batch no = 200 / 656, train loss = 5910.292, wps = 1, since beginning = 19 mins, cuda memory = 0.878 GBs\n",
            "batch no = 300 / 656, train loss = 5070.811, wps = 1, since beginning = 19 mins, cuda memory = 0.878 GBs\n",
            "batch no = 400 / 656, train loss = 5796.240, wps = 1, since beginning = 19 mins, cuda memory = 0.878 GBs\n",
            "batch no = 500 / 656, train loss = 5243.280, wps = 1, since beginning = 19 mins, cuda memory = 0.878 GBs\n",
            "batch no = 600 / 656, train loss = 4656.323, wps = 1, since beginning = 19 mins, cuda memory = 0.878 GBs\n",
            "CE on the training set:  3.8598693518147087 \n",
            "\n",
            "Perplexity on the training set:  47.45915051190715 \n",
            "\n",
            "CE on the validation set:  4.945632307412503 \n",
            "\n",
            "Perplexity on the validation set:  140.55969969861226 \n",
            "\n",
            "*************************************************\n",
            "\n",
            "Epoch : 38\n",
            "batch no = 0 / 656, train loss = 5434.259, wps = 1, since beginning = 19 mins, cuda memory = 0.878 GBs\n",
            "batch no = 100 / 656, train loss = 5833.228, wps = 1, since beginning = 19 mins, cuda memory = 0.878 GBs\n",
            "batch no = 200 / 656, train loss = 5271.530, wps = 1, since beginning = 19 mins, cuda memory = 0.878 GBs\n",
            "batch no = 300 / 656, train loss = 4772.148, wps = 1, since beginning = 19 mins, cuda memory = 0.878 GBs\n",
            "batch no = 400 / 656, train loss = 5931.963, wps = 1, since beginning = 19 mins, cuda memory = 0.878 GBs\n",
            "batch no = 500 / 656, train loss = 5636.473, wps = 1, since beginning = 19 mins, cuda memory = 0.878 GBs\n",
            "batch no = 600 / 656, train loss = 5381.553, wps = 1, since beginning = 19 mins, cuda memory = 0.878 GBs\n",
            "CE on the training set:  3.8275820616950256 \n",
            "\n",
            "Perplexity on the training set:  45.95129640073601 \n",
            "\n",
            "CE on the validation set:  4.941358348679757 \n",
            "\n",
            "Perplexity on the validation set:  139.96023529818837 \n",
            "\n",
            "*************************************************\n",
            "\n",
            "Epoch : 39\n",
            "batch no = 0 / 656, train loss = 4740.741, wps = 1, since beginning = 19 mins, cuda memory = 0.878 GBs\n",
            "batch no = 100 / 656, train loss = 5132.329, wps = 1, since beginning = 20 mins, cuda memory = 0.878 GBs\n",
            "batch no = 200 / 656, train loss = 5239.687, wps = 1, since beginning = 20 mins, cuda memory = 0.878 GBs\n",
            "batch no = 300 / 656, train loss = 5359.367, wps = 1, since beginning = 20 mins, cuda memory = 0.878 GBs\n",
            "batch no = 400 / 656, train loss = 4941.695, wps = 1, since beginning = 20 mins, cuda memory = 0.878 GBs\n",
            "batch no = 500 / 656, train loss = 5177.826, wps = 1, since beginning = 20 mins, cuda memory = 0.878 GBs\n",
            "batch no = 600 / 656, train loss = 5516.301, wps = 1, since beginning = 20 mins, cuda memory = 0.878 GBs\n",
            "CE on the training set:  3.796871952404254 \n",
            "\n",
            "Perplexity on the training set:  44.561575526202255 \n",
            "\n",
            "CE on the validation set:  4.954972604270176 \n",
            "\n",
            "Perplexity on the validation set:  141.87871944865032 \n",
            "\n",
            "*************************************************\n",
            "\n",
            "Epoch : 40\n",
            "batch no = 0 / 656, train loss = 5409.265, wps = 1, since beginning = 20 mins, cuda memory = 0.878 GBs\n",
            "batch no = 100 / 656, train loss = 5350.516, wps = 1, since beginning = 20 mins, cuda memory = 0.878 GBs\n",
            "batch no = 200 / 656, train loss = 5161.364, wps = 1, since beginning = 20 mins, cuda memory = 0.878 GBs\n",
            "batch no = 300 / 656, train loss = 4765.096, wps = 1, since beginning = 20 mins, cuda memory = 0.878 GBs\n",
            "batch no = 400 / 656, train loss = 4849.595, wps = 1, since beginning = 20 mins, cuda memory = 0.878 GBs\n",
            "batch no = 500 / 656, train loss = 4654.913, wps = 1, since beginning = 20 mins, cuda memory = 0.878 GBs\n",
            "batch no = 600 / 656, train loss = 5148.700, wps = 1, since beginning = 20 mins, cuda memory = 0.878 GBs\n",
            "CE on the training set:  3.7663068475273893 \n",
            "\n",
            "Perplexity on the training set:  43.220151116774815 \n",
            "\n",
            "CE on the validation set:  4.961686471457676 \n",
            "\n",
            "Perplexity on the validation set:  142.83447915949162 \n",
            "\n",
            "*************************************************\n",
            "\n",
            "Epoch : 41\n",
            "batch no = 0 / 656, train loss = 5159.684, wps = 1, since beginning = 21 mins, cuda memory = 0.878 GBs\n",
            "batch no = 100 / 656, train loss = 4722.506, wps = 1, since beginning = 21 mins, cuda memory = 0.878 GBs\n",
            "batch no = 200 / 656, train loss = 5300.861, wps = 1, since beginning = 21 mins, cuda memory = 0.878 GBs\n",
            "batch no = 300 / 656, train loss = 5470.877, wps = 1, since beginning = 21 mins, cuda memory = 0.878 GBs\n",
            "batch no = 400 / 656, train loss = 5548.262, wps = 1, since beginning = 21 mins, cuda memory = 0.878 GBs\n",
            "batch no = 500 / 656, train loss = 4856.272, wps = 1, since beginning = 21 mins, cuda memory = 0.878 GBs\n",
            "batch no = 600 / 656, train loss = 4740.499, wps = 1, since beginning = 21 mins, cuda memory = 0.878 GBs\n",
            "CE on the training set:  3.7364193341839376 \n",
            "\n",
            "Perplexity on the training set:  41.947520881563904 \n",
            "\n",
            "CE on the validation set:  4.975615668384026 \n",
            "\n",
            "Perplexity on the validation set:  144.83796986242479 \n",
            "\n",
            "*************************************************\n",
            "\n",
            "Epoch : 42\n",
            "batch no = 0 / 656, train loss = 5207.264, wps = 1, since beginning = 21 mins, cuda memory = 0.878 GBs\n",
            "batch no = 100 / 656, train loss = 4813.226, wps = 1, since beginning = 21 mins, cuda memory = 0.878 GBs\n",
            "batch no = 200 / 656, train loss = 5089.471, wps = 1, since beginning = 21 mins, cuda memory = 0.878 GBs\n",
            "batch no = 300 / 656, train loss = 5342.994, wps = 1, since beginning = 21 mins, cuda memory = 0.878 GBs\n",
            "batch no = 400 / 656, train loss = 5278.630, wps = 1, since beginning = 21 mins, cuda memory = 0.878 GBs\n",
            "batch no = 500 / 656, train loss = 4846.216, wps = 1, since beginning = 21 mins, cuda memory = 0.878 GBs\n",
            "batch no = 600 / 656, train loss = 5166.417, wps = 1, since beginning = 21 mins, cuda memory = 0.878 GBs\n",
            "CE on the training set:  3.7081303864505832 \n",
            "\n",
            "Perplexity on the training set:  40.77749705360288 \n",
            "\n",
            "CE on the validation set:  4.990837715755647 \n",
            "\n",
            "Perplexity on the validation set:  147.05956602475246 \n",
            "\n",
            "*************************************************\n",
            "\n",
            "Epoch : 43\n",
            "batch no = 0 / 656, train loss = 4722.803, wps = 1, since beginning = 22 mins, cuda memory = 0.878 GBs\n",
            "batch no = 100 / 656, train loss = 5488.112, wps = 1, since beginning = 22 mins, cuda memory = 0.878 GBs\n",
            "batch no = 200 / 656, train loss = 5271.183, wps = 1, since beginning = 22 mins, cuda memory = 0.878 GBs\n",
            "batch no = 300 / 656, train loss = 4849.460, wps = 1, since beginning = 22 mins, cuda memory = 0.878 GBs\n",
            "batch no = 400 / 656, train loss = 5106.701, wps = 1, since beginning = 22 mins, cuda memory = 0.878 GBs\n",
            "batch no = 500 / 656, train loss = 5405.743, wps = 1, since beginning = 22 mins, cuda memory = 0.878 GBs\n",
            "batch no = 600 / 656, train loss = 5394.456, wps = 1, since beginning = 22 mins, cuda memory = 0.878 GBs\n",
            "CE on the training set:  3.67949142750691 \n",
            "\n",
            "Perplexity on the training set:  39.626236133418246 \n",
            "\n",
            "CE on the validation set:  5.008974810206137 \n",
            "\n",
            "Perplexity on the validation set:  149.75113409257847 \n",
            "\n",
            "*************************************************\n",
            "\n",
            "Epoch : 44\n",
            "batch no = 0 / 656, train loss = 4652.300, wps = 1, since beginning = 22 mins, cuda memory = 0.878 GBs\n",
            "batch no = 100 / 656, train loss = 4745.804, wps = 1, since beginning = 22 mins, cuda memory = 0.878 GBs\n",
            "batch no = 200 / 656, train loss = 4760.034, wps = 1, since beginning = 22 mins, cuda memory = 0.878 GBs\n",
            "batch no = 300 / 656, train loss = 4467.643, wps = 1, since beginning = 22 mins, cuda memory = 0.878 GBs\n",
            "batch no = 400 / 656, train loss = 4673.700, wps = 1, since beginning = 22 mins, cuda memory = 0.878 GBs\n",
            "batch no = 500 / 656, train loss = 5011.290, wps = 1, since beginning = 22 mins, cuda memory = 0.878 GBs\n",
            "batch no = 600 / 656, train loss = 5513.395, wps = 1, since beginning = 22 mins, cuda memory = 0.878 GBs\n",
            "CE on the training set:  3.651584573263204 \n",
            "\n",
            "Perplexity on the training set:  38.53568030417623 \n",
            "\n",
            "CE on the validation set:  5.014340907789368 \n",
            "\n",
            "Perplexity on the validation set:  150.55687319520698 \n",
            "\n",
            "*************************************************\n",
            "\n",
            "Epoch : 45\n",
            "batch no = 0 / 656, train loss = 5420.910, wps = 1, since beginning = 23 mins, cuda memory = 0.878 GBs\n",
            "batch no = 100 / 656, train loss = 4858.787, wps = 1, since beginning = 23 mins, cuda memory = 0.878 GBs\n",
            "batch no = 200 / 656, train loss = 5171.773, wps = 1, since beginning = 23 mins, cuda memory = 0.878 GBs\n",
            "batch no = 300 / 656, train loss = 5399.206, wps = 1, since beginning = 23 mins, cuda memory = 0.878 GBs\n",
            "batch no = 400 / 656, train loss = 4685.753, wps = 1, since beginning = 23 mins, cuda memory = 0.878 GBs\n",
            "batch no = 500 / 656, train loss = 4723.103, wps = 1, since beginning = 23 mins, cuda memory = 0.878 GBs\n",
            "batch no = 600 / 656, train loss = 4647.074, wps = 1, since beginning = 23 mins, cuda memory = 0.878 GBs\n",
            "CE on the training set:  3.6229160180360345 \n",
            "\n",
            "Perplexity on the training set:  37.44660374133554 \n",
            "\n",
            "CE on the validation set:  5.032837103923869 \n",
            "\n",
            "Perplexity on the validation set:  153.36751561954927 \n",
            "\n",
            "*************************************************\n",
            "\n",
            "Epoch : 46\n",
            "batch no = 0 / 656, train loss = 4866.333, wps = 1, since beginning = 23 mins, cuda memory = 0.878 GBs\n",
            "batch no = 100 / 656, train loss = 4507.233, wps = 1, since beginning = 23 mins, cuda memory = 0.878 GBs\n",
            "batch no = 200 / 656, train loss = 3896.458, wps = 1, since beginning = 23 mins, cuda memory = 0.878 GBs\n",
            "batch no = 300 / 656, train loss = 4896.630, wps = 1, since beginning = 23 mins, cuda memory = 0.878 GBs\n",
            "batch no = 400 / 656, train loss = 4770.320, wps = 1, since beginning = 23 mins, cuda memory = 0.878 GBs\n",
            "batch no = 500 / 656, train loss = 5141.336, wps = 1, since beginning = 23 mins, cuda memory = 0.878 GBs\n",
            "batch no = 600 / 656, train loss = 5427.213, wps = 1, since beginning = 24 mins, cuda memory = 0.878 GBs\n",
            "CE on the training set:  3.597159277631289 \n",
            "\n",
            "Perplexity on the training set:  36.49441654916108 \n",
            "\n",
            "CE on the validation set:  5.043011609446107 \n",
            "\n",
            "Perplexity on the validation set:  154.93591959144032 \n",
            "\n",
            "*************************************************\n",
            "\n",
            "Epoch : 47\n",
            "batch no = 0 / 656, train loss = 4895.904, wps = 1, since beginning = 24 mins, cuda memory = 0.878 GBs\n",
            "batch no = 100 / 656, train loss = 5114.383, wps = 1, since beginning = 24 mins, cuda memory = 0.878 GBs\n",
            "batch no = 200 / 656, train loss = 4186.482, wps = 1, since beginning = 24 mins, cuda memory = 0.878 GBs\n",
            "batch no = 300 / 656, train loss = 4876.769, wps = 1, since beginning = 24 mins, cuda memory = 0.878 GBs\n",
            "batch no = 400 / 656, train loss = 5116.526, wps = 1, since beginning = 24 mins, cuda memory = 0.878 GBs\n",
            "batch no = 500 / 656, train loss = 5068.913, wps = 1, since beginning = 24 mins, cuda memory = 0.878 GBs\n",
            "batch no = 600 / 656, train loss = 4128.076, wps = 1, since beginning = 24 mins, cuda memory = 0.878 GBs\n",
            "CE on the training set:  3.570144465061665 \n",
            "\n",
            "Perplexity on the training set:  35.52172442908449 \n",
            "\n",
            "CE on the validation set:  5.0561662985606315 \n",
            "\n",
            "Perplexity on the validation set:  156.98751793054598 \n",
            "\n",
            "*************************************************\n",
            "\n",
            "Epoch : 48\n",
            "batch no = 0 / 656, train loss = 4569.126, wps = 1, since beginning = 24 mins, cuda memory = 0.878 GBs\n",
            "batch no = 100 / 656, train loss = 4872.608, wps = 1, since beginning = 24 mins, cuda memory = 0.878 GBs\n",
            "batch no = 200 / 656, train loss = 4516.685, wps = 1, since beginning = 24 mins, cuda memory = 0.878 GBs\n",
            "batch no = 300 / 656, train loss = 4802.225, wps = 1, since beginning = 24 mins, cuda memory = 0.878 GBs\n",
            "batch no = 400 / 656, train loss = 5031.650, wps = 1, since beginning = 24 mins, cuda memory = 0.878 GBs\n",
            "batch no = 500 / 656, train loss = 5005.737, wps = 1, since beginning = 24 mins, cuda memory = 0.878 GBs\n",
            "batch no = 600 / 656, train loss = 4915.629, wps = 1, since beginning = 25 mins, cuda memory = 0.878 GBs\n",
            "CE on the training set:  3.5460838737124925 \n",
            "\n",
            "Perplexity on the training set:  34.67725074100884 \n",
            "\n",
            "CE on the validation set:  5.079009803203487 \n",
            "\n",
            "Perplexity on the validation set:  160.6149367661081 \n",
            "\n",
            "*************************************************\n",
            "\n",
            "Epoch : 49\n",
            "batch no = 0 / 656, train loss = 4734.161, wps = 1, since beginning = 25 mins, cuda memory = 0.878 GBs\n",
            "batch no = 100 / 656, train loss = 4527.088, wps = 1, since beginning = 25 mins, cuda memory = 0.878 GBs\n",
            "batch no = 200 / 656, train loss = 4957.647, wps = 1, since beginning = 25 mins, cuda memory = 0.878 GBs\n",
            "batch no = 300 / 656, train loss = 4933.505, wps = 1, since beginning = 25 mins, cuda memory = 0.878 GBs\n",
            "batch no = 400 / 656, train loss = 4483.548, wps = 1, since beginning = 25 mins, cuda memory = 0.878 GBs\n",
            "batch no = 500 / 656, train loss = 4964.847, wps = 1, since beginning = 25 mins, cuda memory = 0.878 GBs\n",
            "batch no = 600 / 656, train loss = 4811.821, wps = 1, since beginning = 25 mins, cuda memory = 0.878 GBs\n",
            "CE on the training set:  3.5212643604439053 \n",
            "\n",
            "Perplexity on the training set:  33.827171174221256 \n",
            "\n",
            "CE on the validation set:  5.086681032281046 \n",
            "\n",
            "Perplexity on the validation set:  161.85178876132267 \n",
            "\n",
            "*************************************************\n",
            "\n",
            "Epoch : 50\n",
            "batch no = 0 / 656, train loss = 4198.944, wps = 1, since beginning = 25 mins, cuda memory = 0.878 GBs\n",
            "batch no = 100 / 656, train loss = 5101.787, wps = 1, since beginning = 25 mins, cuda memory = 0.878 GBs\n",
            "batch no = 200 / 656, train loss = 4359.460, wps = 1, since beginning = 25 mins, cuda memory = 0.878 GBs\n",
            "batch no = 300 / 656, train loss = 4118.292, wps = 1, since beginning = 25 mins, cuda memory = 0.878 GBs\n",
            "batch no = 400 / 656, train loss = 4557.277, wps = 1, since beginning = 25 mins, cuda memory = 0.878 GBs\n",
            "batch no = 500 / 656, train loss = 4387.044, wps = 1, since beginning = 25 mins, cuda memory = 0.878 GBs\n",
            "batch no = 600 / 656, train loss = 4712.750, wps = 1, since beginning = 26 mins, cuda memory = 0.878 GBs\n",
            "CE on the training set:  3.498154285657723 \n",
            "\n",
            "Perplexity on the training set:  33.05438666594022 \n",
            "\n",
            "CE on the validation set:  5.095749764093733 \n",
            "\n",
            "Perplexity on the validation set:  163.3262548907229 \n",
            "\n",
            "*************************************************\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we plot both the train and the valid loss. The gap between the two already becomes very marked after only ten epochs so the overfitting is more than tangible."
      ],
      "metadata": {
        "id": "P8w5AN9De34V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(num = 3, figsize=(8, 5)).patch.set_facecolor('white')\n",
        "plt.title('Baseline: Train and Valid Losses')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "\n",
        "# Plot train loss over the sampled epochs\n",
        "plt.plot(sampled_BaseEpochs, loss_BaseTrain, label='Train loss')\n",
        "\n",
        "# Plot valid loss over the sampled epochs\n",
        "plt.plot(sampled_BaseEpochs, loss_BaseValid, label='Valid loss')\n",
        "\n",
        "# Add legend to the plot\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "d5f2179e-3ead-4eb1-af8d-6a5c4a47307f",
        "id": "bUFvTyoRzAgR"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We perform 4 runs and consider the average result. In this way it should be more reliable."
      ],
      "metadata": {
        "id": "pHAhSBLS-4GH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "runs = 4\n",
        "loss_runs = []\n",
        "perp_runs = []\n",
        "\n",
        "for r in range(1, runs+1):\n",
        "    model = BaseModel(vocab_size, embed_size_reg, hidden_size_reg, layer_num_reg, pad_index=PAD_TOKEN).to(device)\n",
        "    model.apply(init_weights)\n",
        "\n",
        "    # Optimizers   \n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr_reg)\n",
        "    # Loss criterion\n",
        "    loss_function = nn.CrossEntropyLoss(ignore_index = PAD_TOKEN, reduction = 'sum')\n",
        "    # Start a timer\n",
        "    tic = timeit.default_timer() \n",
        "\n",
        "    n_epochs = 50\n",
        "\n",
        "    best_val = 1e10\n",
        "\n",
        "    # Loop over the number of runs\n",
        "    for x in range(1,n_epochs+1):\n",
        "        print(\"Run : {:d},\".format(r), end=' ')\n",
        "        print(\"Epoch : {:d}\".format(x))\n",
        "\n",
        "        _, words_train, loss_train = BaseTrain(train_loader, model, optimizer, loss_function, tic)\n",
        "\n",
        "        if x % 4 == 0:\n",
        "            loss_valid, words_valid = BaseEvaluate(valid_loader, loss_function, model)\n",
        "            ce_valid = np.asarray(loss_valid).sum() / np.asarray(words_valid).sum()\n",
        "\n",
        "            if ce_valid < best_val:\n",
        "              best_val = ce_valid\n",
        "            else:\n",
        "              break\n",
        "\n",
        "    \"\"\" Cross entropy loss and perplexity on the test set \"\"\"\n",
        "    loss_test, words_test = BaseEvaluate(test_loader, loss_function, model)\n",
        "\n",
        "    # Cross entropy\n",
        "    ce_test = np.asarray(loss_test).sum() / np.asarray(words_test).sum()\n",
        "    loss_runs.append(ce_test)\n",
        "\n",
        "    # Perplexity\n",
        "    perplexity_test = np.exp(ce_test)\n",
        "    perp_runs.append(perplexity_test)\n",
        "\n",
        "loss_runs = np.asarray(loss_runs)\n",
        "perp_runs = np.asarray(perp_runs)\n",
        "\n",
        "# Computation of mean and standard deviation values among the results of the 4 runs\n",
        "print('Test loss', round(loss_runs.mean(),3), '+-', round(loss_runs.std(),3))\n",
        "print('Test perplexity', round(perp_runs.mean(), 3), '+-', round(perp_runs.std(), 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e7de1d3-ca2a-42f0-b244-82eb70eda86a",
        "id": "1fqOhEBY0UHd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run : 1, Epoch : 1\n",
            "batch no = 0 / 656, train loss = 12831.079, wps = 16393, since beginning = 0 mins, cuda memory = 0.878 GBs\n",
            "batch no = 100 / 656, train loss = 8616.235, wps = 281, since beginning = 0 mins, cuda memory = 0.926 GBs\n",
            "batch no = 200 / 656, train loss = 9026.781, wps = 159, since beginning = 0 mins, cuda memory = 0.926 GBs\n",
            "batch no = 300 / 656, train loss = 7473.620, wps = 89, since beginning = 0 mins, cuda memory = 0.926 GBs\n",
            "batch no = 400 / 656, train loss = 7345.008, wps = 68, since beginning = 0 mins, cuda memory = 0.937 GBs\n",
            "batch no = 500 / 656, train loss = 8610.490, wps = 64, since beginning = 0 mins, cuda memory = 0.937 GBs\n",
            "batch no = 600 / 656, train loss = 7620.543, wps = 47, since beginning = 0 mins, cuda memory = 0.937 GBs\n",
            "Run : 1, Epoch : 2\n",
            "batch no = 0 / 656, train loss = 6158.947, wps = 39, since beginning = 1 mins, cuda memory = 0.946 GBs\n",
            "batch no = 100 / 656, train loss = 7632.782, wps = 40, since beginning = 1 mins, cuda memory = 0.946 GBs\n",
            "batch no = 200 / 656, train loss = 6748.503, wps = 33, since beginning = 1 mins, cuda memory = 0.946 GBs\n",
            "batch no = 300 / 656, train loss = 7966.934, wps = 33, since beginning = 1 mins, cuda memory = 0.946 GBs\n",
            "batch no = 400 / 656, train loss = 7217.620, wps = 28, since beginning = 1 mins, cuda memory = 0.946 GBs\n",
            "batch no = 500 / 656, train loss = 6741.016, wps = 25, since beginning = 1 mins, cuda memory = 0.946 GBs\n",
            "batch no = 600 / 656, train loss = 6752.875, wps = 22, since beginning = 1 mins, cuda memory = 0.946 GBs\n",
            "Run : 1, Epoch : 3\n",
            "batch no = 0 / 656, train loss = 6699.022, wps = 22, since beginning = 1 mins, cuda memory = 0.946 GBs\n",
            "batch no = 100 / 656, train loss = 6653.438, wps = 20, since beginning = 1 mins, cuda memory = 0.946 GBs\n",
            "batch no = 200 / 656, train loss = 6684.329, wps = 20, since beginning = 1 mins, cuda memory = 0.946 GBs\n",
            "batch no = 300 / 656, train loss = 5953.931, wps = 17, since beginning = 1 mins, cuda memory = 0.946 GBs\n",
            "batch no = 400 / 656, train loss = 6389.989, wps = 17, since beginning = 1 mins, cuda memory = 0.946 GBs\n",
            "batch no = 500 / 656, train loss = 6007.659, wps = 15, since beginning = 1 mins, cuda memory = 0.946 GBs\n",
            "batch no = 600 / 656, train loss = 6536.165, wps = 15, since beginning = 2 mins, cuda memory = 0.946 GBs\n",
            "Run : 1, Epoch : 4\n",
            "batch no = 0 / 656, train loss = 6804.591, wps = 16, since beginning = 2 mins, cuda memory = 0.946 GBs\n",
            "batch no = 100 / 656, train loss = 6858.222, wps = 15, since beginning = 2 mins, cuda memory = 0.946 GBs\n",
            "batch no = 200 / 656, train loss = 5994.938, wps = 13, since beginning = 2 mins, cuda memory = 0.946 GBs\n",
            "batch no = 300 / 656, train loss = 6284.551, wps = 13, since beginning = 2 mins, cuda memory = 0.946 GBs\n",
            "batch no = 400 / 656, train loss = 6234.388, wps = 12, since beginning = 2 mins, cuda memory = 0.946 GBs\n",
            "batch no = 500 / 656, train loss = 6369.468, wps = 12, since beginning = 2 mins, cuda memory = 0.946 GBs\n",
            "batch no = 600 / 656, train loss = 5388.622, wps = 10, since beginning = 2 mins, cuda memory = 0.946 GBs\n",
            "Run : 1, Epoch : 5\n",
            "batch no = 0 / 656, train loss = 5578.856, wps = 11, since beginning = 2 mins, cuda memory = 0.946 GBs\n",
            "batch no = 100 / 656, train loss = 6159.440, wps = 11, since beginning = 2 mins, cuda memory = 0.946 GBs\n",
            "batch no = 200 / 656, train loss = 6067.838, wps = 10, since beginning = 2 mins, cuda memory = 0.946 GBs\n",
            "batch no = 300 / 656, train loss = 6291.465, wps = 10, since beginning = 2 mins, cuda memory = 0.946 GBs\n",
            "batch no = 400 / 656, train loss = 5774.850, wps = 9, since beginning = 2 mins, cuda memory = 0.946 GBs\n",
            "batch no = 500 / 656, train loss = 6286.800, wps = 10, since beginning = 2 mins, cuda memory = 0.946 GBs\n",
            "batch no = 600 / 656, train loss = 6124.299, wps = 9, since beginning = 3 mins, cuda memory = 0.946 GBs\n",
            "Run : 1, Epoch : 6\n",
            "batch no = 0 / 656, train loss = 5217.370, wps = 8, since beginning = 3 mins, cuda memory = 0.946 GBs\n",
            "batch no = 100 / 656, train loss = 5223.025, wps = 8, since beginning = 3 mins, cuda memory = 0.946 GBs\n",
            "batch no = 200 / 656, train loss = 5654.472, wps = 8, since beginning = 3 mins, cuda memory = 0.946 GBs\n",
            "batch no = 300 / 656, train loss = 5239.408, wps = 7, since beginning = 3 mins, cuda memory = 0.946 GBs\n",
            "batch no = 400 / 656, train loss = 5378.244, wps = 7, since beginning = 3 mins, cuda memory = 0.946 GBs\n",
            "batch no = 500 / 656, train loss = 5715.409, wps = 8, since beginning = 3 mins, cuda memory = 0.946 GBs\n",
            "batch no = 600 / 656, train loss = 5781.868, wps = 7, since beginning = 3 mins, cuda memory = 0.946 GBs\n",
            "Run : 1, Epoch : 7\n",
            "batch no = 0 / 656, train loss = 5482.487, wps = 7, since beginning = 3 mins, cuda memory = 0.946 GBs\n",
            "batch no = 100 / 656, train loss = 6490.325, wps = 8, since beginning = 3 mins, cuda memory = 0.946 GBs\n",
            "batch no = 200 / 656, train loss = 5227.848, wps = 6, since beginning = 3 mins, cuda memory = 0.946 GBs\n",
            "batch no = 300 / 656, train loss = 5748.312, wps = 7, since beginning = 3 mins, cuda memory = 0.946 GBs\n",
            "batch no = 400 / 656, train loss = 4972.292, wps = 6, since beginning = 3 mins, cuda memory = 0.946 GBs\n",
            "batch no = 500 / 656, train loss = 6059.107, wps = 7, since beginning = 4 mins, cuda memory = 0.946 GBs\n",
            "batch no = 600 / 656, train loss = 5793.720, wps = 6, since beginning = 4 mins, cuda memory = 0.946 GBs\n",
            "Run : 1, Epoch : 8\n",
            "batch no = 0 / 656, train loss = 5179.226, wps = 6, since beginning = 4 mins, cuda memory = 0.946 GBs\n",
            "batch no = 100 / 656, train loss = 5503.190, wps = 6, since beginning = 4 mins, cuda memory = 0.946 GBs\n",
            "batch no = 200 / 656, train loss = 4915.246, wps = 6, since beginning = 4 mins, cuda memory = 0.946 GBs\n",
            "batch no = 300 / 656, train loss = 5771.499, wps = 6, since beginning = 4 mins, cuda memory = 0.946 GBs\n",
            "batch no = 400 / 656, train loss = 4919.741, wps = 5, since beginning = 4 mins, cuda memory = 0.946 GBs\n",
            "batch no = 500 / 656, train loss = 4904.841, wps = 6, since beginning = 4 mins, cuda memory = 0.946 GBs\n",
            "batch no = 600 / 656, train loss = 6081.615, wps = 6, since beginning = 4 mins, cuda memory = 0.946 GBs\n",
            "Run : 1, Epoch : 9\n",
            "batch no = 0 / 656, train loss = 5074.793, wps = 5, since beginning = 4 mins, cuda memory = 0.946 GBs\n",
            "batch no = 100 / 656, train loss = 4663.782, wps = 5, since beginning = 4 mins, cuda memory = 0.946 GBs\n",
            "batch no = 200 / 656, train loss = 5008.417, wps = 5, since beginning = 4 mins, cuda memory = 0.946 GBs\n",
            "batch no = 300 / 656, train loss = 4217.839, wps = 4, since beginning = 4 mins, cuda memory = 0.946 GBs\n",
            "batch no = 400 / 656, train loss = 5074.346, wps = 5, since beginning = 5 mins, cuda memory = 0.946 GBs\n",
            "batch no = 500 / 656, train loss = 5091.281, wps = 5, since beginning = 5 mins, cuda memory = 0.946 GBs\n",
            "batch no = 600 / 656, train loss = 5692.201, wps = 5, since beginning = 5 mins, cuda memory = 0.946 GBs\n",
            "Run : 1, Epoch : 10\n",
            "batch no = 0 / 656, train loss = 4517.182, wps = 5, since beginning = 5 mins, cuda memory = 0.946 GBs\n",
            "batch no = 100 / 656, train loss = 4595.040, wps = 5, since beginning = 5 mins, cuda memory = 0.946 GBs\n",
            "batch no = 200 / 656, train loss = 4323.892, wps = 4, since beginning = 5 mins, cuda memory = 0.946 GBs\n",
            "batch no = 300 / 656, train loss = 4981.348, wps = 5, since beginning = 5 mins, cuda memory = 0.946 GBs\n",
            "batch no = 400 / 656, train loss = 4526.072, wps = 4, since beginning = 5 mins, cuda memory = 0.946 GBs\n",
            "batch no = 500 / 656, train loss = 4828.189, wps = 4, since beginning = 5 mins, cuda memory = 0.946 GBs\n",
            "batch no = 600 / 656, train loss = 4775.798, wps = 4, since beginning = 5 mins, cuda memory = 0.946 GBs\n",
            "Run : 1, Epoch : 11\n",
            "batch no = 0 / 656, train loss = 4259.656, wps = 4, since beginning = 5 mins, cuda memory = 0.946 GBs\n",
            "batch no = 100 / 656, train loss = 4643.363, wps = 4, since beginning = 5 mins, cuda memory = 0.946 GBs\n",
            "batch no = 200 / 656, train loss = 4504.786, wps = 4, since beginning = 5 mins, cuda memory = 0.946 GBs\n",
            "batch no = 300 / 656, train loss = 4683.892, wps = 4, since beginning = 6 mins, cuda memory = 0.946 GBs\n",
            "batch no = 400 / 656, train loss = 4731.406, wps = 4, since beginning = 6 mins, cuda memory = 0.946 GBs\n",
            "batch no = 500 / 656, train loss = 4928.195, wps = 4, since beginning = 6 mins, cuda memory = 0.946 GBs\n",
            "batch no = 600 / 656, train loss = 4268.773, wps = 4, since beginning = 6 mins, cuda memory = 0.946 GBs\n",
            "Run : 1, Epoch : 12\n",
            "batch no = 0 / 656, train loss = 4466.489, wps = 4, since beginning = 6 mins, cuda memory = 0.946 GBs\n",
            "batch no = 100 / 656, train loss = 4431.290, wps = 4, since beginning = 6 mins, cuda memory = 0.946 GBs\n",
            "batch no = 200 / 656, train loss = 4540.512, wps = 4, since beginning = 6 mins, cuda memory = 0.946 GBs\n",
            "batch no = 300 / 656, train loss = 3841.757, wps = 3, since beginning = 6 mins, cuda memory = 0.946 GBs\n",
            "batch no = 400 / 656, train loss = 4911.819, wps = 4, since beginning = 6 mins, cuda memory = 0.946 GBs\n",
            "batch no = 500 / 656, train loss = 4510.099, wps = 4, since beginning = 6 mins, cuda memory = 0.946 GBs\n",
            "batch no = 600 / 656, train loss = 4347.580, wps = 3, since beginning = 6 mins, cuda memory = 0.946 GBs\n",
            "Run : 2, Epoch : 1\n",
            "batch no = 0 / 656, train loss = 12183.704, wps = 26382, since beginning = 0 mins, cuda memory = 0.946 GBs\n",
            "batch no = 100 / 656, train loss = 8622.946, wps = 277, since beginning = 0 mins, cuda memory = 0.946 GBs\n",
            "batch no = 200 / 656, train loss = 9616.823, wps = 160, since beginning = 0 mins, cuda memory = 1.016 GBs\n",
            "batch no = 300 / 656, train loss = 7574.438, wps = 89, since beginning = 0 mins, cuda memory = 1.016 GBs\n",
            "batch no = 400 / 656, train loss = 8053.763, wps = 73, since beginning = 0 mins, cuda memory = 1.016 GBs\n",
            "batch no = 500 / 656, train loss = 8020.759, wps = 58, since beginning = 0 mins, cuda memory = 1.016 GBs\n",
            "batch no = 600 / 656, train loss = 8124.217, wps = 50, since beginning = 0 mins, cuda memory = 1.016 GBs\n",
            "Run : 2, Epoch : 2\n",
            "batch no = 0 / 656, train loss = 6842.958, wps = 40, since beginning = 1 mins, cuda memory = 1.016 GBs\n",
            "batch no = 100 / 656, train loss = 7223.682, wps = 37, since beginning = 1 mins, cuda memory = 1.016 GBs\n",
            "batch no = 200 / 656, train loss = 7103.410, wps = 34, since beginning = 1 mins, cuda memory = 1.016 GBs\n",
            "batch no = 300 / 656, train loss = 6931.196, wps = 30, since beginning = 1 mins, cuda memory = 1.016 GBs\n",
            "batch no = 400 / 656, train loss = 6866.203, wps = 26, since beginning = 1 mins, cuda memory = 1.016 GBs\n",
            "batch no = 500 / 656, train loss = 6623.050, wps = 24, since beginning = 1 mins, cuda memory = 1.016 GBs\n",
            "batch no = 600 / 656, train loss = 7068.914, wps = 23, since beginning = 1 mins, cuda memory = 1.016 GBs\n",
            "Run : 2, Epoch : 3\n",
            "batch no = 0 / 656, train loss = 7114.207, wps = 23, since beginning = 1 mins, cuda memory = 1.016 GBs\n",
            "batch no = 100 / 656, train loss = 6796.547, wps = 20, since beginning = 1 mins, cuda memory = 1.016 GBs\n",
            "batch no = 200 / 656, train loss = 6833.859, wps = 19, since beginning = 1 mins, cuda memory = 1.016 GBs\n",
            "batch no = 300 / 656, train loss = 6342.726, wps = 17, since beginning = 1 mins, cuda memory = 1.016 GBs\n",
            "batch no = 400 / 656, train loss = 6079.816, wps = 15, since beginning = 1 mins, cuda memory = 1.016 GBs\n",
            "batch no = 500 / 656, train loss = 6545.700, wps = 16, since beginning = 1 mins, cuda memory = 1.016 GBs\n",
            "batch no = 600 / 656, train loss = 6347.781, wps = 14, since beginning = 2 mins, cuda memory = 1.016 GBs\n",
            "Run : 2, Epoch : 4\n",
            "batch no = 0 / 656, train loss = 6715.578, wps = 15, since beginning = 2 mins, cuda memory = 1.016 GBs\n",
            "batch no = 100 / 656, train loss = 6277.865, wps = 13, since beginning = 2 mins, cuda memory = 1.016 GBs\n",
            "batch no = 200 / 656, train loss = 6158.076, wps = 13, since beginning = 2 mins, cuda memory = 1.016 GBs\n",
            "batch no = 300 / 656, train loss = 5833.265, wps = 12, since beginning = 2 mins, cuda memory = 1.016 GBs\n",
            "batch no = 400 / 656, train loss = 6160.848, wps = 12, since beginning = 2 mins, cuda memory = 1.016 GBs\n",
            "batch no = 500 / 656, train loss = 6175.403, wps = 12, since beginning = 2 mins, cuda memory = 1.016 GBs\n",
            "batch no = 600 / 656, train loss = 6033.994, wps = 11, since beginning = 2 mins, cuda memory = 1.016 GBs\n",
            "Run : 2, Epoch : 5\n",
            "batch no = 0 / 656, train loss = 5873.378, wps = 11, since beginning = 2 mins, cuda memory = 1.016 GBs\n",
            "batch no = 100 / 656, train loss = 5795.758, wps = 10, since beginning = 2 mins, cuda memory = 1.016 GBs\n",
            "batch no = 200 / 656, train loss = 5593.102, wps = 10, since beginning = 2 mins, cuda memory = 1.016 GBs\n",
            "batch no = 300 / 656, train loss = 6217.939, wps = 10, since beginning = 2 mins, cuda memory = 1.016 GBs\n",
            "batch no = 400 / 656, train loss = 5053.120, wps = 8, since beginning = 2 mins, cuda memory = 1.016 GBs\n",
            "batch no = 500 / 656, train loss = 5246.501, wps = 8, since beginning = 3 mins, cuda memory = 1.016 GBs\n",
            "batch no = 600 / 656, train loss = 6114.448, wps = 9, since beginning = 3 mins, cuda memory = 1.016 GBs\n",
            "Run : 2, Epoch : 6\n",
            "batch no = 0 / 656, train loss = 6413.392, wps = 9, since beginning = 3 mins, cuda memory = 1.016 GBs\n",
            "batch no = 100 / 656, train loss = 5487.446, wps = 8, since beginning = 3 mins, cuda memory = 1.016 GBs\n",
            "batch no = 200 / 656, train loss = 5809.789, wps = 8, since beginning = 3 mins, cuda memory = 1.016 GBs\n",
            "batch no = 300 / 656, train loss = 5646.509, wps = 8, since beginning = 3 mins, cuda memory = 1.016 GBs\n",
            "batch no = 400 / 656, train loss = 5742.862, wps = 8, since beginning = 3 mins, cuda memory = 1.016 GBs\n",
            "batch no = 500 / 656, train loss = 5937.788, wps = 8, since beginning = 3 mins, cuda memory = 1.016 GBs\n",
            "batch no = 600 / 656, train loss = 5877.036, wps = 7, since beginning = 3 mins, cuda memory = 1.016 GBs\n",
            "Run : 2, Epoch : 7\n",
            "batch no = 0 / 656, train loss = 4988.572, wps = 7, since beginning = 3 mins, cuda memory = 1.016 GBs\n",
            "batch no = 100 / 656, train loss = 5145.478, wps = 7, since beginning = 3 mins, cuda memory = 1.016 GBs\n",
            "batch no = 200 / 656, train loss = 5321.879, wps = 7, since beginning = 3 mins, cuda memory = 1.016 GBs\n",
            "batch no = 300 / 656, train loss = 5720.396, wps = 7, since beginning = 3 mins, cuda memory = 1.016 GBs\n",
            "batch no = 400 / 656, train loss = 6076.518, wps = 7, since beginning = 4 mins, cuda memory = 1.016 GBs\n",
            "batch no = 500 / 656, train loss = 5143.011, wps = 6, since beginning = 4 mins, cuda memory = 1.016 GBs\n",
            "batch no = 600 / 656, train loss = 5677.885, wps = 6, since beginning = 4 mins, cuda memory = 1.016 GBs\n",
            "Run : 2, Epoch : 8\n",
            "batch no = 0 / 656, train loss = 4938.197, wps = 6, since beginning = 4 mins, cuda memory = 1.016 GBs\n",
            "batch no = 100 / 656, train loss = 4350.607, wps = 5, since beginning = 4 mins, cuda memory = 1.016 GBs\n",
            "batch no = 200 / 656, train loss = 4942.329, wps = 6, since beginning = 4 mins, cuda memory = 1.016 GBs\n",
            "batch no = 300 / 656, train loss = 5445.652, wps = 6, since beginning = 4 mins, cuda memory = 1.016 GBs\n",
            "batch no = 400 / 656, train loss = 5077.884, wps = 5, since beginning = 4 mins, cuda memory = 1.016 GBs\n",
            "batch no = 500 / 656, train loss = 5076.739, wps = 5, since beginning = 4 mins, cuda memory = 1.016 GBs\n",
            "batch no = 600 / 656, train loss = 4890.491, wps = 5, since beginning = 4 mins, cuda memory = 1.016 GBs\n",
            "Run : 2, Epoch : 9\n",
            "batch no = 0 / 656, train loss = 4594.191, wps = 5, since beginning = 4 mins, cuda memory = 1.016 GBs\n",
            "batch no = 100 / 656, train loss = 4619.529, wps = 5, since beginning = 4 mins, cuda memory = 1.016 GBs\n",
            "batch no = 200 / 656, train loss = 5854.836, wps = 6, since beginning = 4 mins, cuda memory = 1.016 GBs\n",
            "batch no = 300 / 656, train loss = 4905.142, wps = 5, since beginning = 5 mins, cuda memory = 1.016 GBs\n",
            "batch no = 400 / 656, train loss = 4701.315, wps = 5, since beginning = 5 mins, cuda memory = 1.016 GBs\n",
            "batch no = 500 / 656, train loss = 5027.317, wps = 5, since beginning = 5 mins, cuda memory = 1.016 GBs\n",
            "batch no = 600 / 656, train loss = 5210.121, wps = 5, since beginning = 5 mins, cuda memory = 1.016 GBs\n",
            "Run : 2, Epoch : 10\n",
            "batch no = 0 / 656, train loss = 4512.840, wps = 4, since beginning = 5 mins, cuda memory = 1.016 GBs\n",
            "batch no = 100 / 656, train loss = 4688.703, wps = 4, since beginning = 5 mins, cuda memory = 1.016 GBs\n",
            "batch no = 200 / 656, train loss = 4889.846, wps = 5, since beginning = 5 mins, cuda memory = 1.016 GBs\n",
            "batch no = 300 / 656, train loss = 4916.234, wps = 5, since beginning = 5 mins, cuda memory = 1.016 GBs\n",
            "batch no = 400 / 656, train loss = 4431.141, wps = 4, since beginning = 5 mins, cuda memory = 1.016 GBs\n",
            "batch no = 500 / 656, train loss = 4851.786, wps = 4, since beginning = 5 mins, cuda memory = 1.016 GBs\n",
            "batch no = 600 / 656, train loss = 4902.451, wps = 4, since beginning = 5 mins, cuda memory = 1.016 GBs\n",
            "Run : 2, Epoch : 11\n",
            "batch no = 0 / 656, train loss = 4471.020, wps = 4, since beginning = 5 mins, cuda memory = 1.016 GBs\n",
            "batch no = 100 / 656, train loss = 4420.750, wps = 4, since beginning = 5 mins, cuda memory = 1.016 GBs\n",
            "batch no = 200 / 656, train loss = 4783.165, wps = 4, since beginning = 5 mins, cuda memory = 1.016 GBs\n",
            "batch no = 300 / 656, train loss = 4643.373, wps = 4, since beginning = 6 mins, cuda memory = 1.016 GBs\n",
            "batch no = 400 / 656, train loss = 4723.770, wps = 4, since beginning = 6 mins, cuda memory = 1.016 GBs\n",
            "batch no = 500 / 656, train loss = 5240.271, wps = 4, since beginning = 6 mins, cuda memory = 1.016 GBs\n",
            "batch no = 600 / 656, train loss = 4845.526, wps = 4, since beginning = 6 mins, cuda memory = 1.016 GBs\n",
            "Run : 2, Epoch : 12\n",
            "batch no = 0 / 656, train loss = 4165.084, wps = 4, since beginning = 6 mins, cuda memory = 1.016 GBs\n",
            "batch no = 100 / 656, train loss = 3942.500, wps = 4, since beginning = 6 mins, cuda memory = 1.016 GBs\n",
            "batch no = 200 / 656, train loss = 4240.743, wps = 4, since beginning = 6 mins, cuda memory = 1.016 GBs\n",
            "batch no = 300 / 656, train loss = 4772.991, wps = 4, since beginning = 6 mins, cuda memory = 1.016 GBs\n",
            "batch no = 400 / 656, train loss = 4169.135, wps = 3, since beginning = 6 mins, cuda memory = 1.016 GBs\n",
            "batch no = 500 / 656, train loss = 4460.514, wps = 3, since beginning = 6 mins, cuda memory = 1.016 GBs\n",
            "batch no = 600 / 656, train loss = 4560.237, wps = 4, since beginning = 6 mins, cuda memory = 1.016 GBs\n",
            "Run : 3, Epoch : 1\n",
            "batch no = 0 / 656, train loss = 12350.529, wps = 28843, since beginning = 0 mins, cuda memory = 1.016 GBs\n",
            "batch no = 100 / 656, train loss = 8909.884, wps = 289, since beginning = 0 mins, cuda memory = 1.016 GBs\n",
            "batch no = 200 / 656, train loss = 8340.000, wps = 144, since beginning = 0 mins, cuda memory = 1.016 GBs\n",
            "batch no = 300 / 656, train loss = 7631.865, wps = 87, since beginning = 0 mins, cuda memory = 1.016 GBs\n",
            "batch no = 400 / 656, train loss = 7650.172, wps = 72, since beginning = 0 mins, cuda memory = 1.017 GBs\n",
            "batch no = 500 / 656, train loss = 7314.384, wps = 55, since beginning = 0 mins, cuda memory = 1.017 GBs\n",
            "batch no = 600 / 656, train loss = 7665.194, wps = 47, since beginning = 0 mins, cuda memory = 1.017 GBs\n",
            "Run : 3, Epoch : 2\n",
            "batch no = 0 / 656, train loss = 7044.559, wps = 42, since beginning = 1 mins, cuda memory = 1.017 GBs\n",
            "batch no = 100 / 656, train loss = 6310.307, wps = 34, since beginning = 1 mins, cuda memory = 1.017 GBs\n",
            "batch no = 200 / 656, train loss = 7467.254, wps = 35, since beginning = 1 mins, cuda memory = 1.017 GBs\n",
            "batch no = 300 / 656, train loss = 7378.746, wps = 30, since beginning = 1 mins, cuda memory = 1.017 GBs\n",
            "batch no = 400 / 656, train loss = 7126.152, wps = 27, since beginning = 1 mins, cuda memory = 1.017 GBs\n",
            "batch no = 500 / 656, train loss = 6956.475, wps = 25, since beginning = 1 mins, cuda memory = 1.017 GBs\n",
            "batch no = 600 / 656, train loss = 6940.256, wps = 21, since beginning = 1 mins, cuda memory = 1.017 GBs\n",
            "Run : 3, Epoch : 3\n",
            "batch no = 0 / 656, train loss = 6761.300, wps = 22, since beginning = 1 mins, cuda memory = 1.017 GBs\n",
            "batch no = 100 / 656, train loss = 6738.696, wps = 21, since beginning = 1 mins, cuda memory = 1.017 GBs\n",
            "batch no = 200 / 656, train loss = 6409.795, wps = 18, since beginning = 1 mins, cuda memory = 1.017 GBs\n",
            "batch no = 300 / 656, train loss = 6547.217, wps = 17, since beginning = 1 mins, cuda memory = 1.017 GBs\n",
            "batch no = 400 / 656, train loss = 6579.981, wps = 16, since beginning = 1 mins, cuda memory = 1.017 GBs\n",
            "batch no = 500 / 656, train loss = 7012.123, wps = 16, since beginning = 1 mins, cuda memory = 1.017 GBs\n",
            "batch no = 600 / 656, train loss = 6008.369, wps = 14, since beginning = 2 mins, cuda memory = 1.017 GBs\n",
            "Run : 3, Epoch : 4\n",
            "batch no = 0 / 656, train loss = 6477.942, wps = 14, since beginning = 2 mins, cuda memory = 1.017 GBs\n",
            "batch no = 100 / 656, train loss = 6041.194, wps = 13, since beginning = 2 mins, cuda memory = 1.017 GBs\n",
            "batch no = 200 / 656, train loss = 6196.448, wps = 13, since beginning = 2 mins, cuda memory = 1.017 GBs\n",
            "batch no = 300 / 656, train loss = 5781.637, wps = 12, since beginning = 2 mins, cuda memory = 1.017 GBs\n",
            "batch no = 400 / 656, train loss = 7033.611, wps = 13, since beginning = 2 mins, cuda memory = 1.017 GBs\n",
            "batch no = 500 / 656, train loss = 6523.407, wps = 12, since beginning = 2 mins, cuda memory = 1.017 GBs\n",
            "batch no = 600 / 656, train loss = 6065.928, wps = 10, since beginning = 2 mins, cuda memory = 1.017 GBs\n",
            "Run : 3, Epoch : 5\n",
            "batch no = 0 / 656, train loss = 6286.375, wps = 11, since beginning = 2 mins, cuda memory = 1.017 GBs\n",
            "batch no = 100 / 656, train loss = 6100.474, wps = 10, since beginning = 2 mins, cuda memory = 1.017 GBs\n",
            "batch no = 200 / 656, train loss = 5516.883, wps = 10, since beginning = 2 mins, cuda memory = 1.017 GBs\n",
            "batch no = 300 / 656, train loss = 5937.554, wps = 10, since beginning = 2 mins, cuda memory = 1.017 GBs\n",
            "batch no = 400 / 656, train loss = 6656.221, wps = 10, since beginning = 2 mins, cuda memory = 1.017 GBs\n",
            "batch no = 500 / 656, train loss = 5341.425, wps = 9, since beginning = 3 mins, cuda memory = 1.017 GBs\n",
            "batch no = 600 / 656, train loss = 5766.467, wps = 9, since beginning = 3 mins, cuda memory = 1.017 GBs\n",
            "Run : 3, Epoch : 6\n",
            "batch no = 0 / 656, train loss = 5725.914, wps = 8, since beginning = 3 mins, cuda memory = 1.017 GBs\n",
            "batch no = 100 / 656, train loss = 5867.999, wps = 9, since beginning = 3 mins, cuda memory = 1.017 GBs\n",
            "batch no = 200 / 656, train loss = 6107.804, wps = 9, since beginning = 3 mins, cuda memory = 1.017 GBs\n",
            "batch no = 300 / 656, train loss = 5722.975, wps = 8, since beginning = 3 mins, cuda memory = 1.017 GBs\n",
            "batch no = 400 / 656, train loss = 5699.866, wps = 8, since beginning = 3 mins, cuda memory = 1.017 GBs\n",
            "batch no = 500 / 656, train loss = 6350.974, wps = 8, since beginning = 3 mins, cuda memory = 1.017 GBs\n",
            "batch no = 600 / 656, train loss = 5613.939, wps = 7, since beginning = 3 mins, cuda memory = 1.017 GBs\n",
            "Run : 3, Epoch : 7\n",
            "batch no = 0 / 656, train loss = 5365.523, wps = 7, since beginning = 3 mins, cuda memory = 1.017 GBs\n",
            "batch no = 100 / 656, train loss = 5725.412, wps = 7, since beginning = 3 mins, cuda memory = 1.017 GBs\n",
            "batch no = 200 / 656, train loss = 5583.808, wps = 7, since beginning = 3 mins, cuda memory = 1.017 GBs\n",
            "batch no = 300 / 656, train loss = 5960.131, wps = 7, since beginning = 3 mins, cuda memory = 1.017 GBs\n",
            "batch no = 400 / 656, train loss = 6548.713, wps = 7, since beginning = 4 mins, cuda memory = 1.017 GBs\n",
            "batch no = 500 / 656, train loss = 5289.895, wps = 6, since beginning = 4 mins, cuda memory = 1.017 GBs\n",
            "batch no = 600 / 656, train loss = 5773.692, wps = 7, since beginning = 4 mins, cuda memory = 1.017 GBs\n",
            "Run : 3, Epoch : 8\n",
            "batch no = 0 / 656, train loss = 4865.346, wps = 6, since beginning = 4 mins, cuda memory = 1.017 GBs\n",
            "batch no = 100 / 656, train loss = 5261.366, wps = 6, since beginning = 4 mins, cuda memory = 1.017 GBs\n",
            "batch no = 200 / 656, train loss = 5169.997, wps = 6, since beginning = 4 mins, cuda memory = 1.017 GBs\n",
            "batch no = 300 / 656, train loss = 5173.572, wps = 6, since beginning = 4 mins, cuda memory = 1.017 GBs\n",
            "batch no = 400 / 656, train loss = 5071.240, wps = 5, since beginning = 4 mins, cuda memory = 1.017 GBs\n",
            "batch no = 500 / 656, train loss = 5409.139, wps = 6, since beginning = 4 mins, cuda memory = 1.017 GBs\n",
            "batch no = 600 / 656, train loss = 5642.022, wps = 5, since beginning = 4 mins, cuda memory = 1.017 GBs\n",
            "Run : 3, Epoch : 9\n",
            "batch no = 0 / 656, train loss = 5341.179, wps = 5, since beginning = 4 mins, cuda memory = 1.017 GBs\n",
            "batch no = 100 / 656, train loss = 4958.041, wps = 5, since beginning = 4 mins, cuda memory = 1.017 GBs\n",
            "batch no = 200 / 656, train loss = 5596.466, wps = 5, since beginning = 4 mins, cuda memory = 1.017 GBs\n",
            "batch no = 300 / 656, train loss = 5147.102, wps = 5, since beginning = 5 mins, cuda memory = 1.017 GBs\n",
            "batch no = 400 / 656, train loss = 4731.785, wps = 5, since beginning = 5 mins, cuda memory = 1.017 GBs\n",
            "batch no = 500 / 656, train loss = 5052.096, wps = 5, since beginning = 5 mins, cuda memory = 1.017 GBs\n",
            "batch no = 600 / 656, train loss = 4899.690, wps = 5, since beginning = 5 mins, cuda memory = 1.017 GBs\n",
            "Run : 3, Epoch : 10\n",
            "batch no = 0 / 656, train loss = 4021.468, wps = 4, since beginning = 5 mins, cuda memory = 1.017 GBs\n",
            "batch no = 100 / 656, train loss = 5163.286, wps = 5, since beginning = 5 mins, cuda memory = 1.017 GBs\n",
            "batch no = 200 / 656, train loss = 5500.205, wps = 5, since beginning = 5 mins, cuda memory = 1.017 GBs\n",
            "batch no = 300 / 656, train loss = 4237.979, wps = 4, since beginning = 5 mins, cuda memory = 1.017 GBs\n",
            "batch no = 400 / 656, train loss = 4441.948, wps = 4, since beginning = 5 mins, cuda memory = 1.017 GBs\n",
            "batch no = 500 / 656, train loss = 5163.240, wps = 5, since beginning = 5 mins, cuda memory = 1.017 GBs\n",
            "batch no = 600 / 656, train loss = 5100.416, wps = 4, since beginning = 5 mins, cuda memory = 1.017 GBs\n",
            "Run : 3, Epoch : 11\n",
            "batch no = 0 / 656, train loss = 4007.016, wps = 4, since beginning = 5 mins, cuda memory = 1.017 GBs\n",
            "batch no = 100 / 656, train loss = 4796.692, wps = 4, since beginning = 5 mins, cuda memory = 1.017 GBs\n",
            "batch no = 200 / 656, train loss = 4975.094, wps = 5, since beginning = 5 mins, cuda memory = 1.017 GBs\n",
            "batch no = 300 / 656, train loss = 5370.907, wps = 5, since beginning = 6 mins, cuda memory = 1.017 GBs\n",
            "batch no = 400 / 656, train loss = 5213.435, wps = 4, since beginning = 6 mins, cuda memory = 1.017 GBs\n",
            "batch no = 500 / 656, train loss = 5074.547, wps = 4, since beginning = 6 mins, cuda memory = 1.017 GBs\n",
            "batch no = 600 / 656, train loss = 4779.880, wps = 4, since beginning = 6 mins, cuda memory = 1.017 GBs\n",
            "Run : 3, Epoch : 12\n",
            "batch no = 0 / 656, train loss = 4357.166, wps = 4, since beginning = 6 mins, cuda memory = 1.017 GBs\n",
            "batch no = 100 / 656, train loss = 4255.301, wps = 4, since beginning = 6 mins, cuda memory = 1.017 GBs\n",
            "batch no = 200 / 656, train loss = 4488.493, wps = 4, since beginning = 6 mins, cuda memory = 1.017 GBs\n",
            "batch no = 300 / 656, train loss = 4384.999, wps = 4, since beginning = 6 mins, cuda memory = 1.017 GBs\n",
            "batch no = 400 / 656, train loss = 4931.104, wps = 4, since beginning = 6 mins, cuda memory = 1.017 GBs\n",
            "batch no = 500 / 656, train loss = 4582.248, wps = 4, since beginning = 6 mins, cuda memory = 1.017 GBs\n",
            "batch no = 600 / 656, train loss = 4698.074, wps = 4, since beginning = 6 mins, cuda memory = 1.017 GBs\n",
            "Run : 4, Epoch : 1\n",
            "batch no = 0 / 656, train loss = 13079.271, wps = 27078, since beginning = 0 mins, cuda memory = 1.017 GBs\n",
            "batch no = 100 / 656, train loss = 8323.985, wps = 267, since beginning = 0 mins, cuda memory = 1.017 GBs\n",
            "batch no = 200 / 656, train loss = 8495.355, wps = 141, since beginning = 0 mins, cuda memory = 1.017 GBs\n",
            "batch no = 300 / 656, train loss = 8175.393, wps = 96, since beginning = 0 mins, cuda memory = 1.017 GBs\n",
            "batch no = 400 / 656, train loss = 8677.413, wps = 77, since beginning = 0 mins, cuda memory = 1.017 GBs\n",
            "batch no = 500 / 656, train loss = 7487.022, wps = 53, since beginning = 0 mins, cuda memory = 1.017 GBs\n",
            "batch no = 600 / 656, train loss = 6336.030, wps = 41, since beginning = 0 mins, cuda memory = 1.017 GBs\n",
            "Run : 4, Epoch : 2\n",
            "batch no = 0 / 656, train loss = 7375.263, wps = 43, since beginning = 1 mins, cuda memory = 1.017 GBs\n",
            "batch no = 100 / 656, train loss = 6679.803, wps = 35, since beginning = 1 mins, cuda memory = 1.017 GBs\n",
            "batch no = 200 / 656, train loss = 7601.650, wps = 35, since beginning = 1 mins, cuda memory = 1.017 GBs\n",
            "batch no = 300 / 656, train loss = 7635.768, wps = 31, since beginning = 1 mins, cuda memory = 1.017 GBs\n",
            "batch no = 400 / 656, train loss = 7102.416, wps = 26, since beginning = 1 mins, cuda memory = 1.017 GBs\n",
            "batch no = 500 / 656, train loss = 7406.381, wps = 26, since beginning = 1 mins, cuda memory = 1.017 GBs\n",
            "batch no = 600 / 656, train loss = 6864.937, wps = 24, since beginning = 1 mins, cuda memory = 1.017 GBs\n",
            "Run : 4, Epoch : 3\n",
            "batch no = 0 / 656, train loss = 5878.043, wps = 20, since beginning = 1 mins, cuda memory = 1.017 GBs\n",
            "batch no = 100 / 656, train loss = 6216.922, wps = 18, since beginning = 1 mins, cuda memory = 1.017 GBs\n",
            "batch no = 200 / 656, train loss = 7169.504, wps = 20, since beginning = 1 mins, cuda memory = 1.017 GBs\n",
            "batch no = 300 / 656, train loss = 6736.350, wps = 17, since beginning = 1 mins, cuda memory = 1.017 GBs\n",
            "batch no = 400 / 656, train loss = 6385.420, wps = 16, since beginning = 1 mins, cuda memory = 1.017 GBs\n",
            "batch no = 500 / 656, train loss = 6414.410, wps = 15, since beginning = 1 mins, cuda memory = 1.017 GBs\n",
            "batch no = 600 / 656, train loss = 6141.838, wps = 15, since beginning = 2 mins, cuda memory = 1.017 GBs\n",
            "Run : 4, Epoch : 4\n",
            "batch no = 0 / 656, train loss = 6956.929, wps = 16, since beginning = 2 mins, cuda memory = 1.017 GBs\n",
            "batch no = 100 / 656, train loss = 5382.694, wps = 12, since beginning = 2 mins, cuda memory = 1.017 GBs\n",
            "batch no = 200 / 656, train loss = 6021.815, wps = 12, since beginning = 2 mins, cuda memory = 1.017 GBs\n",
            "batch no = 300 / 656, train loss = 5253.003, wps = 11, since beginning = 2 mins, cuda memory = 1.017 GBs\n",
            "batch no = 400 / 656, train loss = 5861.895, wps = 11, since beginning = 2 mins, cuda memory = 1.017 GBs\n",
            "batch no = 500 / 656, train loss = 5665.859, wps = 11, since beginning = 2 mins, cuda memory = 1.017 GBs\n",
            "batch no = 600 / 656, train loss = 5676.414, wps = 10, since beginning = 2 mins, cuda memory = 1.017 GBs\n",
            "Run : 4, Epoch : 5\n",
            "batch no = 0 / 656, train loss = 5825.615, wps = 11, since beginning = 2 mins, cuda memory = 1.017 GBs\n",
            "batch no = 100 / 656, train loss = 6158.395, wps = 10, since beginning = 2 mins, cuda memory = 1.017 GBs\n",
            "batch no = 200 / 656, train loss = 5522.461, wps = 9, since beginning = 2 mins, cuda memory = 1.017 GBs\n",
            "batch no = 300 / 656, train loss = 5893.744, wps = 10, since beginning = 2 mins, cuda memory = 1.017 GBs\n",
            "batch no = 400 / 656, train loss = 5579.503, wps = 9, since beginning = 2 mins, cuda memory = 1.017 GBs\n",
            "batch no = 500 / 656, train loss = 6673.486, wps = 10, since beginning = 3 mins, cuda memory = 1.017 GBs\n",
            "batch no = 600 / 656, train loss = 5644.101, wps = 8, since beginning = 3 mins, cuda memory = 1.017 GBs\n",
            "Run : 4, Epoch : 6\n",
            "batch no = 0 / 656, train loss = 5056.778, wps = 8, since beginning = 3 mins, cuda memory = 1.017 GBs\n",
            "batch no = 100 / 656, train loss = 5952.448, wps = 9, since beginning = 3 mins, cuda memory = 1.017 GBs\n",
            "batch no = 200 / 656, train loss = 6037.600, wps = 8, since beginning = 3 mins, cuda memory = 1.017 GBs\n",
            "batch no = 300 / 656, train loss = 5874.711, wps = 8, since beginning = 3 mins, cuda memory = 1.017 GBs\n",
            "batch no = 400 / 656, train loss = 5404.458, wps = 7, since beginning = 3 mins, cuda memory = 1.017 GBs\n",
            "batch no = 500 / 656, train loss = 5577.025, wps = 8, since beginning = 3 mins, cuda memory = 1.017 GBs\n",
            "batch no = 600 / 656, train loss = 5463.955, wps = 7, since beginning = 3 mins, cuda memory = 1.017 GBs\n",
            "Run : 4, Epoch : 7\n",
            "batch no = 0 / 656, train loss = 5198.169, wps = 7, since beginning = 3 mins, cuda memory = 1.017 GBs\n",
            "batch no = 100 / 656, train loss = 5248.004, wps = 7, since beginning = 3 mins, cuda memory = 1.017 GBs\n",
            "batch no = 200 / 656, train loss = 5099.229, wps = 6, since beginning = 3 mins, cuda memory = 1.017 GBs\n",
            "batch no = 300 / 656, train loss = 5469.117, wps = 7, since beginning = 3 mins, cuda memory = 1.017 GBs\n",
            "batch no = 400 / 656, train loss = 5583.684, wps = 7, since beginning = 4 mins, cuda memory = 1.017 GBs\n",
            "batch no = 500 / 656, train loss = 5424.763, wps = 6, since beginning = 4 mins, cuda memory = 1.017 GBs\n",
            "batch no = 600 / 656, train loss = 5494.456, wps = 6, since beginning = 4 mins, cuda memory = 1.017 GBs\n",
            "Run : 4, Epoch : 8\n",
            "batch no = 0 / 656, train loss = 5174.028, wps = 6, since beginning = 4 mins, cuda memory = 1.017 GBs\n",
            "batch no = 100 / 656, train loss = 5984.305, wps = 6, since beginning = 4 mins, cuda memory = 1.017 GBs\n",
            "batch no = 200 / 656, train loss = 4320.544, wps = 5, since beginning = 4 mins, cuda memory = 1.017 GBs\n",
            "batch no = 300 / 656, train loss = 4669.269, wps = 5, since beginning = 4 mins, cuda memory = 1.017 GBs\n",
            "batch no = 400 / 656, train loss = 4695.512, wps = 5, since beginning = 4 mins, cuda memory = 1.017 GBs\n",
            "batch no = 500 / 656, train loss = 5451.171, wps = 6, since beginning = 4 mins, cuda memory = 1.017 GBs\n",
            "batch no = 600 / 656, train loss = 5433.815, wps = 5, since beginning = 4 mins, cuda memory = 1.017 GBs\n",
            "Run : 4, Epoch : 9\n",
            "batch no = 0 / 656, train loss = 4685.117, wps = 5, since beginning = 4 mins, cuda memory = 1.017 GBs\n",
            "batch no = 100 / 656, train loss = 4311.035, wps = 5, since beginning = 4 mins, cuda memory = 1.017 GBs\n",
            "batch no = 200 / 656, train loss = 5565.684, wps = 6, since beginning = 4 mins, cuda memory = 1.017 GBs\n",
            "batch no = 300 / 656, train loss = 5321.810, wps = 5, since beginning = 5 mins, cuda memory = 1.017 GBs\n",
            "batch no = 400 / 656, train loss = 4933.054, wps = 5, since beginning = 5 mins, cuda memory = 1.017 GBs\n",
            "batch no = 500 / 656, train loss = 5370.778, wps = 5, since beginning = 5 mins, cuda memory = 1.017 GBs\n",
            "batch no = 600 / 656, train loss = 5015.406, wps = 5, since beginning = 5 mins, cuda memory = 1.017 GBs\n",
            "Run : 4, Epoch : 10\n",
            "batch no = 0 / 656, train loss = 4497.047, wps = 5, since beginning = 5 mins, cuda memory = 1.017 GBs\n",
            "batch no = 100 / 656, train loss = 4670.881, wps = 5, since beginning = 5 mins, cuda memory = 1.017 GBs\n",
            "batch no = 200 / 656, train loss = 5906.593, wps = 5, since beginning = 5 mins, cuda memory = 1.017 GBs\n",
            "batch no = 300 / 656, train loss = 5253.200, wps = 5, since beginning = 5 mins, cuda memory = 1.017 GBs\n",
            "batch no = 400 / 656, train loss = 4618.322, wps = 4, since beginning = 5 mins, cuda memory = 1.017 GBs\n",
            "batch no = 500 / 656, train loss = 4872.776, wps = 5, since beginning = 5 mins, cuda memory = 1.017 GBs\n",
            "batch no = 600 / 656, train loss = 5132.811, wps = 4, since beginning = 5 mins, cuda memory = 1.017 GBs\n",
            "Run : 4, Epoch : 11\n",
            "batch no = 0 / 656, train loss = 4284.481, wps = 4, since beginning = 5 mins, cuda memory = 1.017 GBs\n",
            "batch no = 100 / 656, train loss = 4831.119, wps = 4, since beginning = 5 mins, cuda memory = 1.017 GBs\n",
            "batch no = 200 / 656, train loss = 4088.027, wps = 4, since beginning = 5 mins, cuda memory = 1.017 GBs\n",
            "batch no = 300 / 656, train loss = 4459.603, wps = 4, since beginning = 6 mins, cuda memory = 1.017 GBs\n",
            "batch no = 400 / 656, train loss = 4988.349, wps = 4, since beginning = 6 mins, cuda memory = 1.017 GBs\n",
            "batch no = 500 / 656, train loss = 4739.877, wps = 4, since beginning = 6 mins, cuda memory = 1.017 GBs\n",
            "batch no = 600 / 656, train loss = 4033.504, wps = 3, since beginning = 6 mins, cuda memory = 1.017 GBs\n",
            "Run : 4, Epoch : 12\n",
            "batch no = 0 / 656, train loss = 3903.062, wps = 4, since beginning = 6 mins, cuda memory = 1.017 GBs\n",
            "batch no = 100 / 656, train loss = 4196.844, wps = 4, since beginning = 6 mins, cuda memory = 1.017 GBs\n",
            "batch no = 200 / 656, train loss = 4842.661, wps = 4, since beginning = 6 mins, cuda memory = 1.017 GBs\n",
            "batch no = 300 / 656, train loss = 5303.909, wps = 4, since beginning = 6 mins, cuda memory = 1.017 GBs\n",
            "batch no = 400 / 656, train loss = 4962.172, wps = 4, since beginning = 6 mins, cuda memory = 1.017 GBs\n",
            "batch no = 500 / 656, train loss = 4334.475, wps = 3, since beginning = 6 mins, cuda memory = 1.017 GBs\n",
            "batch no = 600 / 656, train loss = 4438.261, wps = 4, since beginning = 6 mins, cuda memory = 1.017 GBs\n",
            "Test loss 4.892 +- 0.011\n",
            "Test perplexity 133.225 +- 1.407\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIYjnQvWWaVn"
      },
      "source": [
        "#**Neural Network, regularized**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLOzHyj0A26P"
      },
      "source": [
        "##**Instantiation of model**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eB3tjwjaU4zM"
      },
      "source": [
        "In this section we want to try to improve the values of the accuracy gained with perplexity, by adding some regularization tricks.\n",
        "Let's set again the specific parameters."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer_num_reg=1\n",
        "embed_size_reg=400\n",
        "hidden_size_reg=400\n",
        "w_drop=0.5\n",
        "dropout_i=0.4\n",
        "dropout_o=0.4\n",
        "dropout_e=0.1\n",
        "weight_decay=1.2e-6\n",
        "lr_reg=0.001\n",
        "max_grad_norm=0.25\n",
        "non_mono=5\n",
        "log=float(100)\n",
        "regularized = True\n",
        "winit = 0.1\n",
        "vocab_size = len(vocab.word2id)"
      ],
      "metadata": {
        "id": "RKAkfFT_kGz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "big6WbCQhzTB"
      },
      "source": [
        "##**Variational Dropout**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWkBb7Aj-rx3"
      },
      "source": [
        "Variational dropout is used for all dropout operations except those involving the hidden-to-hidden transition within an RNN, which are regularized with DropConnect. So it is used with reference to the inputs and outputs of the LSTM within a certain forward as well as backward pass. Unlike standard dropout, it uses the same dropout mask at every step, meaning that the same units are dropped at each step. This technique is based on the idea of applying dropout consistently across different layers, rather than randomly selecting units to drop out at each step.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class VariationalDropout(nn.Module):\n",
        "   \n",
        "    \"\"\"\n",
        "    A class for implementing Variational Dropout, a dropout technique that is able to learn the dropout rate\n",
        "    at each step of the training process. \n",
        "    \"\"\"\n",
        "   \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, input, dropout):\n",
        "   \n",
        "        \"\"\"\n",
        "        Applies the dropout operation to the input tensor. \n",
        "        :param input: The input tensor\n",
        "        :param dropout: The dropout rate (probability of setting a value to zero)\n",
        "        :return: The output tensor after applying dropout\n",
        "        \"\"\"\n",
        "   \n",
        "        if self.training:\n",
        "   \n",
        "            # Generate a mask of 0s and 1s with probability 1-dropout for 1s\n",
        "            mask = torch.empty(input.size(1), input.size(2), device=input.device).bernoulli_(1 - dropout)\n",
        "   \n",
        "            # Scale the mask by the reciprocal of 1-dropout to preserve the expected value of the tensor\n",
        "            mask = mask / (1 - dropout)\n",
        "   \n",
        "            # Apply the mask element-wise to the input tensor\n",
        "            output = input * mask\n",
        "        else:\n",
        "   \n",
        "            # During evaluation, return the input tensor without applying dropout\n",
        "            output = input\n",
        "        return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"VariationalDropout()\""
      ],
      "metadata": {
        "id": "SNQaSfQshRKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ct3Rf2yKfJoH"
      },
      "source": [
        "##**Weight-dropped LSTM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MlOk-DceYNK"
      },
      "source": [
        "The weight-dropped LSTM applies regularization through a DropConnect mask on the hidden-to-hidden recurrent weights without affecting the structure of the RNN itself.\n",
        "\n",
        "This dropout operation is applied to the weight matrices before the forward as well as the backward pass and remain the same for the entirety of these phases as the same weights are reused over multiple steps.\n",
        "\n",
        "It is used in order to prevent overfitting on the reccurent connections of the LSTMs.\n",
        "\n",
        "While DropConnect is used to regularize the hidden-to-hidden transition within an RNN, variational dropout is used for all other dropout operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1hXto-RsLsV"
      },
      "outputs": [],
      "source": [
        "class WeightDropLSTM(nn.Module):\n",
        "    \n",
        "    \"\"\"A PyTorch implementation of the WeightDrop LSTM.\n",
        "    \n",
        "    Args:\n",
        "        input_size (int): The number of features in the input\n",
        "        hidden_size (int): The number of features in the hidden state\n",
        "        weight_drop (float, optional): The dropout rate for the weights. Default: 0.0\n",
        "        \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, input_size, hidden_size, weight_drop = 0.0):\n",
        "        super().__init__()\n",
        "    \n",
        "        self.input_size = input_size\n",
        "    \n",
        "        self.hidden_size = hidden_size\n",
        "    \n",
        "        self.weight_drop = weight_drop\n",
        "    \n",
        "        self.module = nn.LSTM(input_size, hidden_size, num_layers = 1, bidirectional = False)\n",
        "    \n",
        "        self.weight_name_1 = 'weight_ih_l0'\n",
        "    \n",
        "        self.weight_name_2 = 'weight_hh_l0'\n",
        "        \n",
        "        # Get the weights for the input and hidden layers\n",
        "        w1 = getattr(self.module, self.weight_name_1)\n",
        "        \n",
        "        w2 = getattr(self.module, self.weight_name_2)\n",
        "        \n",
        "        # Create raw parameters for the weights and register them\n",
        "        self.register_parameter(f'{self.weight_name_1}_raw', nn.Parameter(w1.clone().detach()))\n",
        "    \n",
        "        raw_w1 = getattr(self, f'{self.weight_name_1}_raw')\n",
        "    \n",
        "        self.register_parameter(f'{self.weight_name_2}_raw', nn.Parameter(w2.clone().detach()))\n",
        "    \n",
        "        raw_w2 = getattr(self, f'{self.weight_name_2}_raw')\n",
        "        \n",
        "        # Initialize the weights\n",
        "        self.reset_parameters()\n",
        "        \n",
        "        # Apply weight dropout to the input and hidden layer weights\n",
        "        self.module._parameters[self.weight_name_1] = F.dropout(raw_w1, p=self.weight_drop, training=False)\n",
        "        \n",
        "        self.module._parameters[self.weight_name_2] = F.dropout(raw_w2, p=self.weight_drop, training=False)\n",
        "        \n",
        "    def reset_parameters(self):\n",
        "        \n",
        "        # Initialize the weights with a uniform distribution\n",
        "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
        "        \n",
        "        for param in self.parameters():\n",
        "        \n",
        "            nn.init.uniform_(param, -stdv, stdv)\n",
        "\n",
        "    def __repr__(self):\n",
        "        \n",
        "        # Return a string representation of the model\n",
        "        return \"WeightDropLSTM(input: {}, hidden: {}, weight drop: {})\".format(self.input_size, self.hidden_size, self.weight_drop)\n",
        "\n",
        "    def _setweights(self):\n",
        "       \n",
        "        # Set the weights for the input and hidden layers\n",
        "        raw_w1 = getattr(self, f'{self.weight_name_1}_raw')\n",
        "       \n",
        "        self.module._parameters[self.weight_name_1] = F.dropout(raw_w1, p=self.weight_drop, training=self.training)\n",
        "       \n",
        "        raw_w2 = getattr(self, f'{self.weight_name_2}_raw')\n",
        "       \n",
        "        self.module._parameters[self.weight_name_2] = F.dropout(raw_w2, p=self.weight_drop, training=self.training)\n",
        "\n",
        "    def forward(self, *args):\n",
        "        \n",
        "        # Compute the L2 norm of the row vectors in the embedding matrix\n",
        "        norm = torch.norm(self.module.weight_ih_l0, dim=1)\n",
        "        \n",
        "        # Apply a constraint on the norm of the row vectors\n",
        "        constraint = torch.clamp(norm, min=0, max=1)\n",
        "      \n",
        "        # Normalize the row vectors by dividing by the norm\n",
        "        self.module._parameters[\"weight_ih_l0\"] = (self.module.weight_ih_l0 / norm.unsqueeze(1)).to(device='cuda')\n",
        "\n",
        "\n",
        "        # Compute the L2 norm of the row vectors in the projection matrix\n",
        "        norm = torch.norm(self.module.weight_hh_l0, dim=1)\n",
        "\n",
        "        # Apply a constraint on the norm of the row vectors\n",
        "        constraint = torch.clamp(norm, min=0, max=1)\n",
        "\n",
        "        # Normalize the row vectors by dividing by the norm\n",
        "        self.module._parameters[\"weight_hh_l0\"] = (self.module.weight_hh_l0 / norm.unsqueeze(1)).to(device='cuda')\n",
        "     \n",
        "        # Set the weights for the input and hidden layers\n",
        "        self._setweights()\n",
        "\n",
        "        with warnings.catch_warnings():            \n",
        "            \n",
        "            # To avoid the warning that comes because the weights aren't flattened.\n",
        "            warnings.simplefilter(\"ignore\")\n",
        "            \n",
        "            return self.module.forward(*args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gdox6uWDfuTQ"
      },
      "source": [
        "##**LSTM instantation**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is composed of several parts: \n",
        "\n",
        "1.   Embed: an embedding layer that maps each word in the vocabulary to a dense vector representation (embedding);\n",
        "2.   WeightDropLSTM: a variant of the long short-term memory (LSTM) recurrent neural network (RNN) layer that applies dropout to the input-to-hidden and hidden-to-hidden weights;\n",
        "3.   FC_tied: a fully-connected layer that maps the hidden state to the output. This layer is tied to the embedding layer, meaning that it uses the same weight matrix as the embedding layer;\n",
        "4.   VariationalDropout: a dropout layer that applies dropout to the input.\n",
        "\n"
      ],
      "metadata": {
        "id": "caIi4Y_WoRsN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHen3mUGT-V4"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    \"\"\"   \n",
        "    Args:\n",
        "        vocab_size (int): The size of the vocabulary\n",
        "        embed_size (int): The size of the embeddings\n",
        "        hidden_size (int): The size of the hidden state\n",
        "        layer_num (int): The number of layers\n",
        "        w_drop (float): The dropout rate for the weights\n",
        "        dropout_i (float): The dropout rate for the input\n",
        "        dropout_o (float): The dropout rate for the output\n",
        "        dropout_e (float): The dropout rate for the embeddings\n",
        "        pad_index (int, optional): The index for padding tokens. Default: PAD_TOKEN\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, layer_num, w_drop, dropout_i, dropout_o, dropout_e, pad_index = PAD_TOKEN):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embed_size = embed_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.layer_num = layer_num\n",
        "        self.pad_index = pad_index\n",
        "        self.dropout_e = dropout_e\n",
        "        self.dropout_emb = nn.Dropout(self.dropout_e) \n",
        "\n",
        "        # Embedding layer\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size, padding_idx=pad_index)\n",
        "        \n",
        "        # LSTM layer\n",
        "        self.rnns = WeightDropLSTM(embed_size, hidden_size, w_drop) \n",
        "        \n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        # Share weight between embedding and fc layer\n",
        "        self.fc.weight = self.embed.weight\n",
        "\n",
        "        # Variational Dropout\n",
        "        self.dropout = VariationalDropout()\n",
        "        \n",
        "        # Dropout rate for input and output\n",
        "        self.dropout_i = dropout_i\n",
        "        self.dropout_o = dropout_o\n",
        "\n",
        "    def forward(self, x, seq_lengths):\n",
        "        \n",
        "        \"\"\"\n",
        "        Compute the forward pass of the model\n",
        "        :param x: input tensor\n",
        "        :param seq_lengths: sequence lengths of input tensor\n",
        "        :return: output tensor and input tensor (if in training mode)\n",
        "        \"\"\"\n",
        "        \n",
        "        x_emb = self.dropout(self.embed(x), self.dropout_i)\n",
        "        x_emb = x_emb.permute(1,0,2)\n",
        "        packed_x = pack_padded_sequence(x_emb, seq_lengths.cpu().numpy())\n",
        "\n",
        "        packed_y, states = self.rnns(packed_x)\n",
        "        padded_y, _ = pad_packed_sequence(packed_y)\n",
        "        padded_y = self.dropout(padded_y, self.dropout_o)\n",
        "\n",
        "        scores = self.fc(padded_y)\n",
        "        scores = scores.permute(1,0,2).contiguous()\n",
        "        scores = scores.view(-1, scores.shape[-1])\n",
        "\n",
        "        if self.training:\n",
        "            \n",
        "            return scores, x\n",
        "       \n",
        "        else:\n",
        "       \n",
        "            return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here the model, the optimizer, and the criterion are set. "
      ],
      "metadata": {
        "id": "eiVnOZAvMiuU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxZbEUsQUbZE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0238322-bb15-44a7-c657-59c3fcff4057"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(\n",
            "  (dropout_emb): Dropout(p=0.1, inplace=False)\n",
            "  (embed): Embedding(10001, 400, padding_idx=0)\n",
            "  (rnns): WeightDropLSTM(input: 400, hidden: 400, weight drop: 0.5)\n",
            "  (fc): Linear(in_features=400, out_features=10001, bias=True)\n",
            "  (dropout): VariationalDropout()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Instantiation of model\n",
        "vocab_size = len(vocab.word2id)\n",
        "model = Model(vocab_size, embed_size_reg, hidden_size_reg, layer_num_reg, w_drop, dropout_i, dropout_o, dropout_e, pad_index=0).to(device)\n",
        "model.apply(init_weights)\n",
        "\n",
        "# Optimizers\n",
        "lr = 0.001         \n",
        "optimizer = NTASGD(model.parameters(), lr=lr_reg, n=5, weight_decay=weight_decay, fine_tuning=False)\n",
        "\n",
        "# Loss criterion\n",
        "loss_function_reg = nn.CrossEntropyLoss(ignore_index = PAD_TOKEN, reduction = 'sum')\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Early stopping is a technique used to prevent overfitting and it is based on the idea that the model's performance on the validation set will stop improving after a certain number of training epochs, and continuing to train beyond this point will only lead to overfitting.\n",
        "\n",
        "In this case the monitor is on the perplexity: after three times we see the perplexity increasing we stop the training and get the final value.\n",
        "\n",
        "There are several benefits to using early stopping:\n",
        "\n",
        "\n",
        "\n",
        "1.   It can help prevent overfitting by stopping the training process before the model starts to overfit the training data;\n",
        "2.   It can save time and resources by avoiding the need to train the model for a large number of epochs;\n",
        "3.   It can improve the generalization ability of the model by avoiding the use of models that are overfitted to the training data."
      ],
      "metadata": {
        "id": "09OHa8KET1GP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "  \n",
        "  \"\"\"\n",
        "    A class for early stopping of training process. \n",
        "    \n",
        "    Args:\n",
        "        patience (int): Number of epochs to wait before triggering early stopping. Default: 5\n",
        "        verbose (bool): Flag indicating whether to print messages when early stopping is triggered. Default: True\n",
        "        \n",
        "    Attributes:\n",
        "        patience (int): Number of epochs to wait before triggering early stopping.\n",
        "        verbose (bool): Flag indicating whether to print messages when early stopping is triggered.\n",
        "        counter (int): Counter for the number of epochs since the last improvement.\n",
        "        best_score (float): Best score seen so far. None by default.\n",
        "  \"\"\"\n",
        "  \n",
        "  def __init__(self, patience=5, verbose=True):\n",
        "   \n",
        "    # set the number of epochs to wait before triggering early stopping\n",
        "    self.patience = patience\n",
        "   \n",
        "    # set a flag indicating whether to print messages when early stopping is triggered\n",
        "    self.verbose = verbose\n",
        "   \n",
        "    # initialize the counter for the number of epochs since the last improvement\n",
        "    self.counter = 0\n",
        "   \n",
        "    # initialize the best score seen so far to None\n",
        "    self.best_score = None\n",
        "\n",
        "  def step(self, val_loss):\n",
        "   \n",
        "    \"\"\"\n",
        "        Function to check if early stopping should be triggered.\n",
        "        \n",
        "        Args:\n",
        "            val_loss (float): Current validation loss.\n",
        "        \n",
        "        Returns:\n",
        "            bool: Whether early stopping should be triggered or not.\n",
        "    \"\"\"\n",
        "   \n",
        "    # if this is the first epoch, set the best score to the current validation loss\n",
        "    if self.best_score is None:\n",
        "   \n",
        "      self.best_score = val_loss\n",
        "   \n",
        "    # if the current validation loss is worse than the best score seen so far\n",
        "    elif val_loss > self.best_score:\n",
        "   \n",
        "      # increment the counter\n",
        "      self.counter += 1\n",
        "   \n",
        "      # if the counter has reached the patience threshold\n",
        "      if self.counter >= self.patience:\n",
        "   \n",
        "        # if verbose is True, print a message indicating that early stopping has been triggered\n",
        "        if self.verbose:\n",
        "   \n",
        "          print(f'Early stopping triggered with counter {self.counter} and patience {self.patience}')\n",
        "   \n",
        "        # return True to indicate that early stopping should be triggered\n",
        "        return True\n",
        "   \n",
        "    # if the current validation loss is better than the best score seen so far\n",
        "   \n",
        "    else:\n",
        "   \n",
        "      # set the best score to the current validation loss\n",
        "      self.best_score = val_loss\n",
        "   \n",
        "      # reset the counter\n",
        "      self.counter = 0\n",
        "   \n",
        "    # if the counter has not reached the patience threshold, return False\n",
        "    return False"
      ],
      "metadata": {
        "id": "iZFwg-yEvk02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BehBIfaxEoEG"
      },
      "source": [
        "##**Training and evaluation loops**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmE9mDiIBATM"
      },
      "source": [
        "This function is meant for the training of the model.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(data, model, optimizer, loss_predict, tic):\n",
        "    \n",
        "    \"\"\"\n",
        "    Train the model on the given data\n",
        "    \n",
        "    Parameters:\n",
        "    - data (DataLoader): The data to train on\n",
        "    - model (nn.Module): The model to train\n",
        "    - optimizer (Optimizer): The optimizer to use\n",
        "    - loss_predict (nn.Module): The loss function to use\n",
        "    - tic (float): The starting time of the training\n",
        "    \"\"\"\n",
        "\n",
        "    trn = data    \n",
        "    try:\n",
        "       \n",
        "        #Calculate the number of batches in the data\n",
        "        num_batch = len(trn.dataset)// batch_size - 1\n",
        "        \n",
        "        #Adjust the learning rate\n",
        "        optimizer.lr(lr_reg)\n",
        "\n",
        "        #Set the model to training mode\n",
        "        model.train()\n",
        "\n",
        "        # Initialize the lists to store loss and number of words\n",
        "        loss_sum_array = []\n",
        "        num_words_array = []\n",
        "\n",
        "        # Iterate over the batches\n",
        "        for i, (x, y, length) in enumerate(data):\n",
        "          if x.size(0) == batch_size:\n",
        "            # Move the data to the device\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            # Get the scores and the encoded inputs\n",
        "            scores, _ = model(x, length)\n",
        "\n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Calculate the loss\n",
        "            loss = loss_predict(scores, y.reshape(-1))\n",
        "            loss_sum_array.append(loss.item())\n",
        "            num_words_array.append((torch.sum(length)).item())\n",
        "                        \n",
        "            # calculate gradients\n",
        "            loss.backward()\n",
        "        \n",
        "            # optimize\n",
        "            optimizer.step()\n",
        "        \n",
        "            # print out the current status in the given format\n",
        "            if i % (log) == 0:\n",
        "                toc = timeit.default_timer()\n",
        "                print(\"batch no = {:d} / {:d}, \".format(i, num_batch) +\n",
        "                      \"train loss = {:.3f}, \".format(loss.item()) +\n",
        "                      \"wps = {:d}, \".format(round(num_words_array[-1]/(toc-tic))) +\n",
        "                      \"since beginning = {:d} mins, \".format(round((toc-tic)/60)) +\n",
        "                      \"cuda memory = {:.3f} GBs\".format(torch.cuda.max_memory_allocated()/1024/1024/1024))\n",
        "  \n",
        "    # in case of early stopping\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"Finishing training early.\")\n",
        "\n",
        "    # return the trained model, num_words_array, and loss_sum_array\n",
        "    return model, num_words_array, loss_sum_array"
      ],
      "metadata": {
        "id": "6np23bVTdc4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here below the trained model is evaluated."
      ],
      "metadata": {
        "id": "EVXx_d6gSo7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(data_loader, loss_predict, model):\n",
        "  \n",
        "    \"\"\"\n",
        "    Evaluate the model on the given data\n",
        "    \n",
        "    Parameters:\n",
        "        data_loader (DataLoader): DataLoader for the evaluation dataset\n",
        "        loss_predict (function): loss prediction function\n",
        "        model (nn.Module): the model to evaluate\n",
        "    \n",
        "    Returns:\n",
        "        lossess (list): list of cumulative loss values obtained for each batch\n",
        "        num_words_array (list): list of total number of words in each batch\n",
        "    \"\"\"\n",
        "    \n",
        "    # set the model to evaluation mode\n",
        "    model.eval()\n",
        "    \n",
        "    # Cumulative loss values obtained for each batch\n",
        "    lossess = []\n",
        "        \n",
        "    # Total number of words in the batch\n",
        "    num_words_array = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "\n",
        "        for i, (x, y, lengths) in enumerate(data_loader):\n",
        "          if x.size(0) == batch_size:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "    \n",
        "            scores = model(x, lengths)\n",
        "\n",
        "            loss = loss_predict(scores, y.reshape(-1))\n",
        "\n",
        "            lossess.append(loss.data.item())\n",
        "\n",
        "            num_words_array.append((torch.sum(lengths)).item())\n",
        "    \n",
        "    return lossess, num_words_array"
      ],
      "metadata": {
        "id": "Dza-PSg0aEuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6NxMfG18JBa"
      },
      "source": [
        "#**Launch Regularized Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code evaluates the performances of the model on the validation and on the test set.\n",
        "\n",
        "The training loop iterates over 200 epochs (even if we can notice that thanks to Early stopping it exits before).\n",
        "\n",
        "The perplexity, which is a measure of how well the model is able to predict the next word in a sequence, is calculated and printed.\n",
        "\n",
        "If the current perplexity is the best seen so far (i.e., it is lower than the current value of best_val), the model is saved and best_val is updated."
      ],
      "metadata": {
        "id": "cgkmq81J2bGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Initialize the best validation perplexity to a large value\n",
        "best_val = 1e10\n",
        "epochs = 200   \n",
        "\n",
        "# Vectors filled during training that allow to plot the loss variations across epochs\n",
        "loss_train_regolarized = []\n",
        "loss_valid_regolarized = []\n",
        "sampled_epochs_regolarized = []\n",
        "\n",
        "# Start a timer\n",
        "tic = timeit.default_timer() \n",
        "\n",
        "regularized = True  \n",
        "\n",
        "print(\"Starting training.\")\n",
        "\n",
        "# Create the early stopping criterion\n",
        "criterion = EarlyStopping(patience=3, verbose=True)\n",
        "\n",
        "# Loop over the number of epochs\n",
        "for epoch in range(epochs):  \n",
        "\n",
        "    print(\"Epoch : {:d}\".format(epoch+1))\n",
        "\n",
        "    # Train the model on the training data and update the total number of words processed\n",
        "    model, words_train, loss_train = train(train_loader, model, optimizer, loss_function_reg, tic)\n",
        "    \n",
        "    # Cross entropy\n",
        "    ce_train = np.asarray(loss_train).sum() / np.asarray(words_train).sum()\n",
        "    print('CE on the training set: ', ce_train, '\\n')\n",
        "\n",
        "    # Perplexity\n",
        "    perplexity_train = np.exp(ce_train)\n",
        "    print('Perplexity on the training set: ', perplexity_train, '\\n')\n",
        "\n",
        "    loss_train_regolarized.append(ce_train)\n",
        "    sampled_epochs_regolarized.append(epoch)\n",
        "\n",
        "    loss_valid, words_valid = evaluate(valid_loader, loss_function_reg, model)\n",
        "    \n",
        "    # Cross entropy\n",
        "    ce_valid = np.asarray(loss_valid).sum() / np.asarray(words_valid).sum()\n",
        "    print('CE on the validation set: ', ce_valid, '\\n')\n",
        "\n",
        "    # Perplexity\n",
        "    perplexity_valid = np.exp(ce_valid)\n",
        "    print('Perplexity on the validation set: ', perplexity_valid, '\\n')\n",
        "\n",
        "    loss_valid_regolarized.append(ce_valid)\n",
        "    \n",
        "    if ce_valid < best_val:\n",
        " \n",
        "       # Update the best validation perplexity\n",
        "        best_val = ce_valid \n",
        "\n",
        "        print(\"CE on the validation set: {:.3f}\".format(best_val)) \n",
        "        \n",
        "        # Save the model\n",
        "        torch.save({'model_state_dict': model.state_dict()},fold_path+\"RegModel\")\n",
        "        \n",
        "        print(\"Model saved!\")\n",
        "\n",
        "    # Break the loop if the training is interrupted early\n",
        "    if criterion.step(ce_valid):\n",
        "      break\n"
      ],
      "metadata": {
        "id": "TONd7myZfK20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88fd1c54-0cb3-4dbd-91e9-8e8a78da66fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training.\n",
            "Epoch : 1\n",
            "batch no = 0 / 656, train loss = 12234.173, wps = 1290, since beginning = 0 mins, cuda memory = 0.524 GBs\n",
            "batch no = 100 / 656, train loss = 10170.778, wps = 267, since beginning = 0 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 8825.595, wps = 130, since beginning = 0 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 8266.193, wps = 87, since beginning = 0 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 9444.968, wps = 77, since beginning = 0 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 8479.716, wps = 57, since beginning = 0 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 8413.565, wps = 48, since beginning = 0 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  6.642806638942889 \n",
            "\n",
            "Perplexity on the training set:  767.2453546552379 \n",
            "\n",
            "CE on the validation set:  6.215183172904696 \n",
            "\n",
            "Perplexity on the validation set:  500.2876199347679 \n",
            "\n",
            "CE on the validation set: 6.215\n",
            "Model saved!\n",
            "Epoch : 2\n",
            "batch no = 0 / 656, train loss = 8648.791, wps = 44, since beginning = 1 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 7580.767, wps = 35, since beginning = 1 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 7635.225, wps = 31, since beginning = 1 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 8150.755, wps = 29, since beginning = 1 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 8170.312, wps = 27, since beginning = 1 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 7792.819, wps = 25, since beginning = 1 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 8369.987, wps = 24, since beginning = 1 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  6.151261831398744 \n",
            "\n",
            "Perplexity on the training set:  469.309202405439 \n",
            "\n",
            "CE on the validation set:  5.926007564752009 \n",
            "\n",
            "Perplexity on the validation set:  374.65573516425053 \n",
            "\n",
            "CE on the validation set: 5.926\n",
            "Model saved!\n",
            "Epoch : 3\n",
            "batch no = 0 / 656, train loss = 8103.784, wps = 22, since beginning = 1 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 7372.893, wps = 19, since beginning = 1 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 7790.047, wps = 19, since beginning = 1 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 7964.402, wps = 17, since beginning = 1 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 8473.698, wps = 17, since beginning = 1 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 8500.037, wps = 17, since beginning = 1 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 8210.522, wps = 15, since beginning = 1 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  5.9136914099263445 \n",
            "\n",
            "Perplexity on the training set:  370.06971615379246 \n",
            "\n",
            "CE on the validation set:  5.732443519196082 \n",
            "\n",
            "Perplexity on the validation set:  308.7227173551243 \n",
            "\n",
            "CE on the validation set: 5.732\n",
            "Model saved!\n",
            "Epoch : 4\n",
            "batch no = 0 / 656, train loss = 8513.461, wps = 16, since beginning = 2 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 7771.431, wps = 14, since beginning = 2 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 7946.728, wps = 13, since beginning = 2 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 7107.601, wps = 11, since beginning = 2 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 7545.642, wps = 12, since beginning = 2 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 7999.034, wps = 12, since beginning = 2 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 7308.652, wps = 11, since beginning = 2 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  5.746502724291295 \n",
            "\n",
            "Perplexity on the training set:  313.09376810676264 \n",
            "\n",
            "CE on the validation set:  5.614122413216196 \n",
            "\n",
            "Perplexity on the validation set:  274.27257556032026 \n",
            "\n",
            "CE on the validation set: 5.614\n",
            "Model saved!\n",
            "Epoch : 5\n",
            "batch no = 0 / 656, train loss = 7927.019, wps = 11, since beginning = 2 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 7835.841, wps = 11, since beginning = 2 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 7224.877, wps = 10, since beginning = 2 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 7076.881, wps = 9, since beginning = 2 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 7434.230, wps = 9, since beginning = 2 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 8025.362, wps = 10, since beginning = 2 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 7938.976, wps = 9, since beginning = 3 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  5.618863560025673 \n",
            "\n",
            "Perplexity on the training set:  275.5760295987056 \n",
            "\n",
            "CE on the validation set:  5.467868860417747 \n",
            "\n",
            "Perplexity on the validation set:  236.9546708054186 \n",
            "\n",
            "CE on the validation set: 5.468\n",
            "Model saved!\n",
            "Epoch : 6\n",
            "batch no = 0 / 656, train loss = 7691.313, wps = 9, since beginning = 3 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 7286.661, wps = 8, since beginning = 3 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 7677.237, wps = 9, since beginning = 3 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 6202.777, wps = 7, since beginning = 3 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 8332.138, wps = 9, since beginning = 3 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 7323.002, wps = 8, since beginning = 3 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 7727.592, wps = 8, since beginning = 3 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  5.5197092903037035 \n",
            "\n",
            "Perplexity on the training set:  249.56247641141934 \n",
            "\n",
            "CE on the validation set:  5.380418777081263 \n",
            "\n",
            "Perplexity on the validation set:  217.11317841273444 \n",
            "\n",
            "CE on the validation set: 5.380\n",
            "Model saved!\n",
            "Epoch : 7\n",
            "batch no = 0 / 656, train loss = 7894.590, wps = 8, since beginning = 3 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 6999.683, wps = 7, since beginning = 3 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 7477.319, wps = 7, since beginning = 3 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 7812.645, wps = 7, since beginning = 3 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 8215.527, wps = 7, since beginning = 3 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 7524.422, wps = 7, since beginning = 4 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 6432.051, wps = 6, since beginning = 4 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  5.434788544660974 \n",
            "\n",
            "Perplexity on the training set:  229.24436820464567 \n",
            "\n",
            "CE on the validation set:  5.308239187409091 \n",
            "\n",
            "Perplexity on the validation set:  201.99424106368707 \n",
            "\n",
            "CE on the validation set: 5.308\n",
            "Model saved!\n",
            "Epoch : 8\n",
            "batch no = 0 / 656, train loss = 7572.480, wps = 6, since beginning = 4 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 7289.458, wps = 6, since beginning = 4 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 7708.114, wps = 6, since beginning = 4 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 6675.754, wps = 6, since beginning = 4 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 7355.751, wps = 6, since beginning = 4 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 6914.603, wps = 5, since beginning = 4 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 6056.149, wps = 5, since beginning = 4 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  5.361316979132611 \n",
            "\n",
            "Perplexity on the training set:  213.00528532471182 \n",
            "\n",
            "CE on the validation set:  5.253744136518369 \n",
            "\n",
            "Perplexity on the validation set:  191.2811119843206 \n",
            "\n",
            "CE on the validation set: 5.254\n",
            "Model saved!\n",
            "Epoch : 9\n",
            "batch no = 0 / 656, train loss = 7348.674, wps = 6, since beginning = 4 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 7322.302, wps = 5, since beginning = 4 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 7660.117, wps = 6, since beginning = 4 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 6700.039, wps = 5, since beginning = 4 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 7645.249, wps = 5, since beginning = 4 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 7113.031, wps = 5, since beginning = 5 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 6831.138, wps = 5, since beginning = 5 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  5.2983132789850655 \n",
            "\n",
            "Perplexity on the training set:  199.9991824890766 \n",
            "\n",
            "CE on the validation set:  5.198267551572536 \n",
            "\n",
            "Perplexity on the validation set:  180.95846894096408 \n",
            "\n",
            "CE on the validation set: 5.198\n",
            "Model saved!\n",
            "Epoch : 10\n",
            "batch no = 0 / 656, train loss = 7258.734, wps = 5, since beginning = 5 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 6790.049, wps = 4, since beginning = 5 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 6168.891, wps = 4, since beginning = 5 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 6736.779, wps = 4, since beginning = 5 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 7022.291, wps = 4, since beginning = 5 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 6649.653, wps = 4, since beginning = 5 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 6470.250, wps = 4, since beginning = 5 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  5.242547484013103 \n",
            "\n",
            "Perplexity on the training set:  189.1513491992903 \n",
            "\n",
            "CE on the validation set:  5.149425271352682 \n",
            "\n",
            "Perplexity on the validation set:  172.33241747239742 \n",
            "\n",
            "CE on the validation set: 5.149\n",
            "Model saved!\n",
            "Epoch : 11\n",
            "batch no = 0 / 656, train loss = 6603.166, wps = 4, since beginning = 5 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 7527.044, wps = 5, since beginning = 5 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 7747.060, wps = 5, since beginning = 5 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 7430.343, wps = 4, since beginning = 5 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 7235.306, wps = 4, since beginning = 6 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 6705.649, wps = 4, since beginning = 6 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 7670.895, wps = 4, since beginning = 6 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  5.192570493916305 \n",
            "\n",
            "Perplexity on the training set:  179.93046917742106 \n",
            "\n",
            "CE on the validation set:  5.111045991005465 \n",
            "\n",
            "Perplexity on the validation set:  165.8437352361417 \n",
            "\n",
            "CE on the validation set: 5.111\n",
            "Model saved!\n",
            "Epoch : 12\n",
            "batch no = 0 / 656, train loss = 7003.031, wps = 4, since beginning = 6 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 7897.922, wps = 4, since beginning = 6 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 7908.309, wps = 4, since beginning = 6 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 6847.091, wps = 4, since beginning = 6 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 6510.289, wps = 3, since beginning = 6 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 6630.600, wps = 4, since beginning = 6 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 6620.186, wps = 3, since beginning = 6 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  5.146009039073077 \n",
            "\n",
            "Perplexity on the training set:  171.7446943759048 \n",
            "\n",
            "CE on the validation set:  5.069650087746893 \n",
            "\n",
            "Perplexity on the validation set:  159.11864003918276 \n",
            "\n",
            "CE on the validation set: 5.070\n",
            "Model saved!\n",
            "Epoch : 13\n",
            "batch no = 0 / 656, train loss = 6835.415, wps = 4, since beginning = 6 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 7105.670, wps = 4, since beginning = 6 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 6601.081, wps = 3, since beginning = 6 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 7348.079, wps = 4, since beginning = 6 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 6805.617, wps = 3, since beginning = 7 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 6514.186, wps = 3, since beginning = 7 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 6864.943, wps = 3, since beginning = 7 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  5.102781700737244 \n",
            "\n",
            "Perplexity on the training set:  164.47880233837893 \n",
            "\n",
            "CE on the validation set:  5.0378720270241075 \n",
            "\n",
            "Perplexity on the validation set:  154.1416565012714 \n",
            "\n",
            "CE on the validation set: 5.038\n",
            "Model saved!\n",
            "Epoch : 14\n",
            "batch no = 0 / 656, train loss = 6055.687, wps = 3, since beginning = 7 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 7091.550, wps = 3, since beginning = 7 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 6514.642, wps = 3, since beginning = 7 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 7014.440, wps = 3, since beginning = 7 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 6950.944, wps = 3, since beginning = 7 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 6208.842, wps = 3, since beginning = 7 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 7285.653, wps = 3, since beginning = 7 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  5.06451357353247 \n",
            "\n",
            "Perplexity on the training set:  158.30342036893447 \n",
            "\n",
            "CE on the validation set:  5.0173551011783895 \n",
            "\n",
            "Perplexity on the validation set:  151.01136534658065 \n",
            "\n",
            "CE on the validation set: 5.017\n",
            "Model saved!\n",
            "Epoch : 15\n",
            "batch no = 0 / 656, train loss = 7016.285, wps = 3, since beginning = 7 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 6760.101, wps = 3, since beginning = 7 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 6824.296, wps = 3, since beginning = 7 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 6406.239, wps = 3, since beginning = 8 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 6928.239, wps = 3, since beginning = 8 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 7007.699, wps = 3, since beginning = 8 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 6896.637, wps = 3, since beginning = 8 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  5.026978429391604 \n",
            "\n",
            "Perplexity on the training set:  152.47161222749128 \n",
            "\n",
            "CE on the validation set:  4.985879465004977 \n",
            "\n",
            "Perplexity on the validation set:  146.33221251178742 \n",
            "\n",
            "CE on the validation set: 4.986\n",
            "Model saved!\n",
            "Epoch : 16\n",
            "batch no = 0 / 656, train loss = 6169.203, wps = 3, since beginning = 8 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 6749.017, wps = 3, since beginning = 8 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 6394.610, wps = 3, since beginning = 8 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 5962.928, wps = 3, since beginning = 8 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 7178.916, wps = 3, since beginning = 8 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 6911.013, wps = 3, since beginning = 8 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 6773.647, wps = 3, since beginning = 8 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.991852707903817 \n",
            "\n",
            "Perplexity on the training set:  147.20890610626503 \n",
            "\n",
            "CE on the validation set:  4.951360192866523 \n",
            "\n",
            "Perplexity on the validation set:  141.3671197552056 \n",
            "\n",
            "CE on the validation set: 4.951\n",
            "Model saved!\n",
            "Epoch : 17\n",
            "batch no = 0 / 656, train loss = 7275.624, wps = 3, since beginning = 8 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 6021.821, wps = 2, since beginning = 8 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 6199.275, wps = 2, since beginning = 8 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 6511.871, wps = 3, since beginning = 9 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 6376.114, wps = 2, since beginning = 9 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 6638.551, wps = 3, since beginning = 9 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 7047.788, wps = 3, since beginning = 9 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.959701194997374 \n",
            "\n",
            "Perplexity on the training set:  142.55119452250048 \n",
            "\n",
            "CE on the validation set:  4.932241058063938 \n",
            "\n",
            "Perplexity on the validation set:  138.6899766162708 \n",
            "\n",
            "CE on the validation set: 4.932\n",
            "Model saved!\n",
            "Epoch : 18\n",
            "batch no = 0 / 656, train loss = 6899.869, wps = 3, since beginning = 9 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 7034.969, wps = 3, since beginning = 9 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 7096.010, wps = 3, since beginning = 9 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 6353.602, wps = 2, since beginning = 9 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 7016.215, wps = 3, since beginning = 9 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 6770.759, wps = 2, since beginning = 9 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 6215.200, wps = 2, since beginning = 9 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.92814109071257 \n",
            "\n",
            "Perplexity on the training set:  138.12251631943164 \n",
            "\n",
            "CE on the validation set:  4.91238916033908 \n",
            "\n",
            "Perplexity on the validation set:  135.96386614706563 \n",
            "\n",
            "CE on the validation set: 4.912\n",
            "Model saved!\n",
            "Epoch : 19\n",
            "batch no = 0 / 656, train loss = 6039.418, wps = 2, since beginning = 9 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 6460.780, wps = 2, since beginning = 9 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 6708.586, wps = 3, since beginning = 10 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 6368.634, wps = 2, since beginning = 10 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 6718.723, wps = 2, since beginning = 10 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 5710.958, wps = 2, since beginning = 10 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 7075.193, wps = 2, since beginning = 10 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.899237613437995 \n",
            "\n",
            "Perplexity on the training set:  134.187437978417 \n",
            "\n",
            "CE on the validation set:  4.891264794138521 \n",
            "\n",
            "Perplexity on the validation set:  133.12183934048275 \n",
            "\n",
            "CE on the validation set: 4.891\n",
            "Model saved!\n",
            "Epoch : 20\n",
            "batch no = 0 / 656, train loss = 6128.463, wps = 2, since beginning = 10 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 6560.625, wps = 2, since beginning = 10 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 6126.751, wps = 2, since beginning = 10 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 6192.727, wps = 2, since beginning = 10 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 6481.167, wps = 2, since beginning = 10 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 6749.160, wps = 2, since beginning = 10 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 6276.498, wps = 2, since beginning = 10 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.870537526031773 \n",
            "\n",
            "Perplexity on the training set:  130.39098661223625 \n",
            "\n",
            "CE on the validation set:  4.871576535818149 \n",
            "\n",
            "Perplexity on the validation set:  130.52653452899344 \n",
            "\n",
            "CE on the validation set: 4.872\n",
            "Model saved!\n",
            "Epoch : 21\n",
            "batch no = 0 / 656, train loss = 6455.761, wps = 2, since beginning = 10 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 5972.592, wps = 2, since beginning = 10 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 6950.003, wps = 2, since beginning = 11 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 6782.134, wps = 2, since beginning = 11 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 6289.069, wps = 2, since beginning = 11 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 6984.697, wps = 2, since beginning = 11 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 6939.634, wps = 2, since beginning = 11 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.843529427860693 \n",
            "\n",
            "Perplexity on the training set:  126.9165048167389 \n",
            "\n",
            "CE on the validation set:  4.849966810123105 \n",
            "\n",
            "Perplexity on the validation set:  127.7361502285719 \n",
            "\n",
            "CE on the validation set: 4.850\n",
            "Model saved!\n",
            "Epoch : 22\n",
            "batch no = 0 / 656, train loss = 6682.331, wps = 2, since beginning = 11 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 5954.217, wps = 2, since beginning = 11 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 6948.915, wps = 2, since beginning = 11 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 6936.114, wps = 2, since beginning = 11 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 7193.797, wps = 2, since beginning = 11 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 6684.740, wps = 2, since beginning = 11 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 6826.701, wps = 2, since beginning = 11 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.817211828231887 \n",
            "\n",
            "Perplexity on the training set:  123.61993621219037 \n",
            "\n",
            "CE on the validation set:  4.836092455004825 \n",
            "\n",
            "Perplexity on the validation set:  125.97613131577742 \n",
            "\n",
            "CE on the validation set: 4.836\n",
            "Model saved!\n",
            "Epoch : 23\n",
            "batch no = 0 / 656, train loss = 6309.179, wps = 2, since beginning = 11 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 7146.799, wps = 2, since beginning = 12 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 6238.318, wps = 2, since beginning = 12 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 5908.063, wps = 2, since beginning = 12 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 6907.757, wps = 2, since beginning = 12 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 6572.915, wps = 2, since beginning = 12 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 7568.716, wps = 2, since beginning = 12 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.793769886573393 \n",
            "\n",
            "Perplexity on the training set:  120.75574711717898 \n",
            "\n",
            "CE on the validation set:  4.83217699321247 \n",
            "\n",
            "Perplexity on the validation set:  125.48384098778209 \n",
            "\n",
            "CE on the validation set: 4.832\n",
            "Model saved!\n",
            "Epoch : 24\n",
            "batch no = 0 / 656, train loss = 6206.829, wps = 2, since beginning = 12 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 6123.129, wps = 2, since beginning = 12 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 6889.266, wps = 2, since beginning = 12 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 6162.829, wps = 2, since beginning = 12 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 7437.581, wps = 2, since beginning = 12 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 6464.055, wps = 2, since beginning = 12 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 5946.820, wps = 2, since beginning = 12 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.769473976843334 \n",
            "\n",
            "Perplexity on the training set:  117.8572300200711 \n",
            "\n",
            "CE on the validation set:  4.811460621514427 \n",
            "\n",
            "Perplexity on the validation set:  122.91101294082519 \n",
            "\n",
            "CE on the validation set: 4.811\n",
            "Model saved!\n",
            "Epoch : 25\n",
            "batch no = 0 / 656, train loss = 6139.176, wps = 2, since beginning = 12 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 6071.126, wps = 2, since beginning = 13 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 7073.868, wps = 2, since beginning = 13 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 6328.233, wps = 2, since beginning = 13 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 6004.042, wps = 2, since beginning = 13 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 6711.901, wps = 2, since beginning = 13 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 6237.904, wps = 2, since beginning = 13 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.7465650523979 \n",
            "\n",
            "Perplexity on the training set:  115.18793966782421 \n",
            "\n",
            "CE on the validation set:  4.791749702888053 \n",
            "\n",
            "Perplexity on the validation set:  120.51204457177705 \n",
            "\n",
            "CE on the validation set: 4.792\n",
            "Model saved!\n",
            "Epoch : 26\n",
            "batch no = 0 / 656, train loss = 6286.912, wps = 2, since beginning = 13 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 6315.045, wps = 2, since beginning = 13 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 6030.984, wps = 2, since beginning = 13 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 5609.940, wps = 1, since beginning = 13 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 6151.148, wps = 2, since beginning = 13 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 5819.026, wps = 2, since beginning = 13 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 6178.514, wps = 2, since beginning = 13 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.72557631404715 \n",
            "\n",
            "Perplexity on the training set:  112.79548526845574 \n",
            "\n",
            "CE on the validation set:  4.781862587707514 \n",
            "\n",
            "Perplexity on the validation set:  119.32639907181787 \n",
            "\n",
            "CE on the validation set: 4.782\n",
            "Model saved!\n",
            "Epoch : 27\n",
            "batch no = 0 / 656, train loss = 6563.460, wps = 2, since beginning = 14 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 6546.227, wps = 2, since beginning = 14 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 6760.949, wps = 2, since beginning = 14 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 5376.492, wps = 1, since beginning = 14 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 6651.901, wps = 2, since beginning = 14 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 6270.020, wps = 2, since beginning = 14 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 5957.256, wps = 2, since beginning = 14 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.704705051937443 \n",
            "\n",
            "Perplexity on the training set:  110.4656984983021 \n",
            "\n",
            "CE on the validation set:  4.7678350742355144 \n",
            "\n",
            "Perplexity on the validation set:  117.66423169436997 \n",
            "\n",
            "CE on the validation set: 4.768\n",
            "Model saved!\n",
            "Epoch : 28\n",
            "batch no = 0 / 656, train loss = 6608.518, wps = 2, since beginning = 14 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 6404.040, wps = 2, since beginning = 14 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 6808.795, wps = 2, since beginning = 14 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 6286.014, wps = 2, since beginning = 14 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 6393.046, wps = 2, since beginning = 14 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 6755.856, wps = 2, since beginning = 14 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 6357.835, wps = 2, since beginning = 14 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.683134358369865 \n",
            "\n",
            "Perplexity on the training set:  108.10839253140787 \n",
            "\n",
            "CE on the validation set:  4.757285891365093 \n",
            "\n",
            "Perplexity on the validation set:  116.42949438241939 \n",
            "\n",
            "CE on the validation set: 4.757\n",
            "Model saved!\n",
            "Epoch : 29\n",
            "batch no = 0 / 656, train loss = 6004.469, wps = 1, since beginning = 15 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 6472.282, wps = 2, since beginning = 15 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 6477.771, wps = 2, since beginning = 15 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 6484.011, wps = 2, since beginning = 15 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 5946.374, wps = 1, since beginning = 15 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 5856.106, wps = 2, since beginning = 15 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 5656.001, wps = 1, since beginning = 15 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.664306489536811 \n",
            "\n",
            "Perplexity on the training set:  106.09198380242033 \n",
            "\n",
            "CE on the validation set:  4.7423459568923 \n",
            "\n",
            "Perplexity on the validation set:  114.70297452579368 \n",
            "\n",
            "CE on the validation set: 4.742\n",
            "Model saved!\n",
            "Epoch : 30\n",
            "batch no = 0 / 656, train loss = 6525.359, wps = 2, since beginning = 15 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 6986.723, wps = 2, since beginning = 15 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 6815.900, wps = 2, since beginning = 15 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 6034.572, wps = 1, since beginning = 15 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 6210.034, wps = 1, since beginning = 15 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 6858.853, wps = 2, since beginning = 15 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 6321.728, wps = 1, since beginning = 16 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.646813841869535 \n",
            "\n",
            "Perplexity on the training set:  104.25229156430811 \n",
            "\n",
            "CE on the validation set:  4.734363998588444 \n",
            "\n",
            "Perplexity on the validation set:  113.79106442161836 \n",
            "\n",
            "CE on the validation set: 4.734\n",
            "Model saved!\n",
            "Epoch : 31\n",
            "batch no = 0 / 656, train loss = 6468.324, wps = 1, since beginning = 16 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 7072.074, wps = 2, since beginning = 16 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 5896.330, wps = 1, since beginning = 16 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 5724.723, wps = 1, since beginning = 16 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 6787.621, wps = 1, since beginning = 16 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 7167.275, wps = 2, since beginning = 16 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 6401.677, wps = 1, since beginning = 16 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.626718716887716 \n",
            "\n",
            "Perplexity on the training set:  102.17823771521094 \n",
            "\n",
            "CE on the validation set:  4.726372585891596 \n",
            "\n",
            "Perplexity on the validation set:  112.88533690597265 \n",
            "\n",
            "CE on the validation set: 4.726\n",
            "Model saved!\n",
            "Epoch : 32\n",
            "batch no = 0 / 656, train loss = 6432.940, wps = 1, since beginning = 16 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 6904.856, wps = 2, since beginning = 16 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 6448.717, wps = 1, since beginning = 16 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 6394.917, wps = 1, since beginning = 16 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 6057.046, wps = 1, since beginning = 16 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 5719.516, wps = 1, since beginning = 16 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 7006.910, wps = 1, since beginning = 17 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.610356421512805 \n",
            "\n",
            "Perplexity on the training set:  100.51997073234102 \n",
            "\n",
            "CE on the validation set:  4.712771131879788 \n",
            "\n",
            "Perplexity on the validation set:  111.36032687478378 \n",
            "\n",
            "CE on the validation set: 4.713\n",
            "Model saved!\n",
            "Epoch : 33\n",
            "batch no = 0 / 656, train loss = 6293.933, wps = 1, since beginning = 17 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 5615.849, wps = 1, since beginning = 17 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 6731.479, wps = 1, since beginning = 17 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 6127.883, wps = 1, since beginning = 17 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 6001.440, wps = 1, since beginning = 17 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 6238.430, wps = 1, since beginning = 17 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 5834.595, wps = 1, since beginning = 17 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.59393125477178 \n",
            "\n",
            "Perplexity on the training set:  98.88239896295063 \n",
            "\n",
            "CE on the validation set:  4.71346815867325 \n",
            "\n",
            "Perplexity on the validation set:  111.43797506463486 \n",
            "\n",
            "Epoch : 34\n",
            "batch no = 0 / 656, train loss = 6501.208, wps = 1, since beginning = 17 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 6079.436, wps = 1, since beginning = 17 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 6190.084, wps = 1, since beginning = 17 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 6690.145, wps = 1, since beginning = 17 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 6047.854, wps = 1, since beginning = 17 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 5532.920, wps = 1, since beginning = 18 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 6858.795, wps = 1, since beginning = 18 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.576064428268462 \n",
            "\n",
            "Perplexity on the training set:  97.13137351088196 \n",
            "\n",
            "CE on the validation set:  4.6963872032480865 \n",
            "\n",
            "Perplexity on the validation set:  109.55067233304734 \n",
            "\n",
            "CE on the validation set: 4.696\n",
            "Model saved!\n",
            "Epoch : 35\n",
            "batch no = 0 / 656, train loss = 6175.316, wps = 1, since beginning = 18 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 6097.076, wps = 1, since beginning = 18 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 5169.109, wps = 1, since beginning = 18 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 5758.927, wps = 1, since beginning = 18 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 6062.581, wps = 1, since beginning = 18 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 5924.326, wps = 1, since beginning = 18 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 6950.765, wps = 1, since beginning = 18 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.561174454062785 \n",
            "\n",
            "Perplexity on the training set:  95.69580418327317 \n",
            "\n",
            "CE on the validation set:  4.701668956676055 \n",
            "\n",
            "Perplexity on the validation set:  110.13082272915021 \n",
            "\n",
            "Epoch : 36\n",
            "batch no = 0 / 656, train loss = 6329.152, wps = 1, since beginning = 18 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 6157.763, wps = 1, since beginning = 18 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 5889.023, wps = 1, since beginning = 18 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 6106.668, wps = 1, since beginning = 18 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 6745.251, wps = 1, since beginning = 18 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 6418.381, wps = 1, since beginning = 19 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 6431.648, wps = 1, since beginning = 19 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.546724708639423 \n",
            "\n",
            "Perplexity on the training set:  94.32296663799391 \n",
            "\n",
            "CE on the validation set:  4.688161223861575 \n",
            "\n",
            "Perplexity on the validation set:  108.65320708667194 \n",
            "\n",
            "CE on the validation set: 4.688\n",
            "Model saved!\n",
            "Epoch : 37\n",
            "batch no = 0 / 656, train loss = 5619.075, wps = 1, since beginning = 19 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 5799.265, wps = 1, since beginning = 19 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 6234.447, wps = 1, since beginning = 19 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 6509.756, wps = 1, since beginning = 19 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 6194.542, wps = 1, since beginning = 19 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 6097.065, wps = 1, since beginning = 19 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 5824.023, wps = 1, since beginning = 19 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.529200359176911 \n",
            "\n",
            "Perplexity on the training set:  92.68441719816845 \n",
            "\n",
            "CE on the validation set:  4.67716938975687 \n",
            "\n",
            "Perplexity on the validation set:  107.46544883900167 \n",
            "\n",
            "CE on the validation set: 4.677\n",
            "Model saved!\n",
            "Epoch : 38\n",
            "batch no = 0 / 656, train loss = 5973.905, wps = 1, since beginning = 19 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 6730.907, wps = 1, since beginning = 19 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 5881.145, wps = 1, since beginning = 19 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 5959.309, wps = 1, since beginning = 19 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 6288.173, wps = 1, since beginning = 20 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 6923.828, wps = 1, since beginning = 20 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 5701.167, wps = 1, since beginning = 20 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.515526532189103 \n",
            "\n",
            "Perplexity on the training set:  91.42569192106646 \n",
            "\n",
            "CE on the validation set:  4.669436673723322 \n",
            "\n",
            "Perplexity on the validation set:  106.63765371684163 \n",
            "\n",
            "CE on the validation set: 4.669\n",
            "Model saved!\n",
            "Epoch : 39\n",
            "batch no = 0 / 656, train loss = 6317.819, wps = 1, since beginning = 20 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 5108.241, wps = 1, since beginning = 20 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 5917.862, wps = 1, since beginning = 20 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 5234.317, wps = 1, since beginning = 20 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 6270.519, wps = 1, since beginning = 20 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 6370.280, wps = 1, since beginning = 20 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 5848.078, wps = 1, since beginning = 20 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.501505900071343 \n",
            "\n",
            "Perplexity on the training set:  90.15279022372385 \n",
            "\n",
            "CE on the validation set:  4.664480599346494 \n",
            "\n",
            "Perplexity on the validation set:  106.11045706567063 \n",
            "\n",
            "CE on the validation set: 4.664\n",
            "Model saved!\n",
            "Epoch : 40\n",
            "batch no = 0 / 656, train loss = 6629.845, wps = 1, since beginning = 20 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 6094.125, wps = 1, since beginning = 20 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 6107.917, wps = 1, since beginning = 20 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 5952.180, wps = 1, since beginning = 20 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 7044.185, wps = 1, since beginning = 21 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 7082.391, wps = 1, since beginning = 21 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 6978.378, wps = 1, since beginning = 21 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.487807629529595 \n",
            "\n",
            "Perplexity on the training set:  88.92627267659168 \n",
            "\n",
            "CE on the validation set:  4.660038426956646 \n",
            "\n",
            "Perplexity on the validation set:  105.64014150780255 \n",
            "\n",
            "CE on the validation set: 4.660\n",
            "Model saved!\n",
            "Epoch : 41\n",
            "batch no = 0 / 656, train loss = 6054.241, wps = 1, since beginning = 21 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 5639.298, wps = 1, since beginning = 21 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 5323.346, wps = 1, since beginning = 21 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 6243.796, wps = 1, since beginning = 21 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 5353.072, wps = 1, since beginning = 21 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 5533.485, wps = 1, since beginning = 21 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 5651.365, wps = 1, since beginning = 21 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.473171016677944 \n",
            "\n",
            "Perplexity on the training set:  87.63417230472479 \n",
            "\n",
            "CE on the validation set:  4.655492952402233 \n",
            "\n",
            "Perplexity on the validation set:  105.16104661437276 \n",
            "\n",
            "CE on the validation set: 4.655\n",
            "Model saved!\n",
            "Epoch : 42\n",
            "batch no = 0 / 656, train loss = 6670.468, wps = 1, since beginning = 21 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 5677.702, wps = 1, since beginning = 21 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 6276.773, wps = 1, since beginning = 21 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 5265.859, wps = 1, since beginning = 22 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 5655.244, wps = 1, since beginning = 22 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 6394.284, wps = 1, since beginning = 22 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 5740.732, wps = 1, since beginning = 22 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.4601159226516325 \n",
            "\n",
            "Perplexity on the training set:  86.4975355388516 \n",
            "\n",
            "CE on the validation set:  4.651124435675755 \n",
            "\n",
            "Perplexity on the validation set:  104.70265080713911 \n",
            "\n",
            "CE on the validation set: 4.651\n",
            "Model saved!\n",
            "Epoch : 43\n",
            "batch no = 0 / 656, train loss = 5527.297, wps = 1, since beginning = 22 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 5804.381, wps = 1, since beginning = 22 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 6433.918, wps = 1, since beginning = 22 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 5886.572, wps = 1, since beginning = 22 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 6027.141, wps = 1, since beginning = 22 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 6110.260, wps = 1, since beginning = 22 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 5726.064, wps = 1, since beginning = 22 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.445893876447912 \n",
            "\n",
            "Perplexity on the training set:  85.27607004962518 \n",
            "\n",
            "CE on the validation set:  4.64353046611163 \n",
            "\n",
            "Perplexity on the validation set:  103.91055345180747 \n",
            "\n",
            "CE on the validation set: 4.644\n",
            "Model saved!\n",
            "Epoch : 44\n",
            "batch no = 0 / 656, train loss = 5813.130, wps = 1, since beginning = 22 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 6405.063, wps = 1, since beginning = 22 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 6403.263, wps = 1, since beginning = 22 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 5934.211, wps = 1, since beginning = 23 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 6410.236, wps = 1, since beginning = 23 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 6475.768, wps = 1, since beginning = 23 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 6428.332, wps = 1, since beginning = 23 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.434671289645983 \n",
            "\n",
            "Perplexity on the training set:  84.3244020281017 \n",
            "\n",
            "CE on the validation set:  4.644121138894814 \n",
            "\n",
            "Perplexity on the validation set:  103.97194871808837 \n",
            "\n",
            "Epoch : 45\n",
            "batch no = 0 / 656, train loss = 6348.504, wps = 1, since beginning = 23 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 7049.781, wps = 1, since beginning = 23 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 6530.710, wps = 1, since beginning = 23 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 5648.453, wps = 1, since beginning = 23 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 5382.250, wps = 1, since beginning = 23 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 6218.606, wps = 1, since beginning = 23 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 5406.478, wps = 1, since beginning = 23 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.423187195381387 \n",
            "\n",
            "Perplexity on the training set:  83.36155195893755 \n",
            "\n",
            "CE on the validation set:  4.635312302684814 \n",
            "\n",
            "Perplexity on the validation set:  103.06009891546233 \n",
            "\n",
            "CE on the validation set: 4.635\n",
            "Model saved!\n",
            "Epoch : 46\n",
            "batch no = 0 / 656, train loss = 5705.894, wps = 1, since beginning = 23 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 6254.779, wps = 1, since beginning = 23 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 5822.910, wps = 1, since beginning = 23 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 5725.094, wps = 1, since beginning = 24 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 6445.069, wps = 1, since beginning = 24 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 5610.462, wps = 1, since beginning = 24 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 5836.615, wps = 1, since beginning = 24 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.4117998952085555 \n",
            "\n",
            "Perplexity on the training set:  82.41767325837455 \n",
            "\n",
            "CE on the validation set:  4.633009069804178 \n",
            "\n",
            "Perplexity on the validation set:  102.82300065802471 \n",
            "\n",
            "CE on the validation set: 4.633\n",
            "Model saved!\n",
            "Epoch : 47\n",
            "batch no = 0 / 656, train loss = 6494.542, wps = 1, since beginning = 24 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 5698.610, wps = 1, since beginning = 24 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 5473.434, wps = 1, since beginning = 24 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 6585.921, wps = 1, since beginning = 24 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 5464.884, wps = 1, since beginning = 24 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 6675.393, wps = 1, since beginning = 24 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 5563.353, wps = 1, since beginning = 24 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.399057456680204 \n",
            "\n",
            "Perplexity on the training set:  81.37413386143545 \n",
            "\n",
            "CE on the validation set:  4.629792083377213 \n",
            "\n",
            "Perplexity on the validation set:  102.49275194814861 \n",
            "\n",
            "CE on the validation set: 4.630\n",
            "Model saved!\n",
            "Epoch : 48\n",
            "batch no = 0 / 656, train loss = 5585.421, wps = 1, since beginning = 24 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 5877.524, wps = 1, since beginning = 24 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 5872.421, wps = 1, since beginning = 25 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 6743.602, wps = 1, since beginning = 25 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 5414.830, wps = 1, since beginning = 25 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 6151.336, wps = 1, since beginning = 25 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 6633.661, wps = 1, since beginning = 25 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.38679292377698 \n",
            "\n",
            "Perplexity on the training set:  80.3822132741521 \n",
            "\n",
            "CE on the validation set:  4.633345064656375 \n",
            "\n",
            "Perplexity on the validation set:  102.85755446155822 \n",
            "\n",
            "Epoch : 49\n",
            "batch no = 0 / 656, train loss = 6038.568, wps = 1, since beginning = 25 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 6657.309, wps = 1, since beginning = 25 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 5510.357, wps = 1, since beginning = 25 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 5676.223, wps = 1, since beginning = 25 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 5763.963, wps = 1, since beginning = 25 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 6102.482, wps = 1, since beginning = 25 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 5065.730, wps = 1, since beginning = 25 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.376634029697339 \n",
            "\n",
            "Perplexity on the training set:  79.56975272132348 \n",
            "\n",
            "CE on the validation set:  4.622440236282052 \n",
            "\n",
            "Perplexity on the validation set:  101.74200398106345 \n",
            "\n",
            "CE on the validation set: 4.622\n",
            "Model saved!\n",
            "Epoch : 50\n",
            "batch no = 0 / 656, train loss = 6030.117, wps = 1, since beginning = 25 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 5652.976, wps = 1, since beginning = 25 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 6227.340, wps = 1, since beginning = 26 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 6068.439, wps = 1, since beginning = 26 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 6114.296, wps = 1, since beginning = 26 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 6645.960, wps = 1, since beginning = 26 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 5876.699, wps = 1, since beginning = 26 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.3657100006272325 \n",
            "\n",
            "Perplexity on the training set:  78.70526089337574 \n",
            "\n",
            "CE on the validation set:  4.617453112539469 \n",
            "\n",
            "Perplexity on the validation set:  101.23586714992263 \n",
            "\n",
            "CE on the validation set: 4.617\n",
            "Model saved!\n",
            "Epoch : 51\n",
            "batch no = 0 / 656, train loss = 5562.825, wps = 1, since beginning = 26 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 5047.476, wps = 1, since beginning = 26 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 5314.484, wps = 1, since beginning = 26 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 5799.064, wps = 1, since beginning = 26 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 6521.509, wps = 1, since beginning = 26 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 5046.305, wps = 1, since beginning = 26 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 6049.997, wps = 1, since beginning = 26 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.355227179728221 \n",
            "\n",
            "Perplexity on the training set:  77.8845171105756 \n",
            "\n",
            "CE on the validation set:  4.617207632337031 \n",
            "\n",
            "Perplexity on the validation set:  101.21101879877465 \n",
            "\n",
            "CE on the validation set: 4.617\n",
            "Model saved!\n",
            "Epoch : 52\n",
            "batch no = 0 / 656, train loss = 5606.890, wps = 1, since beginning = 26 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 6164.029, wps = 1, since beginning = 27 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 6718.437, wps = 1, since beginning = 27 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 5638.366, wps = 1, since beginning = 27 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 5741.362, wps = 1, since beginning = 27 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 5460.555, wps = 1, since beginning = 27 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 6392.802, wps = 1, since beginning = 27 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.343816286818694 \n",
            "\n",
            "Perplexity on the training set:  77.0008366068506 \n",
            "\n",
            "CE on the validation set:  4.616167691299252 \n",
            "\n",
            "Perplexity on the validation set:  101.1058200165966 \n",
            "\n",
            "CE on the validation set: 4.616\n",
            "Model saved!\n",
            "Epoch : 53\n",
            "batch no = 0 / 656, train loss = 5757.915, wps = 1, since beginning = 27 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 6156.432, wps = 1, since beginning = 27 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 5700.962, wps = 1, since beginning = 27 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 5351.697, wps = 1, since beginning = 27 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 5631.974, wps = 1, since beginning = 27 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 6133.184, wps = 1, since beginning = 27 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 5484.021, wps = 1, since beginning = 27 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.333114367368511 \n",
            "\n",
            "Perplexity on the training set:  76.18117366235015 \n",
            "\n",
            "CE on the validation set:  4.612493072169597 \n",
            "\n",
            "Perplexity on the validation set:  100.73497640803947 \n",
            "\n",
            "CE on the validation set: 4.612\n",
            "Model saved!\n",
            "Epoch : 54\n",
            "batch no = 0 / 656, train loss = 6377.069, wps = 1, since beginning = 27 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 5764.193, wps = 1, since beginning = 28 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 5841.795, wps = 1, since beginning = 28 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 6424.589, wps = 1, since beginning = 28 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 5019.807, wps = 1, since beginning = 28 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 5549.402, wps = 1, since beginning = 28 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 5496.936, wps = 1, since beginning = 28 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.32571573775164 \n",
            "\n",
            "Perplexity on the training set:  75.61961730999143 \n",
            "\n",
            "CE on the validation set:  4.6063539997293 \n",
            "\n",
            "Perplexity on the validation set:  100.11845147252795 \n",
            "\n",
            "CE on the validation set: 4.606\n",
            "Model saved!\n",
            "Epoch : 55\n",
            "batch no = 0 / 656, train loss = 5768.875, wps = 1, since beginning = 28 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 5711.579, wps = 1, since beginning = 28 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 6701.063, wps = 1, since beginning = 28 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 6219.396, wps = 1, since beginning = 28 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 6093.741, wps = 1, since beginning = 28 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 6242.036, wps = 1, since beginning = 28 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 7164.412, wps = 1, since beginning = 28 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.313890813957056 \n",
            "\n",
            "Perplexity on the training set:  74.73068722135014 \n",
            "\n",
            "CE on the validation set:  4.606300096680222 \n",
            "\n",
            "Perplexity on the validation set:  100.11305492817105 \n",
            "\n",
            "CE on the validation set: 4.606\n",
            "Model saved!\n",
            "Epoch : 56\n",
            "batch no = 0 / 656, train loss = 5399.271, wps = 1, since beginning = 29 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 5995.421, wps = 1, since beginning = 29 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 5874.905, wps = 1, since beginning = 29 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 6127.129, wps = 1, since beginning = 29 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 5919.378, wps = 1, since beginning = 29 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 6072.928, wps = 1, since beginning = 29 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 5355.887, wps = 1, since beginning = 29 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.303835309203368 \n",
            "\n",
            "Perplexity on the training set:  73.98299793741647 \n",
            "\n",
            "CE on the validation set:  4.602538239767401 \n",
            "\n",
            "Perplexity on the validation set:  99.73715143131182 \n",
            "\n",
            "CE on the validation set: 4.603\n",
            "Model saved!\n",
            "Epoch : 57\n",
            "batch no = 0 / 656, train loss = 5385.760, wps = 1, since beginning = 29 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 6666.465, wps = 1, since beginning = 29 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 5830.201, wps = 1, since beginning = 29 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 6221.327, wps = 1, since beginning = 29 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 5788.569, wps = 1, since beginning = 29 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 6324.865, wps = 1, since beginning = 29 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 5962.673, wps = 1, since beginning = 30 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.295800780699582 \n",
            "\n",
            "Perplexity on the training set:  73.39096098541783 \n",
            "\n",
            "CE on the validation set:  4.597680253399391 \n",
            "\n",
            "Perplexity on the validation set:  99.25380470575904 \n",
            "\n",
            "CE on the validation set: 4.598\n",
            "Model saved!\n",
            "Epoch : 58\n",
            "batch no = 0 / 656, train loss = 7089.964, wps = 1, since beginning = 30 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 6352.342, wps = 1, since beginning = 30 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 5268.004, wps = 1, since beginning = 30 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 5417.915, wps = 1, since beginning = 30 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 6561.587, wps = 1, since beginning = 30 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 6856.994, wps = 1, since beginning = 30 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 5460.129, wps = 1, since beginning = 30 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.285676346889253 \n",
            "\n",
            "Perplexity on the training set:  72.65166783596469 \n",
            "\n",
            "CE on the validation set:  4.5984261765273065 \n",
            "\n",
            "Perplexity on the validation set:  99.32786803356318 \n",
            "\n",
            "Epoch : 59\n",
            "batch no = 0 / 656, train loss = 5472.623, wps = 1, since beginning = 30 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 5684.188, wps = 1, since beginning = 30 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 5269.384, wps = 1, since beginning = 30 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 5834.137, wps = 1, since beginning = 30 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 5852.026, wps = 1, since beginning = 30 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 6393.153, wps = 1, since beginning = 30 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 5827.367, wps = 1, since beginning = 31 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.277792686846454 \n",
            "\n",
            "Perplexity on the training set:  72.08115859053378 \n",
            "\n",
            "CE on the validation set:  4.599863089560336 \n",
            "\n",
            "Perplexity on the validation set:  99.47069613288737 \n",
            "\n",
            "Epoch : 60\n",
            "batch no = 0 / 656, train loss = 5535.241, wps = 1, since beginning = 31 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 5074.848, wps = 1, since beginning = 31 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 6176.520, wps = 1, since beginning = 31 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 5310.531, wps = 1, since beginning = 31 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 6043.513, wps = 1, since beginning = 31 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 6274.280, wps = 1, since beginning = 31 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 5389.748, wps = 1, since beginning = 31 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.268835460874362 \n",
            "\n",
            "Perplexity on the training set:  71.43839435447316 \n",
            "\n",
            "CE on the validation set:  4.5967636765441835 \n",
            "\n",
            "Perplexity on the validation set:  99.1628726450517 \n",
            "\n",
            "CE on the validation set: 4.597\n",
            "Model saved!\n",
            "Epoch : 61\n",
            "batch no = 0 / 656, train loss = 5360.690, wps = 1, since beginning = 31 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 6303.083, wps = 1, since beginning = 31 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 5724.474, wps = 1, since beginning = 31 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 5891.483, wps = 1, since beginning = 31 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 5305.328, wps = 1, since beginning = 31 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 5455.920, wps = 1, since beginning = 32 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 5584.364, wps = 1, since beginning = 32 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.258941446769703 \n",
            "\n",
            "Perplexity on the training set:  70.73506697608931 \n",
            "\n",
            "CE on the validation set:  4.591569603909242 \n",
            "\n",
            "Perplexity on the validation set:  98.64914879628931 \n",
            "\n",
            "CE on the validation set: 4.592\n",
            "Model saved!\n",
            "Epoch : 62\n",
            "batch no = 0 / 656, train loss = 4867.890, wps = 1, since beginning = 32 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 5318.678, wps = 1, since beginning = 32 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 5160.655, wps = 1, since beginning = 32 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 5276.790, wps = 1, since beginning = 32 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 5969.566, wps = 1, since beginning = 32 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 6168.688, wps = 1, since beginning = 32 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 6049.834, wps = 1, since beginning = 32 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.24871335933801 \n",
            "\n",
            "Perplexity on the training set:  70.01526987545698 \n",
            "\n",
            "CE on the validation set:  4.591534950693425 \n",
            "\n",
            "Perplexity on the validation set:  98.64573034527633 \n",
            "\n",
            "CE on the validation set: 4.592\n",
            "Model saved!\n",
            "Epoch : 63\n",
            "batch no = 0 / 656, train loss = 5870.112, wps = 1, since beginning = 32 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 5966.933, wps = 1, since beginning = 32 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 4927.542, wps = 1, since beginning = 32 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 4893.471, wps = 1, since beginning = 32 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 5700.128, wps = 1, since beginning = 32 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 5449.407, wps = 1, since beginning = 33 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 6199.288, wps = 1, since beginning = 33 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.243135250451226 \n",
            "\n",
            "Perplexity on the training set:  69.62580432683387 \n",
            "\n",
            "CE on the validation set:  4.589333339352642 \n",
            "\n",
            "Perplexity on the validation set:  98.42878968376631 \n",
            "\n",
            "CE on the validation set: 4.589\n",
            "Model saved!\n",
            "Epoch : 64\n",
            "batch no = 0 / 656, train loss = 5532.924, wps = 1, since beginning = 33 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 5985.698, wps = 1, since beginning = 33 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 5667.195, wps = 1, since beginning = 33 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 5811.930, wps = 1, since beginning = 33 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 6027.031, wps = 1, since beginning = 33 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 5256.468, wps = 1, since beginning = 33 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 5191.129, wps = 1, since beginning = 33 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.234972273839023 \n",
            "\n",
            "Perplexity on the training set:  69.05976394484128 \n",
            "\n",
            "CE on the validation set:  4.590776507262999 \n",
            "\n",
            "Perplexity on the validation set:  98.57094150429405 \n",
            "\n",
            "Epoch : 65\n",
            "batch no = 0 / 656, train loss = 6304.197, wps = 1, since beginning = 33 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 5532.011, wps = 1, since beginning = 33 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 5395.289, wps = 1, since beginning = 33 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 5903.561, wps = 1, since beginning = 33 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 5943.319, wps = 1, since beginning = 34 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 5921.301, wps = 1, since beginning = 34 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 5654.621, wps = 1, since beginning = 34 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.2249142406200475 \n",
            "\n",
            "Perplexity on the training set:  68.36864004495804 \n",
            "\n",
            "CE on the validation set:  4.586835727105475 \n",
            "\n",
            "Perplexity on the validation set:  98.18325948043592 \n",
            "\n",
            "CE on the validation set: 4.587\n",
            "Model saved!\n",
            "Epoch : 66\n",
            "batch no = 0 / 656, train loss = 6645.121, wps = 1, since beginning = 34 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 5840.751, wps = 1, since beginning = 34 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 5724.157, wps = 1, since beginning = 34 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 5690.043, wps = 1, since beginning = 34 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 6022.809, wps = 1, since beginning = 34 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 4918.007, wps = 1, since beginning = 34 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 5272.182, wps = 1, since beginning = 34 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.218061478794058 \n",
            "\n",
            "Perplexity on the training set:  67.9017276851565 \n",
            "\n",
            "CE on the validation set:  4.585954373050194 \n",
            "\n",
            "Perplexity on the validation set:  98.09676338897172 \n",
            "\n",
            "CE on the validation set: 4.586\n",
            "Model saved!\n",
            "Epoch : 67\n",
            "batch no = 0 / 656, train loss = 4902.446, wps = 1, since beginning = 34 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 5650.062, wps = 1, since beginning = 34 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 5825.951, wps = 1, since beginning = 34 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 6732.302, wps = 1, since beginning = 34 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 5746.669, wps = 1, since beginning = 35 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 4918.469, wps = 1, since beginning = 35 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 5724.079, wps = 1, since beginning = 35 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.20887934248092 \n",
            "\n",
            "Perplexity on the training set:  67.28109847716304 \n",
            "\n",
            "CE on the validation set:  4.585803176597288 \n",
            "\n",
            "Perplexity on the validation set:  98.08193262751325 \n",
            "\n",
            "CE on the validation set: 4.586\n",
            "Model saved!\n",
            "Epoch : 68\n",
            "batch no = 0 / 656, train loss = 5897.944, wps = 1, since beginning = 35 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 5297.398, wps = 1, since beginning = 35 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 5469.142, wps = 1, since beginning = 35 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 5246.271, wps = 1, since beginning = 35 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 5915.815, wps = 1, since beginning = 35 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 5474.953, wps = 1, since beginning = 35 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 5913.383, wps = 1, since beginning = 35 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.2002046648175195 \n",
            "\n",
            "Perplexity on the training set:  66.69998078346207 \n",
            "\n",
            "CE on the validation set:  4.584349738593241 \n",
            "\n",
            "Perplexity on the validation set:  97.93948016710927 \n",
            "\n",
            "CE on the validation set: 4.584\n",
            "Model saved!\n",
            "Epoch : 69\n",
            "batch no = 0 / 656, train loss = 5918.004, wps = 1, since beginning = 35 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 6061.687, wps = 1, since beginning = 35 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 5183.696, wps = 1, since beginning = 35 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 6067.958, wps = 1, since beginning = 35 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 6538.686, wps = 1, since beginning = 36 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 5209.283, wps = 1, since beginning = 36 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 6283.880, wps = 1, since beginning = 36 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.1953154073140615 \n",
            "\n",
            "Perplexity on the training set:  66.37466333039009 \n",
            "\n",
            "CE on the validation set:  4.5815035689999535 \n",
            "\n",
            "Perplexity on the validation set:  97.6611241088559 \n",
            "\n",
            "CE on the validation set: 4.582\n",
            "Model saved!\n",
            "Epoch : 70\n",
            "batch no = 0 / 656, train loss = 5709.771, wps = 1, since beginning = 36 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 5999.379, wps = 1, since beginning = 36 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 5800.186, wps = 1, since beginning = 36 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 5375.191, wps = 1, since beginning = 36 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 6427.474, wps = 1, since beginning = 36 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 5969.039, wps = 1, since beginning = 36 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 6364.979, wps = 1, since beginning = 36 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.187301252565737 \n",
            "\n",
            "Perplexity on the training set:  65.84485233640255 \n",
            "\n",
            "CE on the validation set:  4.57653638421708 \n",
            "\n",
            "Perplexity on the validation set:  97.17722605974836 \n",
            "\n",
            "CE on the validation set: 4.577\n",
            "Model saved!\n",
            "Epoch : 71\n",
            "batch no = 0 / 656, train loss = 6015.199, wps = 1, since beginning = 36 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 5264.898, wps = 1, since beginning = 36 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 6376.479, wps = 1, since beginning = 36 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 5226.023, wps = 1, since beginning = 37 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 5922.913, wps = 1, since beginning = 37 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 5405.019, wps = 1, since beginning = 37 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 5042.725, wps = 1, since beginning = 37 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.178812063438208 \n",
            "\n",
            "Perplexity on the training set:  65.28824883075262 \n",
            "\n",
            "CE on the validation set:  4.579877402857361 \n",
            "\n",
            "Perplexity on the validation set:  97.50243995375162 \n",
            "\n",
            "Epoch : 72\n",
            "batch no = 0 / 656, train loss = 5742.165, wps = 1, since beginning = 37 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 5134.264, wps = 1, since beginning = 37 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 6156.773, wps = 1, since beginning = 37 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 5426.636, wps = 1, since beginning = 37 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 5852.500, wps = 1, since beginning = 37 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 5588.305, wps = 1, since beginning = 37 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 6215.342, wps = 1, since beginning = 37 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.170984365664649 \n",
            "\n",
            "Perplexity on the training set:  64.77918714057245 \n",
            "\n",
            "CE on the validation set:  4.581044568592215 \n",
            "\n",
            "Perplexity on the validation set:  97.61630789918587 \n",
            "\n",
            "Epoch : 73\n",
            "batch no = 0 / 656, train loss = 5333.331, wps = 1, since beginning = 37 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 5952.421, wps = 1, since beginning = 37 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 5946.667, wps = 1, since beginning = 37 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 5894.769, wps = 1, since beginning = 38 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 5759.416, wps = 1, since beginning = 38 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 5929.787, wps = 1, since beginning = 38 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 6160.649, wps = 1, since beginning = 38 mins, cuda memory = 0.873 GBs\n",
            "CE on the training set:  4.164957042183033 \n",
            "\n",
            "Perplexity on the training set:  64.38991633380944 \n",
            "\n",
            "CE on the validation set:  4.5799410872829 \n",
            "\n",
            "Perplexity on the validation set:  97.50864953835357 \n",
            "\n",
            "Early stopping triggered with counter 3 and patience 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train and valid loss are plotted and it is possible to observe that the regularization techniques are really preventing from overfitting."
      ],
      "metadata": {
        "id": "shM33ROxd9MH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(num = 3, figsize=(8, 5)).patch.set_facecolor('white')\n",
        "plt.title('Regolarized: Train and Valid Losses')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.plot(sampled_epochs_regolarized, loss_train_regolarized, label='Train loss')\n",
        "plt.plot(sampled_epochs_regolarized, loss_valid_regolarized, label='Valid loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "voCXLa1y1Udx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "1790c0d2-d1f9-4bd9-c955-882a91f21ba1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also here we perform 4 runs in order to have more stable results."
      ],
      "metadata": {
        "id": "1zJTZzlFd-mC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "runs = 4\n",
        "loss_runs = []\n",
        "perp_runs = []\n",
        "\n",
        "for r in range(1, runs+1):\n",
        "    model = Model(vocab_size, embed_size_reg, hidden_size_reg, layer_num_reg, w_drop, dropout_i, dropout_o, dropout_e, pad_index=0).to(device)\n",
        "    model.apply(init_weights)\n",
        "\n",
        "    optimizer = NTASGD(model.parameters(), lr=lr_reg, n=5, weight_decay=weight_decay, fine_tuning=False)\n",
        "    criterion_slots = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN, reduction='sum')\n",
        "    # Start a timer\n",
        "    tic = timeit.default_timer() \n",
        "\n",
        "    n_epochs = 200\n",
        "\n",
        "    # Create the early stopping criterion\n",
        "    criterion = EarlyStopping(patience=2, verbose=True)\n",
        "    best_val = 1e10\n",
        "\n",
        "    # Loop over the number of runs\n",
        "    for x in range(1,n_epochs+1):\n",
        "        print(\"Run : {:d},\".format(r), end=' ')\n",
        "        print(\"Epoch : {:d}\".format(x))\n",
        "\n",
        "        _, words_train, loss_train = train(train_loader, model, optimizer, criterion_slots, tic)\n",
        "\n",
        "        if x % 5 == 0:\n",
        "            loss_valid, words_valid = evaluate(valid_loader, criterion_slots, model)\n",
        "            ce_valid = np.asarray(loss_valid).sum() / np.asarray(words_valid).sum()\n",
        "\n",
        "            if ce_valid < best_val:\n",
        "              best_val = ce_valid\n",
        "\n",
        "            if criterion.step(ce_valid):\n",
        "              break\n",
        "\n",
        "    \"\"\" Cross entropy loss and perplexity on the test set \"\"\"\n",
        "    loss_test, words_test = evaluate(test_loader, criterion_slots, model)\n",
        "\n",
        "    # Cross entropy\n",
        "    ce_test = np.asarray(loss_test).sum() / np.asarray(words_test).sum()\n",
        "    loss_runs.append(ce_test)\n",
        "\n",
        "    # Perplexity\n",
        "    perplexity_test = np.exp(ce_test)\n",
        "    perp_runs.append(perplexity_test)\n",
        "\n",
        "loss_runs = np.asarray(loss_runs)\n",
        "perp_runs = np.asarray(perp_runs)\n",
        "\n",
        "# Computation of mean and standard deviation values among the results of the 4 runs\n",
        "print('Test loss', round(loss_runs.mean(),3), '+-', round(loss_runs.std(),3))\n",
        "print('Test perplexity', round(perp_runs.mean(), 3), '+-', round(perp_runs.std(), 3))"
      ],
      "metadata": {
        "id": "YD6tUCr5rsyX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b57cb828-d35e-4ef2-deb3-bcf253e9057b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run : 1, Epoch : 1\n",
            "batch no = 0 / 656, train loss = 11061.963, wps = 1058, since beginning = 0 mins, cuda memory = 0.562 GBs\n",
            "batch no = 100 / 656, train loss = 8675.430, wps = 223, since beginning = 0 mins, cuda memory = 0.834 GBs\n",
            "batch no = 200 / 656, train loss = 9237.012, wps = 139, since beginning = 0 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 8108.169, wps = 88, since beginning = 0 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 7921.439, wps = 65, since beginning = 0 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 8333.383, wps = 57, since beginning = 0 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 7849.701, wps = 46, since beginning = 0 mins, cuda memory = 0.873 GBs\n",
            "Run : 1, Epoch : 2\n",
            "batch no = 0 / 656, train loss = 8259.416, wps = 43, since beginning = 1 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 7664.753, wps = 35, since beginning = 1 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 7930.415, wps = 33, since beginning = 1 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 7810.974, wps = 30, since beginning = 1 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 8611.364, wps = 29, since beginning = 1 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 8088.898, wps = 25, since beginning = 1 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 8670.622, wps = 25, since beginning = 1 mins, cuda memory = 0.873 GBs\n",
            "Run : 1, Epoch : 3\n",
            "batch no = 0 / 656, train loss = 8028.184, wps = 22, since beginning = 1 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 6953.899, wps = 19, since beginning = 1 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 8200.541, wps = 20, since beginning = 1 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 8891.801, wps = 20, since beginning = 1 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 7685.802, wps = 16, since beginning = 1 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 6507.406, wps = 14, since beginning = 1 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 8720.808, wps = 17, since beginning = 1 mins, cuda memory = 0.873 GBs\n",
            "Run : 1, Epoch : 4\n",
            "batch no = 0 / 656, train loss = 8222.403, wps = 16, since beginning = 1 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 7791.975, wps = 14, since beginning = 2 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 7730.579, wps = 14, since beginning = 2 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 7596.146, wps = 13, since beginning = 2 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 7471.050, wps = 12, since beginning = 2 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 7549.169, wps = 12, since beginning = 2 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 7943.257, wps = 12, since beginning = 2 mins, cuda memory = 0.873 GBs\n",
            "Run : 1, Epoch : 5\n",
            "batch no = 0 / 656, train loss = 7384.889, wps = 11, since beginning = 2 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 7555.112, wps = 11, since beginning = 2 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 7853.950, wps = 11, since beginning = 2 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 7588.159, wps = 10, since beginning = 2 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 7884.920, wps = 10, since beginning = 2 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 6631.624, wps = 9, since beginning = 2 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 7886.821, wps = 10, since beginning = 2 mins, cuda memory = 0.873 GBs\n",
            "Run : 1, Epoch : 6\n",
            "batch no = 0 / 656, train loss = 6715.999, wps = 8, since beginning = 2 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 7794.128, wps = 9, since beginning = 3 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 7897.553, wps = 9, since beginning = 3 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 8210.471, wps = 9, since beginning = 3 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 7400.303, wps = 8, since beginning = 3 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 6536.044, wps = 7, since beginning = 3 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 6860.084, wps = 7, since beginning = 3 mins, cuda memory = 0.873 GBs\n",
            "Run : 1, Epoch : 7\n",
            "batch no = 0 / 656, train loss = 6921.588, wps = 7, since beginning = 3 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 7289.202, wps = 7, since beginning = 3 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 7235.670, wps = 7, since beginning = 3 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 7567.894, wps = 7, since beginning = 3 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 7799.342, wps = 7, since beginning = 3 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 7213.628, wps = 7, since beginning = 3 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 8213.843, wps = 7, since beginning = 3 mins, cuda memory = 0.873 GBs\n",
            "Run : 1, Epoch : 8\n",
            "batch no = 0 / 656, train loss = 6829.424, wps = 6, since beginning = 3 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 7906.690, wps = 7, since beginning = 4 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 7671.514, wps = 7, since beginning = 4 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 6577.785, wps = 6, since beginning = 4 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 7907.340, wps = 6, since beginning = 4 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 7428.336, wps = 6, since beginning = 4 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 6469.647, wps = 5, since beginning = 4 mins, cuda memory = 0.873 GBs\n",
            "Run : 1, Epoch : 9\n",
            "batch no = 0 / 656, train loss = 7239.752, wps = 6, since beginning = 4 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 7185.522, wps = 6, since beginning = 4 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 7989.442, wps = 6, since beginning = 4 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 7908.118, wps = 6, since beginning = 4 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 7338.826, wps = 5, since beginning = 4 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 6866.871, wps = 5, since beginning = 4 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 7858.259, wps = 6, since beginning = 4 mins, cuda memory = 0.873 GBs\n",
            "Run : 1, Epoch : 10\n",
            "batch no = 0 / 656, train loss = 7236.461, wps = 5, since beginning = 4 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 6534.205, wps = 5, since beginning = 5 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 5943.206, wps = 4, since beginning = 5 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 7272.846, wps = 5, since beginning = 5 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 7475.921, wps = 5, since beginning = 5 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 7174.484, wps = 5, since beginning = 5 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 6145.763, wps = 4, since beginning = 5 mins, cuda memory = 0.873 GBs\n",
            "Run : 1, Epoch : 11\n",
            "batch no = 0 / 656, train loss = 7097.405, wps = 5, since beginning = 5 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 6734.585, wps = 4, since beginning = 5 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 6932.468, wps = 4, since beginning = 5 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 7813.292, wps = 5, since beginning = 5 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 7042.717, wps = 4, since beginning = 5 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 7478.076, wps = 4, since beginning = 5 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 6976.399, wps = 4, since beginning = 5 mins, cuda memory = 0.873 GBs\n",
            "Run : 1, Epoch : 12\n",
            "batch no = 0 / 656, train loss = 6929.875, wps = 4, since beginning = 5 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 6798.159, wps = 4, since beginning = 6 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 7529.380, wps = 4, since beginning = 6 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 7185.765, wps = 4, since beginning = 6 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 7754.526, wps = 4, since beginning = 6 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 6680.005, wps = 4, since beginning = 6 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 6349.125, wps = 4, since beginning = 6 mins, cuda memory = 0.873 GBs\n",
            "Run : 1, Epoch : 13\n",
            "batch no = 0 / 656, train loss = 7151.558, wps = 4, since beginning = 6 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 7358.365, wps = 4, since beginning = 6 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 6283.648, wps = 3, since beginning = 6 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 7089.112, wps = 4, since beginning = 6 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 7129.125, wps = 4, since beginning = 6 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 6264.130, wps = 3, since beginning = 6 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 6431.661, wps = 3, since beginning = 6 mins, cuda memory = 0.873 GBs\n",
            "Run : 1, Epoch : 14\n",
            "batch no = 0 / 656, train loss = 7138.001, wps = 4, since beginning = 6 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 6752.369, wps = 3, since beginning = 7 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 6991.033, wps = 3, since beginning = 7 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 7221.992, wps = 4, since beginning = 7 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 7045.286, wps = 3, since beginning = 7 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 6370.496, wps = 3, since beginning = 7 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 6082.816, wps = 3, since beginning = 7 mins, cuda memory = 0.873 GBs\n",
            "Run : 1, Epoch : 15\n",
            "batch no = 0 / 656, train loss = 7134.119, wps = 3, since beginning = 7 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 6551.923, wps = 3, since beginning = 7 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 6567.349, wps = 3, since beginning = 7 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 7122.491, wps = 3, since beginning = 7 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 6515.388, wps = 3, since beginning = 7 mins, cuda memory = 0.873 GBs\n",
            "batch no = 500 / 656, train loss = 7357.500, wps = 3, since beginning = 7 mins, cuda memory = 0.873 GBs\n",
            "batch no = 600 / 656, train loss = 6346.403, wps = 3, since beginning = 7 mins, cuda memory = 0.873 GBs\n",
            "Run : 1, Epoch : 16\n",
            "batch no = 0 / 656, train loss = 6967.479, wps = 3, since beginning = 7 mins, cuda memory = 0.873 GBs\n",
            "batch no = 100 / 656, train loss = 7234.292, wps = 3, since beginning = 8 mins, cuda memory = 0.873 GBs\n",
            "batch no = 200 / 656, train loss = 7033.418, wps = 3, since beginning = 8 mins, cuda memory = 0.873 GBs\n",
            "batch no = 300 / 656, train loss = 6375.978, wps = 3, since beginning = 8 mins, cuda memory = 0.873 GBs\n",
            "batch no = 400 / 656, train loss = 7254.610, wps = 3, since beginning = 8 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 6803.279, wps = 3, since beginning = 8 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 6714.119, wps = 3, since beginning = 8 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 17\n",
            "batch no = 0 / 656, train loss = 6281.564, wps = 3, since beginning = 8 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 6758.755, wps = 3, since beginning = 8 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 7048.620, wps = 3, since beginning = 8 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 6594.472, wps = 3, since beginning = 8 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 6361.718, wps = 3, since beginning = 8 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 5956.760, wps = 2, since beginning = 8 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 7243.437, wps = 3, since beginning = 8 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 18\n",
            "batch no = 0 / 656, train loss = 7179.694, wps = 3, since beginning = 8 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 5756.698, wps = 2, since beginning = 9 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 6613.709, wps = 3, since beginning = 9 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 6635.451, wps = 3, since beginning = 9 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 6548.304, wps = 2, since beginning = 9 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 6750.876, wps = 3, since beginning = 9 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 6666.416, wps = 3, since beginning = 9 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 19\n",
            "batch no = 0 / 656, train loss = 6714.360, wps = 2, since beginning = 9 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 6591.620, wps = 3, since beginning = 9 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 6979.236, wps = 3, since beginning = 9 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 5920.874, wps = 3, since beginning = 9 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 6710.525, wps = 2, since beginning = 9 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 6293.731, wps = 2, since beginning = 9 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 5625.249, wps = 2, since beginning = 9 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 20\n",
            "batch no = 0 / 656, train loss = 6789.477, wps = 2, since beginning = 9 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 7507.830, wps = 3, since beginning = 10 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 6323.534, wps = 2, since beginning = 10 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 6931.360, wps = 2, since beginning = 10 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 6573.961, wps = 2, since beginning = 10 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 6255.631, wps = 2, since beginning = 10 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 6523.410, wps = 2, since beginning = 10 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 21\n",
            "batch no = 0 / 656, train loss = 6491.525, wps = 2, since beginning = 10 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 5861.956, wps = 2, since beginning = 10 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 5847.002, wps = 2, since beginning = 10 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 7034.299, wps = 2, since beginning = 10 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 6749.803, wps = 2, since beginning = 10 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 6356.365, wps = 2, since beginning = 10 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 6293.844, wps = 2, since beginning = 10 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 22\n",
            "batch no = 0 / 656, train loss = 5864.033, wps = 2, since beginning = 10 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 6719.613, wps = 2, since beginning = 11 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 6070.424, wps = 2, since beginning = 11 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 5970.492, wps = 2, since beginning = 11 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 6096.371, wps = 2, since beginning = 11 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 6328.289, wps = 2, since beginning = 11 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 6133.222, wps = 2, since beginning = 11 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 23\n",
            "batch no = 0 / 656, train loss = 6350.845, wps = 2, since beginning = 11 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 6415.870, wps = 2, since beginning = 11 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 6590.705, wps = 2, since beginning = 11 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 6034.089, wps = 2, since beginning = 11 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 6213.859, wps = 2, since beginning = 11 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 6256.490, wps = 2, since beginning = 11 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 5723.113, wps = 2, since beginning = 11 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 24\n",
            "batch no = 0 / 656, train loss = 6253.893, wps = 2, since beginning = 11 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 6671.738, wps = 2, since beginning = 12 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 5767.032, wps = 2, since beginning = 12 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 6012.479, wps = 2, since beginning = 12 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 6159.958, wps = 2, since beginning = 12 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 6175.452, wps = 2, since beginning = 12 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 6015.210, wps = 2, since beginning = 12 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 25\n",
            "batch no = 0 / 656, train loss = 6569.863, wps = 2, since beginning = 12 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 5800.123, wps = 2, since beginning = 12 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 6972.947, wps = 2, since beginning = 12 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 5751.098, wps = 2, since beginning = 12 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 6636.686, wps = 2, since beginning = 12 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 6412.778, wps = 2, since beginning = 12 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 7044.050, wps = 2, since beginning = 12 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 26\n",
            "batch no = 0 / 656, train loss = 6615.192, wps = 2, since beginning = 12 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 6224.961, wps = 2, since beginning = 13 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 6577.208, wps = 2, since beginning = 13 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 6072.580, wps = 2, since beginning = 13 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 5565.410, wps = 2, since beginning = 13 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 6043.942, wps = 2, since beginning = 13 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 6141.658, wps = 2, since beginning = 13 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 27\n",
            "batch no = 0 / 656, train loss = 5731.736, wps = 2, since beginning = 13 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 6277.394, wps = 2, since beginning = 13 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 5887.390, wps = 2, since beginning = 13 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 6135.388, wps = 2, since beginning = 13 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 6312.340, wps = 2, since beginning = 13 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 6593.021, wps = 2, since beginning = 13 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 6464.648, wps = 2, since beginning = 13 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 28\n",
            "batch no = 0 / 656, train loss = 6321.745, wps = 2, since beginning = 13 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 7330.354, wps = 2, since beginning = 14 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 5987.767, wps = 2, since beginning = 14 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 6130.512, wps = 2, since beginning = 14 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 6048.770, wps = 2, since beginning = 14 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 5945.782, wps = 1, since beginning = 14 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 7427.402, wps = 2, since beginning = 14 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 29\n",
            "batch no = 0 / 656, train loss = 5926.475, wps = 2, since beginning = 14 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 6993.874, wps = 2, since beginning = 14 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 6151.499, wps = 2, since beginning = 14 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 6832.701, wps = 2, since beginning = 14 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 6541.907, wps = 2, since beginning = 14 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 6298.166, wps = 2, since beginning = 14 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 6176.318, wps = 2, since beginning = 14 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 30\n",
            "batch no = 0 / 656, train loss = 5973.276, wps = 1, since beginning = 14 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 6258.542, wps = 2, since beginning = 14 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 6241.469, wps = 2, since beginning = 15 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 6125.241, wps = 1, since beginning = 15 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 6540.831, wps = 2, since beginning = 15 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 6669.820, wps = 2, since beginning = 15 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 6785.002, wps = 2, since beginning = 15 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 31\n",
            "batch no = 0 / 656, train loss = 5151.598, wps = 1, since beginning = 15 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 6272.339, wps = 1, since beginning = 15 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 6416.852, wps = 2, since beginning = 15 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 6478.821, wps = 2, since beginning = 15 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 6701.359, wps = 2, since beginning = 15 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 5498.515, wps = 1, since beginning = 15 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 6505.204, wps = 1, since beginning = 15 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 32\n",
            "batch no = 0 / 656, train loss = 5967.880, wps = 1, since beginning = 15 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 5881.853, wps = 1, since beginning = 15 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 6133.197, wps = 1, since beginning = 16 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 6175.509, wps = 1, since beginning = 16 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 6346.371, wps = 1, since beginning = 16 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 6351.940, wps = 1, since beginning = 16 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 5873.823, wps = 1, since beginning = 16 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 33\n",
            "batch no = 0 / 656, train loss = 6296.927, wps = 2, since beginning = 16 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 6043.677, wps = 1, since beginning = 16 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 6122.541, wps = 1, since beginning = 16 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 7030.735, wps = 2, since beginning = 16 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 6943.398, wps = 2, since beginning = 16 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 5624.958, wps = 1, since beginning = 16 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 6644.353, wps = 1, since beginning = 16 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 34\n",
            "batch no = 0 / 656, train loss = 6338.869, wps = 1, since beginning = 16 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 6583.655, wps = 1, since beginning = 16 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 6913.877, wps = 1, since beginning = 17 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 6399.669, wps = 1, since beginning = 17 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 5636.573, wps = 1, since beginning = 17 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 6497.407, wps = 1, since beginning = 17 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 5924.349, wps = 1, since beginning = 17 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 35\n",
            "batch no = 0 / 656, train loss = 6337.127, wps = 1, since beginning = 17 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 6455.599, wps = 1, since beginning = 17 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 5690.995, wps = 1, since beginning = 17 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 5714.478, wps = 1, since beginning = 17 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 6381.747, wps = 1, since beginning = 17 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 6679.212, wps = 1, since beginning = 17 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 6347.104, wps = 1, since beginning = 17 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 36\n",
            "batch no = 0 / 656, train loss = 6393.240, wps = 1, since beginning = 17 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 6177.389, wps = 1, since beginning = 17 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 6025.051, wps = 1, since beginning = 18 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 6503.360, wps = 1, since beginning = 18 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 5706.785, wps = 1, since beginning = 18 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 5879.213, wps = 1, since beginning = 18 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 6538.939, wps = 1, since beginning = 18 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 37\n",
            "batch no = 0 / 656, train loss = 6127.073, wps = 1, since beginning = 18 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 6065.745, wps = 1, since beginning = 18 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 6318.461, wps = 1, since beginning = 18 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 5691.176, wps = 1, since beginning = 18 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 7097.101, wps = 1, since beginning = 18 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 6520.039, wps = 1, since beginning = 18 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 6203.658, wps = 1, since beginning = 18 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 38\n",
            "batch no = 0 / 656, train loss = 6264.815, wps = 1, since beginning = 18 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 6225.315, wps = 1, since beginning = 18 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 5766.146, wps = 1, since beginning = 19 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 6622.173, wps = 1, since beginning = 19 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 6121.839, wps = 1, since beginning = 19 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 6384.935, wps = 1, since beginning = 19 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 5130.526, wps = 1, since beginning = 19 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 39\n",
            "batch no = 0 / 656, train loss = 6209.438, wps = 1, since beginning = 19 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 6355.557, wps = 1, since beginning = 19 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 6050.083, wps = 1, since beginning = 19 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 6487.604, wps = 1, since beginning = 19 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 6610.172, wps = 1, since beginning = 19 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 6257.882, wps = 1, since beginning = 19 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 6140.688, wps = 1, since beginning = 19 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 40\n",
            "batch no = 0 / 656, train loss = 5938.178, wps = 1, since beginning = 19 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 5993.561, wps = 1, since beginning = 19 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 6051.010, wps = 1, since beginning = 20 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 6043.180, wps = 1, since beginning = 20 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 6054.196, wps = 1, since beginning = 20 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 5713.453, wps = 1, since beginning = 20 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 6799.618, wps = 1, since beginning = 20 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 41\n",
            "batch no = 0 / 656, train loss = 5855.691, wps = 1, since beginning = 20 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 6174.393, wps = 1, since beginning = 20 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 6552.868, wps = 1, since beginning = 20 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 6446.420, wps = 1, since beginning = 20 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 5848.737, wps = 1, since beginning = 20 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 6214.896, wps = 1, since beginning = 20 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 5865.844, wps = 1, since beginning = 20 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 42\n",
            "batch no = 0 / 656, train loss = 6047.483, wps = 1, since beginning = 20 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 5772.373, wps = 1, since beginning = 20 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 6106.463, wps = 1, since beginning = 21 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 6135.518, wps = 1, since beginning = 21 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 5786.783, wps = 1, since beginning = 21 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 5286.929, wps = 1, since beginning = 21 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 5920.877, wps = 1, since beginning = 21 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 43\n",
            "batch no = 0 / 656, train loss = 5457.580, wps = 1, since beginning = 21 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 6595.597, wps = 1, since beginning = 21 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 6238.632, wps = 1, since beginning = 21 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 5663.420, wps = 1, since beginning = 21 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 7013.145, wps = 1, since beginning = 21 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 6081.242, wps = 1, since beginning = 21 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 5852.446, wps = 1, since beginning = 21 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 44\n",
            "batch no = 0 / 656, train loss = 5746.645, wps = 1, since beginning = 21 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 5754.023, wps = 1, since beginning = 21 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 6034.200, wps = 1, since beginning = 22 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 6391.390, wps = 1, since beginning = 22 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 5950.693, wps = 1, since beginning = 22 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 5063.423, wps = 1, since beginning = 22 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 6309.420, wps = 1, since beginning = 22 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 45\n",
            "batch no = 0 / 656, train loss = 6203.137, wps = 1, since beginning = 22 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 5323.136, wps = 1, since beginning = 22 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 5743.109, wps = 1, since beginning = 22 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 5818.702, wps = 1, since beginning = 22 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 5763.072, wps = 1, since beginning = 22 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 5333.145, wps = 1, since beginning = 22 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 5785.047, wps = 1, since beginning = 22 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 46\n",
            "batch no = 0 / 656, train loss = 6008.689, wps = 1, since beginning = 22 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 6064.838, wps = 1, since beginning = 22 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 6207.804, wps = 1, since beginning = 23 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 5787.884, wps = 1, since beginning = 23 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 5800.159, wps = 1, since beginning = 23 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 5864.172, wps = 1, since beginning = 23 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 5759.278, wps = 1, since beginning = 23 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 47\n",
            "batch no = 0 / 656, train loss = 6080.207, wps = 1, since beginning = 23 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 6411.177, wps = 1, since beginning = 23 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 5625.962, wps = 1, since beginning = 23 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 6356.939, wps = 1, since beginning = 23 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 5700.882, wps = 1, since beginning = 23 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 5651.145, wps = 1, since beginning = 23 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 5767.699, wps = 1, since beginning = 23 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 48\n",
            "batch no = 0 / 656, train loss = 5761.494, wps = 1, since beginning = 23 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 5875.589, wps = 1, since beginning = 23 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 5200.441, wps = 1, since beginning = 24 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 5716.725, wps = 1, since beginning = 24 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 6330.891, wps = 1, since beginning = 24 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 5761.375, wps = 1, since beginning = 24 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 6104.724, wps = 1, since beginning = 24 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 49\n",
            "batch no = 0 / 656, train loss = 6028.701, wps = 1, since beginning = 24 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 5654.854, wps = 1, since beginning = 24 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 6112.351, wps = 1, since beginning = 24 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 6374.020, wps = 1, since beginning = 24 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 5498.831, wps = 1, since beginning = 24 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 5939.324, wps = 1, since beginning = 24 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 6062.392, wps = 1, since beginning = 24 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 50\n",
            "batch no = 0 / 656, train loss = 5545.357, wps = 1, since beginning = 24 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 5765.919, wps = 1, since beginning = 24 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 6074.609, wps = 1, since beginning = 25 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 5315.427, wps = 1, since beginning = 25 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 5560.124, wps = 1, since beginning = 25 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 6383.038, wps = 1, since beginning = 25 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 5608.917, wps = 1, since beginning = 25 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 51\n",
            "batch no = 0 / 656, train loss = 5867.080, wps = 1, since beginning = 25 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 6296.764, wps = 1, since beginning = 25 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 5590.752, wps = 1, since beginning = 25 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 6710.174, wps = 1, since beginning = 25 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 5382.878, wps = 1, since beginning = 25 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 5380.788, wps = 1, since beginning = 25 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 5542.729, wps = 1, since beginning = 25 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 52\n",
            "batch no = 0 / 656, train loss = 5668.029, wps = 1, since beginning = 25 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 5826.290, wps = 1, since beginning = 25 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 5651.583, wps = 1, since beginning = 26 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 5740.693, wps = 1, since beginning = 26 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 6143.125, wps = 1, since beginning = 26 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 6271.267, wps = 1, since beginning = 26 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 6040.456, wps = 1, since beginning = 26 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 53\n",
            "batch no = 0 / 656, train loss = 5832.827, wps = 1, since beginning = 26 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 6659.923, wps = 1, since beginning = 26 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 5863.156, wps = 1, since beginning = 26 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 5848.368, wps = 1, since beginning = 26 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 6123.514, wps = 1, since beginning = 26 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 5495.570, wps = 1, since beginning = 26 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 5629.787, wps = 1, since beginning = 26 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 54\n",
            "batch no = 0 / 656, train loss = 6452.636, wps = 1, since beginning = 26 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 6324.214, wps = 1, since beginning = 26 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 6149.475, wps = 1, since beginning = 27 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 6034.659, wps = 1, since beginning = 27 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 5512.611, wps = 1, since beginning = 27 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 5545.167, wps = 1, since beginning = 27 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 6366.303, wps = 1, since beginning = 27 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 55\n",
            "batch no = 0 / 656, train loss = 5632.736, wps = 1, since beginning = 27 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 5934.960, wps = 1, since beginning = 27 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 6332.888, wps = 1, since beginning = 27 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 5957.359, wps = 1, since beginning = 27 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 6457.637, wps = 1, since beginning = 27 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 6538.554, wps = 1, since beginning = 27 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 5522.467, wps = 1, since beginning = 27 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 56\n",
            "batch no = 0 / 656, train loss = 5644.161, wps = 1, since beginning = 27 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 5394.512, wps = 1, since beginning = 27 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 6069.935, wps = 1, since beginning = 28 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 6070.240, wps = 1, since beginning = 28 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 5794.794, wps = 1, since beginning = 28 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 5328.938, wps = 1, since beginning = 28 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 5673.855, wps = 1, since beginning = 28 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 57\n",
            "batch no = 0 / 656, train loss = 5090.003, wps = 1, since beginning = 28 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 5713.805, wps = 1, since beginning = 28 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 6027.338, wps = 1, since beginning = 28 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 5997.483, wps = 1, since beginning = 28 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 5437.289, wps = 1, since beginning = 28 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 5693.078, wps = 1, since beginning = 28 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 5756.723, wps = 1, since beginning = 28 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 58\n",
            "batch no = 0 / 656, train loss = 5592.712, wps = 1, since beginning = 28 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 5869.620, wps = 1, since beginning = 28 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 5509.796, wps = 1, since beginning = 28 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 5807.685, wps = 1, since beginning = 29 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 6494.467, wps = 1, since beginning = 29 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 5432.634, wps = 1, since beginning = 29 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 5786.073, wps = 1, since beginning = 29 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 59\n",
            "batch no = 0 / 656, train loss = 5774.550, wps = 1, since beginning = 29 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 5493.979, wps = 1, since beginning = 29 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 5376.265, wps = 1, since beginning = 29 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 5972.460, wps = 1, since beginning = 29 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 5757.482, wps = 1, since beginning = 29 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 5598.235, wps = 1, since beginning = 29 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 6646.433, wps = 1, since beginning = 29 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 60\n",
            "batch no = 0 / 656, train loss = 5971.080, wps = 1, since beginning = 29 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 6280.148, wps = 1, since beginning = 29 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 5930.771, wps = 1, since beginning = 29 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 5640.972, wps = 1, since beginning = 30 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 5628.178, wps = 1, since beginning = 30 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 6007.205, wps = 1, since beginning = 30 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 4908.573, wps = 1, since beginning = 30 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 61\n",
            "batch no = 0 / 656, train loss = 6186.702, wps = 1, since beginning = 30 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 5399.521, wps = 1, since beginning = 30 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 5985.849, wps = 1, since beginning = 30 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 4767.635, wps = 1, since beginning = 30 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 5959.678, wps = 1, since beginning = 30 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 6486.762, wps = 1, since beginning = 30 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 6592.737, wps = 1, since beginning = 30 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 62\n",
            "batch no = 0 / 656, train loss = 5345.758, wps = 1, since beginning = 30 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 5455.038, wps = 1, since beginning = 30 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 5775.505, wps = 1, since beginning = 30 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 5279.645, wps = 1, since beginning = 31 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 5673.757, wps = 1, since beginning = 31 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 5161.686, wps = 1, since beginning = 31 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 6015.542, wps = 1, since beginning = 31 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 63\n",
            "batch no = 0 / 656, train loss = 5231.090, wps = 1, since beginning = 31 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 4982.447, wps = 1, since beginning = 31 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 4736.163, wps = 1, since beginning = 31 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 5660.055, wps = 1, since beginning = 31 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 5819.962, wps = 1, since beginning = 31 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 5240.247, wps = 1, since beginning = 31 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 5644.333, wps = 1, since beginning = 31 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 64\n",
            "batch no = 0 / 656, train loss = 5565.150, wps = 1, since beginning = 31 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 6375.060, wps = 1, since beginning = 31 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 5333.682, wps = 1, since beginning = 31 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 5148.094, wps = 1, since beginning = 32 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 5702.149, wps = 1, since beginning = 32 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 5315.655, wps = 1, since beginning = 32 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 6557.778, wps = 1, since beginning = 32 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 65\n",
            "batch no = 0 / 656, train loss = 5678.183, wps = 1, since beginning = 32 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 5367.934, wps = 1, since beginning = 32 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 5865.929, wps = 1, since beginning = 32 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 5735.001, wps = 1, since beginning = 32 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 6126.234, wps = 1, since beginning = 32 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 5980.527, wps = 1, since beginning = 32 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 5516.266, wps = 1, since beginning = 32 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 66\n",
            "batch no = 0 / 656, train loss = 6055.931, wps = 1, since beginning = 32 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 5575.711, wps = 1, since beginning = 32 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 5579.895, wps = 1, since beginning = 32 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 5778.097, wps = 1, since beginning = 33 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 5928.017, wps = 1, since beginning = 33 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 6145.335, wps = 1, since beginning = 33 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 6103.851, wps = 1, since beginning = 33 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 67\n",
            "batch no = 0 / 656, train loss = 5551.008, wps = 1, since beginning = 33 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 5942.852, wps = 1, since beginning = 33 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 6337.107, wps = 1, since beginning = 33 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 5705.188, wps = 1, since beginning = 33 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 5933.182, wps = 1, since beginning = 33 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 5094.767, wps = 1, since beginning = 33 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 5910.381, wps = 1, since beginning = 33 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 68\n",
            "batch no = 0 / 656, train loss = 5730.689, wps = 1, since beginning = 33 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 5599.154, wps = 1, since beginning = 33 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 5591.290, wps = 1, since beginning = 33 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 5271.617, wps = 1, since beginning = 34 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 5986.280, wps = 1, since beginning = 34 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 5622.249, wps = 1, since beginning = 34 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 5995.271, wps = 1, since beginning = 34 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 69\n",
            "batch no = 0 / 656, train loss = 5377.048, wps = 1, since beginning = 34 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 5213.090, wps = 1, since beginning = 34 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 6152.542, wps = 1, since beginning = 34 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 6194.506, wps = 1, since beginning = 34 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 5555.138, wps = 1, since beginning = 34 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 5373.590, wps = 1, since beginning = 34 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 6374.143, wps = 1, since beginning = 34 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 70\n",
            "batch no = 0 / 656, train loss = 5245.803, wps = 1, since beginning = 34 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 5926.125, wps = 1, since beginning = 34 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 5044.222, wps = 1, since beginning = 34 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 6043.382, wps = 1, since beginning = 35 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 5873.583, wps = 1, since beginning = 35 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 6235.848, wps = 1, since beginning = 35 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 5352.734, wps = 1, since beginning = 35 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 71\n",
            "batch no = 0 / 656, train loss = 5101.838, wps = 1, since beginning = 35 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 5693.370, wps = 1, since beginning = 35 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 5675.103, wps = 1, since beginning = 35 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 6221.386, wps = 1, since beginning = 35 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 5391.047, wps = 1, since beginning = 35 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 5771.732, wps = 1, since beginning = 35 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 5244.809, wps = 1, since beginning = 35 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 72\n",
            "batch no = 0 / 656, train loss = 5397.118, wps = 1, since beginning = 35 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 4874.488, wps = 1, since beginning = 35 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 4501.883, wps = 1, since beginning = 35 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 5596.542, wps = 1, since beginning = 36 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 5408.056, wps = 1, since beginning = 36 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 5274.856, wps = 1, since beginning = 36 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 5150.301, wps = 1, since beginning = 36 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 73\n",
            "batch no = 0 / 656, train loss = 5231.321, wps = 1, since beginning = 36 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 5923.443, wps = 1, since beginning = 36 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 5883.169, wps = 1, since beginning = 36 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 5632.672, wps = 1, since beginning = 36 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 5238.341, wps = 1, since beginning = 36 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 5877.566, wps = 1, since beginning = 36 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 5271.762, wps = 1, since beginning = 36 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 74\n",
            "batch no = 0 / 656, train loss = 5694.390, wps = 1, since beginning = 36 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 5868.313, wps = 1, since beginning = 36 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 5146.919, wps = 1, since beginning = 36 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 5120.929, wps = 1, since beginning = 37 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 5647.492, wps = 1, since beginning = 37 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 6235.122, wps = 1, since beginning = 37 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 5376.155, wps = 1, since beginning = 37 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 75\n",
            "batch no = 0 / 656, train loss = 6153.418, wps = 1, since beginning = 37 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 5573.801, wps = 1, since beginning = 37 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 5259.478, wps = 1, since beginning = 37 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 6171.466, wps = 1, since beginning = 37 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 5580.898, wps = 1, since beginning = 37 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 5337.747, wps = 1, since beginning = 37 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 6017.676, wps = 1, since beginning = 37 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 76\n",
            "batch no = 0 / 656, train loss = 5466.861, wps = 1, since beginning = 37 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 5660.500, wps = 1, since beginning = 37 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 5691.275, wps = 1, since beginning = 37 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 5532.508, wps = 1, since beginning = 38 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 5765.003, wps = 1, since beginning = 38 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 5671.799, wps = 1, since beginning = 38 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 5745.246, wps = 1, since beginning = 38 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 77\n",
            "batch no = 0 / 656, train loss = 5276.907, wps = 1, since beginning = 38 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 6465.261, wps = 1, since beginning = 38 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 5518.710, wps = 1, since beginning = 38 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 5515.910, wps = 1, since beginning = 38 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 5327.397, wps = 1, since beginning = 38 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 5572.820, wps = 1, since beginning = 38 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 6055.858, wps = 1, since beginning = 38 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 78\n",
            "batch no = 0 / 656, train loss = 5663.778, wps = 1, since beginning = 38 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 5906.000, wps = 1, since beginning = 38 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 5889.030, wps = 1, since beginning = 38 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 6079.785, wps = 1, since beginning = 39 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 4848.844, wps = 1, since beginning = 39 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 5795.913, wps = 1, since beginning = 39 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 4763.716, wps = 1, since beginning = 39 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 79\n",
            "batch no = 0 / 656, train loss = 5674.763, wps = 1, since beginning = 39 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 5271.355, wps = 1, since beginning = 39 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 5157.165, wps = 1, since beginning = 39 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 6551.415, wps = 1, since beginning = 39 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 5977.631, wps = 1, since beginning = 39 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 6119.128, wps = 1, since beginning = 39 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 5153.941, wps = 1, since beginning = 39 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 80\n",
            "batch no = 0 / 656, train loss = 5633.377, wps = 1, since beginning = 39 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 5800.536, wps = 1, since beginning = 39 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 5214.952, wps = 1, since beginning = 39 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 6525.418, wps = 1, since beginning = 40 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 5870.854, wps = 1, since beginning = 40 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 5242.578, wps = 1, since beginning = 40 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 5478.662, wps = 1, since beginning = 40 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 81\n",
            "batch no = 0 / 656, train loss = 5588.907, wps = 1, since beginning = 40 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 6174.420, wps = 1, since beginning = 40 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 4841.816, wps = 0, since beginning = 40 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 5531.979, wps = 1, since beginning = 40 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 5650.458, wps = 1, since beginning = 40 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 5311.745, wps = 1, since beginning = 40 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 5704.920, wps = 1, since beginning = 40 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 82\n",
            "batch no = 0 / 656, train loss = 6224.122, wps = 1, since beginning = 40 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 4769.234, wps = 1, since beginning = 40 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 5496.786, wps = 1, since beginning = 40 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 5569.794, wps = 1, since beginning = 41 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 5340.931, wps = 1, since beginning = 41 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 5976.516, wps = 1, since beginning = 41 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 5904.793, wps = 1, since beginning = 41 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 83\n",
            "batch no = 0 / 656, train loss = 5168.688, wps = 1, since beginning = 41 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 5861.472, wps = 1, since beginning = 41 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 4324.486, wps = 0, since beginning = 41 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 5413.393, wps = 1, since beginning = 41 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 5995.907, wps = 1, since beginning = 41 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 5221.232, wps = 1, since beginning = 41 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 6060.071, wps = 1, since beginning = 41 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 84\n",
            "batch no = 0 / 656, train loss = 5598.689, wps = 1, since beginning = 41 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 5521.826, wps = 1, since beginning = 41 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 5016.330, wps = 0, since beginning = 41 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 5607.416, wps = 1, since beginning = 41 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 5360.282, wps = 1, since beginning = 42 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 5755.266, wps = 1, since beginning = 42 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 5774.610, wps = 1, since beginning = 42 mins, cuda memory = 0.874 GBs\n",
            "Run : 1, Epoch : 85\n",
            "batch no = 0 / 656, train loss = 5247.730, wps = 1, since beginning = 42 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 6001.524, wps = 1, since beginning = 42 mins, cuda memory = 0.874 GBs\n",
            "batch no = 200 / 656, train loss = 5684.979, wps = 1, since beginning = 42 mins, cuda memory = 0.874 GBs\n",
            "batch no = 300 / 656, train loss = 5721.022, wps = 1, since beginning = 42 mins, cuda memory = 0.874 GBs\n",
            "batch no = 400 / 656, train loss = 5310.586, wps = 1, since beginning = 42 mins, cuda memory = 0.874 GBs\n",
            "batch no = 500 / 656, train loss = 5545.279, wps = 1, since beginning = 42 mins, cuda memory = 0.874 GBs\n",
            "batch no = 600 / 656, train loss = 5221.594, wps = 1, since beginning = 42 mins, cuda memory = 0.874 GBs\n",
            "Early stopping triggered with counter 2 and patience 2\n",
            "Run : 2, Epoch : 1\n",
            "batch no = 0 / 656, train loss = 13872.032, wps = 29573, since beginning = 0 mins, cuda memory = 0.874 GBs\n",
            "batch no = 100 / 656, train loss = 9745.684, wps = 311, since beginning = 0 mins, cuda memory = 0.902 GBs\n",
            "batch no = 200 / 656, train loss = 9701.206, wps = 157, since beginning = 0 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 8981.104, wps = 100, since beginning = 0 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 9219.227, wps = 80, since beginning = 0 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 8681.053, wps = 60, since beginning = 0 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 8332.168, wps = 48, since beginning = 0 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 2\n",
            "batch no = 0 / 656, train loss = 7952.193, wps = 43, since beginning = 0 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 7926.557, wps = 38, since beginning = 1 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 8860.767, wps = 38, since beginning = 1 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 8685.491, wps = 32, since beginning = 1 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 7529.665, wps = 26, since beginning = 1 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 8020.320, wps = 25, since beginning = 1 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 8023.188, wps = 23, since beginning = 1 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 3\n",
            "batch no = 0 / 656, train loss = 8095.844, wps = 22, since beginning = 1 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 7758.761, wps = 20, since beginning = 1 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 8274.372, wps = 20, since beginning = 1 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 8493.451, wps = 20, since beginning = 1 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 7988.392, wps = 18, since beginning = 1 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 8140.012, wps = 16, since beginning = 1 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 8829.426, wps = 17, since beginning = 1 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 4\n",
            "batch no = 0 / 656, train loss = 8364.955, wps = 16, since beginning = 1 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 7558.508, wps = 14, since beginning = 2 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 7040.060, wps = 13, since beginning = 2 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 7393.090, wps = 13, since beginning = 2 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 7881.749, wps = 13, since beginning = 2 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 7529.560, wps = 12, since beginning = 2 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 7984.120, wps = 12, since beginning = 2 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 5\n",
            "batch no = 0 / 656, train loss = 8220.152, wps = 13, since beginning = 2 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 7674.825, wps = 11, since beginning = 2 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 6685.924, wps = 9, since beginning = 2 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 8027.754, wps = 10, since beginning = 2 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 8193.648, wps = 11, since beginning = 2 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 8365.630, wps = 11, since beginning = 2 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 7305.038, wps = 9, since beginning = 2 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 6\n",
            "batch no = 0 / 656, train loss = 7117.979, wps = 9, since beginning = 2 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 7394.351, wps = 8, since beginning = 3 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 7619.062, wps = 9, since beginning = 3 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 7522.134, wps = 8, since beginning = 3 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 7080.381, wps = 8, since beginning = 3 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 7392.009, wps = 8, since beginning = 3 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 6598.305, wps = 7, since beginning = 3 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 7\n",
            "batch no = 0 / 656, train loss = 7140.760, wps = 7, since beginning = 3 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 6114.664, wps = 6, since beginning = 3 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 6943.815, wps = 7, since beginning = 3 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 7420.283, wps = 7, since beginning = 3 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 7428.949, wps = 7, since beginning = 3 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 8209.491, wps = 7, since beginning = 3 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 8086.027, wps = 7, since beginning = 3 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 8\n",
            "batch no = 0 / 656, train loss = 7164.421, wps = 7, since beginning = 3 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 7336.297, wps = 6, since beginning = 4 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 6550.828, wps = 6, since beginning = 4 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 7660.103, wps = 6, since beginning = 4 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 7826.214, wps = 7, since beginning = 4 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 7762.640, wps = 6, since beginning = 4 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 7429.330, wps = 6, since beginning = 4 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 9\n",
            "batch no = 0 / 656, train loss = 7213.922, wps = 6, since beginning = 4 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 7560.958, wps = 6, since beginning = 4 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 7815.644, wps = 6, since beginning = 4 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 7278.157, wps = 5, since beginning = 4 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 6885.264, wps = 5, since beginning = 4 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 7327.828, wps = 5, since beginning = 4 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 6948.478, wps = 5, since beginning = 4 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 10\n",
            "batch no = 0 / 656, train loss = 6898.489, wps = 5, since beginning = 4 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 7166.183, wps = 5, since beginning = 5 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 7414.968, wps = 5, since beginning = 5 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 7550.021, wps = 5, since beginning = 5 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 6655.351, wps = 5, since beginning = 5 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 7372.519, wps = 5, since beginning = 5 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 6902.498, wps = 4, since beginning = 5 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 11\n",
            "batch no = 0 / 656, train loss = 6982.020, wps = 4, since beginning = 5 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 6627.971, wps = 4, since beginning = 5 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 6812.589, wps = 4, since beginning = 5 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 7394.660, wps = 5, since beginning = 5 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 6832.687, wps = 4, since beginning = 5 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 7074.636, wps = 4, since beginning = 5 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 7674.479, wps = 5, since beginning = 5 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 12\n",
            "batch no = 0 / 656, train loss = 6958.517, wps = 4, since beginning = 5 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 7053.681, wps = 4, since beginning = 6 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 6450.269, wps = 4, since beginning = 6 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 6807.217, wps = 4, since beginning = 6 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 6798.662, wps = 4, since beginning = 6 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 7771.328, wps = 4, since beginning = 6 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 6816.621, wps = 4, since beginning = 6 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 13\n",
            "batch no = 0 / 656, train loss = 6765.083, wps = 4, since beginning = 6 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 6219.912, wps = 3, since beginning = 6 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 6753.616, wps = 4, since beginning = 6 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 7109.679, wps = 4, since beginning = 6 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 6622.937, wps = 3, since beginning = 6 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 6768.642, wps = 3, since beginning = 6 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 7162.626, wps = 4, since beginning = 6 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 14\n",
            "batch no = 0 / 656, train loss = 6367.899, wps = 3, since beginning = 6 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 6320.545, wps = 3, since beginning = 7 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 6445.906, wps = 3, since beginning = 7 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 6919.100, wps = 3, since beginning = 7 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 6297.269, wps = 3, since beginning = 7 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 6936.584, wps = 3, since beginning = 7 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 7225.776, wps = 3, since beginning = 7 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 15\n",
            "batch no = 0 / 656, train loss = 6674.827, wps = 3, since beginning = 7 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 6721.430, wps = 3, since beginning = 7 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 6777.668, wps = 3, since beginning = 7 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 7599.010, wps = 3, since beginning = 7 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 7211.589, wps = 3, since beginning = 7 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 6088.357, wps = 3, since beginning = 7 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 6837.813, wps = 3, since beginning = 7 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 16\n",
            "batch no = 0 / 656, train loss = 6814.386, wps = 3, since beginning = 7 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 7229.837, wps = 3, since beginning = 8 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 6381.104, wps = 3, since beginning = 8 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 6202.223, wps = 3, since beginning = 8 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 6332.207, wps = 3, since beginning = 8 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 6988.050, wps = 3, since beginning = 8 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 6232.581, wps = 2, since beginning = 8 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 17\n",
            "batch no = 0 / 656, train loss = 6113.490, wps = 3, since beginning = 8 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 7365.505, wps = 3, since beginning = 8 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 6355.064, wps = 3, since beginning = 8 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 6507.021, wps = 3, since beginning = 8 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 7298.941, wps = 3, since beginning = 8 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 6319.739, wps = 3, since beginning = 8 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 7531.841, wps = 3, since beginning = 8 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 18\n",
            "batch no = 0 / 656, train loss = 6567.004, wps = 3, since beginning = 8 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 7266.034, wps = 3, since beginning = 9 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 7158.856, wps = 3, since beginning = 9 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 7062.277, wps = 3, since beginning = 9 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 6440.695, wps = 3, since beginning = 9 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 6225.942, wps = 2, since beginning = 9 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 7417.757, wps = 3, since beginning = 9 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 19\n",
            "batch no = 0 / 656, train loss = 6882.803, wps = 3, since beginning = 9 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 6517.407, wps = 3, since beginning = 9 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 6012.502, wps = 2, since beginning = 9 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 6341.122, wps = 2, since beginning = 9 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 7625.318, wps = 3, since beginning = 9 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 6291.961, wps = 2, since beginning = 9 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 6604.183, wps = 3, since beginning = 9 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 20\n",
            "batch no = 0 / 656, train loss = 6108.836, wps = 2, since beginning = 9 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 6320.098, wps = 2, since beginning = 9 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 6099.754, wps = 2, since beginning = 10 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 6596.404, wps = 2, since beginning = 10 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 5960.143, wps = 2, since beginning = 10 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 6623.743, wps = 2, since beginning = 10 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 7295.938, wps = 3, since beginning = 10 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 21\n",
            "batch no = 0 / 656, train loss = 6254.035, wps = 2, since beginning = 10 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 6909.107, wps = 2, since beginning = 10 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 6671.255, wps = 2, since beginning = 10 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 6362.586, wps = 2, since beginning = 10 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 6595.144, wps = 2, since beginning = 10 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 6397.158, wps = 2, since beginning = 10 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 5995.610, wps = 2, since beginning = 10 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 22\n",
            "batch no = 0 / 656, train loss = 6389.646, wps = 2, since beginning = 10 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 6597.007, wps = 2, since beginning = 10 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 6400.798, wps = 2, since beginning = 11 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 6453.194, wps = 2, since beginning = 11 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 6473.208, wps = 2, since beginning = 11 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 6866.661, wps = 2, since beginning = 11 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 6935.219, wps = 2, since beginning = 11 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 23\n",
            "batch no = 0 / 656, train loss = 6233.141, wps = 2, since beginning = 11 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 5975.656, wps = 2, since beginning = 11 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 5757.035, wps = 2, since beginning = 11 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 5280.501, wps = 2, since beginning = 11 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 7164.570, wps = 2, since beginning = 11 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 6188.486, wps = 2, since beginning = 11 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 6162.182, wps = 2, since beginning = 11 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 24\n",
            "batch no = 0 / 656, train loss = 6713.125, wps = 2, since beginning = 11 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 6451.433, wps = 2, since beginning = 11 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 6029.714, wps = 2, since beginning = 12 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 5802.626, wps = 2, since beginning = 12 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 7072.662, wps = 2, since beginning = 12 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 6197.627, wps = 2, since beginning = 12 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 6617.242, wps = 2, since beginning = 12 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 25\n",
            "batch no = 0 / 656, train loss = 5969.844, wps = 2, since beginning = 12 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 6056.946, wps = 2, since beginning = 12 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 6610.263, wps = 2, since beginning = 12 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 5993.304, wps = 2, since beginning = 12 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 7106.877, wps = 2, since beginning = 12 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 7560.029, wps = 2, since beginning = 12 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 5692.634, wps = 2, since beginning = 12 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 26\n",
            "batch no = 0 / 656, train loss = 6966.649, wps = 2, since beginning = 12 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 6379.728, wps = 2, since beginning = 12 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 5983.138, wps = 2, since beginning = 13 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 6753.307, wps = 2, since beginning = 13 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 5856.194, wps = 2, since beginning = 13 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 6677.742, wps = 2, since beginning = 13 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 5937.388, wps = 2, since beginning = 13 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 27\n",
            "batch no = 0 / 656, train loss = 6200.685, wps = 2, since beginning = 13 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 5909.067, wps = 2, since beginning = 13 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 5904.450, wps = 2, since beginning = 13 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 5795.326, wps = 2, since beginning = 13 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 5820.917, wps = 2, since beginning = 13 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 6338.653, wps = 2, since beginning = 13 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 7583.398, wps = 2, since beginning = 13 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 28\n",
            "batch no = 0 / 656, train loss = 6528.769, wps = 2, since beginning = 13 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 6251.497, wps = 2, since beginning = 13 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 6162.876, wps = 2, since beginning = 14 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 6473.646, wps = 2, since beginning = 14 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 6086.668, wps = 2, since beginning = 14 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 6551.146, wps = 2, since beginning = 14 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 6607.781, wps = 2, since beginning = 14 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 29\n",
            "batch no = 0 / 656, train loss = 5916.358, wps = 2, since beginning = 14 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 6561.396, wps = 2, since beginning = 14 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 6296.438, wps = 2, since beginning = 14 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 5481.406, wps = 1, since beginning = 14 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 6174.078, wps = 2, since beginning = 14 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 6771.913, wps = 2, since beginning = 14 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 6535.150, wps = 2, since beginning = 14 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 30\n",
            "batch no = 0 / 656, train loss = 6030.660, wps = 2, since beginning = 14 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 7668.470, wps = 2, since beginning = 14 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 6932.178, wps = 2, since beginning = 15 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 6128.678, wps = 1, since beginning = 15 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 6359.927, wps = 2, since beginning = 15 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 5950.177, wps = 1, since beginning = 15 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 5916.281, wps = 1, since beginning = 15 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 31\n",
            "batch no = 0 / 656, train loss = 5655.202, wps = 1, since beginning = 15 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 5544.187, wps = 1, since beginning = 15 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 6603.733, wps = 2, since beginning = 15 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 7013.846, wps = 2, since beginning = 15 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 6754.066, wps = 2, since beginning = 15 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 7033.559, wps = 2, since beginning = 15 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 6221.747, wps = 1, since beginning = 15 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 32\n",
            "batch no = 0 / 656, train loss = 6418.263, wps = 1, since beginning = 15 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 6146.358, wps = 1, since beginning = 15 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 5697.039, wps = 1, since beginning = 16 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 6858.621, wps = 1, since beginning = 16 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 6337.576, wps = 1, since beginning = 16 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 5999.362, wps = 1, since beginning = 16 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 5949.564, wps = 1, since beginning = 16 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 33\n",
            "batch no = 0 / 656, train loss = 7002.384, wps = 2, since beginning = 16 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 5387.957, wps = 1, since beginning = 16 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 5844.771, wps = 1, since beginning = 16 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 5873.136, wps = 1, since beginning = 16 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 7685.611, wps = 2, since beginning = 16 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 5581.430, wps = 1, since beginning = 16 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 6767.647, wps = 1, since beginning = 16 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 34\n",
            "batch no = 0 / 656, train loss = 5550.425, wps = 1, since beginning = 16 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 6362.284, wps = 1, since beginning = 16 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 5725.334, wps = 1, since beginning = 17 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 6296.617, wps = 1, since beginning = 17 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 5903.562, wps = 1, since beginning = 17 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 6229.784, wps = 1, since beginning = 17 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 5675.206, wps = 1, since beginning = 17 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 35\n",
            "batch no = 0 / 656, train loss = 5754.811, wps = 1, since beginning = 17 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 7010.063, wps = 2, since beginning = 17 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 6160.441, wps = 1, since beginning = 17 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 7115.072, wps = 2, since beginning = 17 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 5984.022, wps = 1, since beginning = 17 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 7080.891, wps = 1, since beginning = 17 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 5956.504, wps = 1, since beginning = 17 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 36\n",
            "batch no = 0 / 656, train loss = 5434.817, wps = 1, since beginning = 17 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 5963.715, wps = 1, since beginning = 17 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 6389.819, wps = 1, since beginning = 18 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 6728.663, wps = 1, since beginning = 18 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 5550.427, wps = 1, since beginning = 18 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 6543.540, wps = 1, since beginning = 18 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 5920.609, wps = 1, since beginning = 18 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 37\n",
            "batch no = 0 / 656, train loss = 6765.980, wps = 1, since beginning = 18 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 6531.544, wps = 1, since beginning = 18 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 5686.715, wps = 1, since beginning = 18 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 5628.818, wps = 1, since beginning = 18 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 5524.922, wps = 1, since beginning = 18 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 5531.501, wps = 1, since beginning = 18 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 6857.209, wps = 1, since beginning = 18 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 38\n",
            "batch no = 0 / 656, train loss = 6395.654, wps = 1, since beginning = 18 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 5800.465, wps = 1, since beginning = 18 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 6648.770, wps = 1, since beginning = 19 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 5702.566, wps = 1, since beginning = 19 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 5328.956, wps = 1, since beginning = 19 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 5930.182, wps = 1, since beginning = 19 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 5678.486, wps = 1, since beginning = 19 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 39\n",
            "batch no = 0 / 656, train loss = 6254.595, wps = 1, since beginning = 19 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 6283.973, wps = 1, since beginning = 19 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 6218.830, wps = 1, since beginning = 19 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 5456.376, wps = 1, since beginning = 19 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 6508.054, wps = 1, since beginning = 19 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 5896.995, wps = 1, since beginning = 19 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 6321.925, wps = 1, since beginning = 19 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 40\n",
            "batch no = 0 / 656, train loss = 5562.149, wps = 1, since beginning = 19 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 5277.355, wps = 1, since beginning = 19 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 5959.309, wps = 1, since beginning = 20 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 5969.517, wps = 1, since beginning = 20 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 6259.446, wps = 1, since beginning = 20 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 5547.253, wps = 1, since beginning = 20 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 6274.047, wps = 1, since beginning = 20 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 41\n",
            "batch no = 0 / 656, train loss = 5844.334, wps = 1, since beginning = 20 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 5606.838, wps = 1, since beginning = 20 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 6504.773, wps = 1, since beginning = 20 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 5884.829, wps = 1, since beginning = 20 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 5842.515, wps = 1, since beginning = 20 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 6434.585, wps = 1, since beginning = 20 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 6228.525, wps = 1, since beginning = 20 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 42\n",
            "batch no = 0 / 656, train loss = 6040.167, wps = 1, since beginning = 20 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 6094.296, wps = 1, since beginning = 20 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 5746.909, wps = 1, since beginning = 21 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 6283.898, wps = 1, since beginning = 21 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 6379.281, wps = 1, since beginning = 21 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 5135.734, wps = 1, since beginning = 21 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 5598.088, wps = 1, since beginning = 21 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 43\n",
            "batch no = 0 / 656, train loss = 6016.115, wps = 1, since beginning = 21 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 6349.115, wps = 1, since beginning = 21 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 5913.886, wps = 1, since beginning = 21 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 5905.458, wps = 1, since beginning = 21 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 5811.580, wps = 1, since beginning = 21 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 5589.129, wps = 1, since beginning = 21 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 6072.444, wps = 1, since beginning = 21 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 44\n",
            "batch no = 0 / 656, train loss = 5784.880, wps = 1, since beginning = 21 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 6024.157, wps = 1, since beginning = 21 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 6012.733, wps = 1, since beginning = 22 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 6030.756, wps = 1, since beginning = 22 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 5758.422, wps = 1, since beginning = 22 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 6535.661, wps = 1, since beginning = 22 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 5750.925, wps = 1, since beginning = 22 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 45\n",
            "batch no = 0 / 656, train loss = 5476.443, wps = 1, since beginning = 22 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 5727.059, wps = 1, since beginning = 22 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 5420.920, wps = 1, since beginning = 22 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 5876.971, wps = 1, since beginning = 22 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 5871.911, wps = 1, since beginning = 22 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 6602.051, wps = 1, since beginning = 22 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 6185.685, wps = 1, since beginning = 22 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 46\n",
            "batch no = 0 / 656, train loss = 7535.137, wps = 1, since beginning = 22 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 6342.316, wps = 1, since beginning = 22 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 6146.616, wps = 1, since beginning = 23 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 5644.792, wps = 1, since beginning = 23 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 5855.439, wps = 1, since beginning = 23 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 6486.590, wps = 1, since beginning = 23 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 5349.233, wps = 1, since beginning = 23 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 47\n",
            "batch no = 0 / 656, train loss = 5833.518, wps = 1, since beginning = 23 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 6879.131, wps = 1, since beginning = 23 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 5781.354, wps = 1, since beginning = 23 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 6279.818, wps = 1, since beginning = 23 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 5392.105, wps = 1, since beginning = 23 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 5833.527, wps = 1, since beginning = 23 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 6262.223, wps = 1, since beginning = 23 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 48\n",
            "batch no = 0 / 656, train loss = 5533.632, wps = 1, since beginning = 23 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 5920.625, wps = 1, since beginning = 23 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 5525.757, wps = 1, since beginning = 24 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 5819.584, wps = 1, since beginning = 24 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 6384.728, wps = 1, since beginning = 24 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 5816.128, wps = 1, since beginning = 24 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 5353.408, wps = 1, since beginning = 24 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 49\n",
            "batch no = 0 / 656, train loss = 6210.991, wps = 1, since beginning = 24 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 6370.656, wps = 1, since beginning = 24 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 5050.455, wps = 1, since beginning = 24 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 5750.640, wps = 1, since beginning = 24 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 5359.566, wps = 1, since beginning = 24 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 6200.458, wps = 1, since beginning = 24 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 5173.688, wps = 1, since beginning = 24 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 50\n",
            "batch no = 0 / 656, train loss = 6222.044, wps = 1, since beginning = 24 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 5980.314, wps = 1, since beginning = 24 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 6421.420, wps = 1, since beginning = 24 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 5740.110, wps = 1, since beginning = 25 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 6226.938, wps = 1, since beginning = 25 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 6533.863, wps = 1, since beginning = 25 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 5722.358, wps = 1, since beginning = 25 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 51\n",
            "batch no = 0 / 656, train loss = 6085.190, wps = 1, since beginning = 25 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 5031.665, wps = 1, since beginning = 25 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 5999.991, wps = 1, since beginning = 25 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 5859.956, wps = 1, since beginning = 25 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 6083.661, wps = 1, since beginning = 25 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 5718.134, wps = 1, since beginning = 25 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 5089.982, wps = 1, since beginning = 25 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 52\n",
            "batch no = 0 / 656, train loss = 4989.066, wps = 1, since beginning = 25 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 6124.170, wps = 1, since beginning = 25 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 5987.203, wps = 1, since beginning = 25 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 6487.166, wps = 1, since beginning = 26 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 5333.055, wps = 1, since beginning = 26 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 5645.389, wps = 1, since beginning = 26 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 6392.231, wps = 1, since beginning = 26 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 53\n",
            "batch no = 0 / 656, train loss = 6007.690, wps = 1, since beginning = 26 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 5534.711, wps = 1, since beginning = 26 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 4802.415, wps = 1, since beginning = 26 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 5456.399, wps = 1, since beginning = 26 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 5403.977, wps = 1, since beginning = 26 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 5685.306, wps = 1, since beginning = 26 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 5709.598, wps = 1, since beginning = 26 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 54\n",
            "batch no = 0 / 656, train loss = 5612.916, wps = 1, since beginning = 26 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 5310.979, wps = 1, since beginning = 26 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 5758.049, wps = 1, since beginning = 26 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 6042.997, wps = 1, since beginning = 27 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 5332.568, wps = 1, since beginning = 27 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 5757.491, wps = 1, since beginning = 27 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 6229.174, wps = 1, since beginning = 27 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 55\n",
            "batch no = 0 / 656, train loss = 6078.462, wps = 1, since beginning = 27 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 6145.393, wps = 1, since beginning = 27 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 5686.589, wps = 1, since beginning = 27 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 5897.510, wps = 1, since beginning = 27 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 5656.815, wps = 1, since beginning = 27 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 5349.577, wps = 1, since beginning = 27 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 6321.148, wps = 1, since beginning = 27 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 56\n",
            "batch no = 0 / 656, train loss = 5505.202, wps = 1, since beginning = 27 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 5898.864, wps = 1, since beginning = 27 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 6251.451, wps = 1, since beginning = 27 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 5549.628, wps = 1, since beginning = 28 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 5881.811, wps = 1, since beginning = 28 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 6032.445, wps = 1, since beginning = 28 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 6249.796, wps = 1, since beginning = 28 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 57\n",
            "batch no = 0 / 656, train loss = 6023.300, wps = 1, since beginning = 28 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 5403.288, wps = 1, since beginning = 28 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 5978.894, wps = 1, since beginning = 28 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 5502.964, wps = 1, since beginning = 28 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 5175.688, wps = 1, since beginning = 28 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 6205.731, wps = 1, since beginning = 28 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 5950.529, wps = 1, since beginning = 28 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 58\n",
            "batch no = 0 / 656, train loss = 5530.845, wps = 1, since beginning = 28 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 5765.783, wps = 1, since beginning = 28 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 5169.184, wps = 1, since beginning = 28 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 5801.837, wps = 1, since beginning = 29 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 5720.994, wps = 1, since beginning = 29 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 5234.798, wps = 1, since beginning = 29 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 5516.085, wps = 1, since beginning = 29 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 59\n",
            "batch no = 0 / 656, train loss = 5763.409, wps = 1, since beginning = 29 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 5925.972, wps = 1, since beginning = 29 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 6069.467, wps = 1, since beginning = 29 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 5568.712, wps = 1, since beginning = 29 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 6207.859, wps = 1, since beginning = 29 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 5619.177, wps = 1, since beginning = 29 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 5646.261, wps = 1, since beginning = 29 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 60\n",
            "batch no = 0 / 656, train loss = 5395.996, wps = 1, since beginning = 29 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 4903.226, wps = 1, since beginning = 29 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 6276.659, wps = 1, since beginning = 29 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 6001.642, wps = 1, since beginning = 30 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 5795.361, wps = 1, since beginning = 30 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 4816.550, wps = 1, since beginning = 30 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 5569.016, wps = 1, since beginning = 30 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 61\n",
            "batch no = 0 / 656, train loss = 5899.061, wps = 1, since beginning = 30 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 5419.219, wps = 1, since beginning = 30 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 4988.954, wps = 1, since beginning = 30 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 5651.256, wps = 1, since beginning = 30 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 5476.500, wps = 1, since beginning = 30 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 6319.093, wps = 1, since beginning = 30 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 6155.149, wps = 1, since beginning = 30 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 62\n",
            "batch no = 0 / 656, train loss = 5395.257, wps = 1, since beginning = 30 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 5846.717, wps = 1, since beginning = 30 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 5525.754, wps = 1, since beginning = 30 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 5915.500, wps = 1, since beginning = 31 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 6351.751, wps = 1, since beginning = 31 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 6320.046, wps = 1, since beginning = 31 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 6268.640, wps = 1, since beginning = 31 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 63\n",
            "batch no = 0 / 656, train loss = 5587.321, wps = 1, since beginning = 31 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 6256.936, wps = 1, since beginning = 31 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 6059.831, wps = 1, since beginning = 31 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 6246.143, wps = 1, since beginning = 31 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 5555.958, wps = 1, since beginning = 31 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 5830.894, wps = 1, since beginning = 31 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 5670.720, wps = 1, since beginning = 31 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 64\n",
            "batch no = 0 / 656, train loss = 6090.380, wps = 1, since beginning = 31 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 5750.401, wps = 1, since beginning = 31 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 5679.950, wps = 1, since beginning = 31 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 5604.841, wps = 1, since beginning = 32 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 5335.836, wps = 1, since beginning = 32 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 6698.316, wps = 1, since beginning = 32 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 5681.047, wps = 1, since beginning = 32 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 65\n",
            "batch no = 0 / 656, train loss = 5379.037, wps = 1, since beginning = 32 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 5322.971, wps = 1, since beginning = 32 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 5718.078, wps = 1, since beginning = 32 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 5413.236, wps = 1, since beginning = 32 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 5957.911, wps = 1, since beginning = 32 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 6268.853, wps = 1, since beginning = 32 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 5737.098, wps = 1, since beginning = 32 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 66\n",
            "batch no = 0 / 656, train loss = 5207.321, wps = 1, since beginning = 32 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 5012.252, wps = 1, since beginning = 32 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 5992.430, wps = 1, since beginning = 32 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 5442.136, wps = 1, since beginning = 33 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 6475.083, wps = 1, since beginning = 33 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 5607.993, wps = 1, since beginning = 33 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 6178.550, wps = 1, since beginning = 33 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 67\n",
            "batch no = 0 / 656, train loss = 6354.017, wps = 1, since beginning = 33 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 5900.882, wps = 1, since beginning = 33 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 5562.701, wps = 1, since beginning = 33 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 5314.067, wps = 1, since beginning = 33 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 5392.475, wps = 1, since beginning = 33 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 5367.263, wps = 1, since beginning = 33 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 5261.009, wps = 1, since beginning = 33 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 68\n",
            "batch no = 0 / 656, train loss = 5754.816, wps = 1, since beginning = 33 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 5725.997, wps = 1, since beginning = 33 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 5278.079, wps = 1, since beginning = 33 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 5574.892, wps = 1, since beginning = 34 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 4720.537, wps = 1, since beginning = 34 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 5171.238, wps = 1, since beginning = 34 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 5777.676, wps = 1, since beginning = 34 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 69\n",
            "batch no = 0 / 656, train loss = 6269.997, wps = 1, since beginning = 34 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 5622.551, wps = 1, since beginning = 34 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 5416.381, wps = 1, since beginning = 34 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 6130.518, wps = 1, since beginning = 34 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 5862.420, wps = 1, since beginning = 34 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 5494.965, wps = 1, since beginning = 34 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 5698.769, wps = 1, since beginning = 34 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 70\n",
            "batch no = 0 / 656, train loss = 5593.651, wps = 1, since beginning = 34 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 6024.493, wps = 1, since beginning = 34 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 5604.588, wps = 1, since beginning = 34 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 6006.439, wps = 1, since beginning = 35 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 5957.197, wps = 1, since beginning = 35 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 6068.810, wps = 1, since beginning = 35 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 5332.999, wps = 1, since beginning = 35 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 71\n",
            "batch no = 0 / 656, train loss = 6088.435, wps = 1, since beginning = 35 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 5718.711, wps = 1, since beginning = 35 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 5389.213, wps = 1, since beginning = 35 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 5079.172, wps = 1, since beginning = 35 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 6023.825, wps = 1, since beginning = 35 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 6303.062, wps = 1, since beginning = 35 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 5365.441, wps = 1, since beginning = 35 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 72\n",
            "batch no = 0 / 656, train loss = 5454.045, wps = 1, since beginning = 35 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 5222.647, wps = 1, since beginning = 35 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 5998.030, wps = 1, since beginning = 35 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 5728.052, wps = 1, since beginning = 36 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 5955.542, wps = 1, since beginning = 36 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 5827.616, wps = 1, since beginning = 36 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 5756.011, wps = 1, since beginning = 36 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 73\n",
            "batch no = 0 / 656, train loss = 5516.951, wps = 1, since beginning = 36 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 5607.237, wps = 1, since beginning = 36 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 5606.622, wps = 1, since beginning = 36 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 5814.812, wps = 1, since beginning = 36 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 4777.395, wps = 1, since beginning = 36 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 6194.675, wps = 1, since beginning = 36 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 5373.109, wps = 1, since beginning = 36 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 74\n",
            "batch no = 0 / 656, train loss = 6318.421, wps = 1, since beginning = 36 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 5094.304, wps = 1, since beginning = 36 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 5673.160, wps = 1, since beginning = 36 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 5341.048, wps = 1, since beginning = 36 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 5148.428, wps = 1, since beginning = 37 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 5735.530, wps = 1, since beginning = 37 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 4979.132, wps = 1, since beginning = 37 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 75\n",
            "batch no = 0 / 656, train loss = 5355.139, wps = 1, since beginning = 37 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 6513.854, wps = 1, since beginning = 37 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 5401.758, wps = 1, since beginning = 37 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 5718.671, wps = 1, since beginning = 37 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 5968.025, wps = 1, since beginning = 37 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 5412.704, wps = 1, since beginning = 37 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 5090.196, wps = 1, since beginning = 37 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 76\n",
            "batch no = 0 / 656, train loss = 5579.486, wps = 1, since beginning = 37 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 4866.006, wps = 1, since beginning = 37 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 5331.558, wps = 1, since beginning = 37 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 5086.542, wps = 1, since beginning = 38 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 5230.006, wps = 1, since beginning = 38 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 5564.102, wps = 1, since beginning = 38 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 5803.415, wps = 1, since beginning = 38 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 77\n",
            "batch no = 0 / 656, train loss = 5565.111, wps = 1, since beginning = 38 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 5734.499, wps = 1, since beginning = 38 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 5761.319, wps = 1, since beginning = 38 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 5846.458, wps = 1, since beginning = 38 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 5043.127, wps = 1, since beginning = 38 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 5899.965, wps = 1, since beginning = 38 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 6026.573, wps = 1, since beginning = 38 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 78\n",
            "batch no = 0 / 656, train loss = 6198.177, wps = 1, since beginning = 38 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 5532.440, wps = 1, since beginning = 38 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 5007.609, wps = 1, since beginning = 38 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 6553.044, wps = 1, since beginning = 38 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 5155.600, wps = 1, since beginning = 39 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 6074.277, wps = 1, since beginning = 39 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 5933.844, wps = 1, since beginning = 39 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 79\n",
            "batch no = 0 / 656, train loss = 5669.340, wps = 1, since beginning = 39 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 5149.677, wps = 1, since beginning = 39 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 6094.571, wps = 1, since beginning = 39 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 5388.450, wps = 1, since beginning = 39 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 5723.538, wps = 1, since beginning = 39 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 5800.074, wps = 1, since beginning = 39 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 5968.243, wps = 1, since beginning = 39 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 80\n",
            "batch no = 0 / 656, train loss = 5738.509, wps = 1, since beginning = 39 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 5909.247, wps = 1, since beginning = 39 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 6106.742, wps = 1, since beginning = 39 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 6077.453, wps = 1, since beginning = 39 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 5156.194, wps = 1, since beginning = 40 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 5251.562, wps = 1, since beginning = 40 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 5673.323, wps = 1, since beginning = 40 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 81\n",
            "batch no = 0 / 656, train loss = 5834.881, wps = 1, since beginning = 40 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 5616.874, wps = 1, since beginning = 40 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 5261.601, wps = 1, since beginning = 40 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 5044.438, wps = 1, since beginning = 40 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 5317.327, wps = 1, since beginning = 40 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 5610.987, wps = 1, since beginning = 40 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 6615.030, wps = 1, since beginning = 40 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 82\n",
            "batch no = 0 / 656, train loss = 4978.842, wps = 1, since beginning = 40 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 5003.450, wps = 1, since beginning = 40 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 5759.304, wps = 1, since beginning = 40 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 5102.014, wps = 1, since beginning = 40 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 5657.414, wps = 1, since beginning = 41 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 5592.614, wps = 1, since beginning = 41 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 5203.436, wps = 1, since beginning = 41 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 83\n",
            "batch no = 0 / 656, train loss = 6181.459, wps = 1, since beginning = 41 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 5408.575, wps = 1, since beginning = 41 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 5776.016, wps = 1, since beginning = 41 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 5508.450, wps = 1, since beginning = 41 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 5326.248, wps = 1, since beginning = 41 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 5254.048, wps = 1, since beginning = 41 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 5878.027, wps = 1, since beginning = 41 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 84\n",
            "batch no = 0 / 656, train loss = 5163.869, wps = 1, since beginning = 41 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 5035.239, wps = 1, since beginning = 41 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 5231.344, wps = 1, since beginning = 41 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 6098.885, wps = 1, since beginning = 41 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 5191.831, wps = 1, since beginning = 42 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 5987.854, wps = 1, since beginning = 42 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 5315.065, wps = 1, since beginning = 42 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 85\n",
            "batch no = 0 / 656, train loss = 5344.071, wps = 1, since beginning = 42 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 4953.688, wps = 0, since beginning = 42 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 5429.055, wps = 1, since beginning = 42 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 5237.006, wps = 1, since beginning = 42 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 5739.669, wps = 1, since beginning = 42 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 5224.475, wps = 1, since beginning = 42 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 5455.940, wps = 1, since beginning = 42 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 86\n",
            "batch no = 0 / 656, train loss = 5573.917, wps = 1, since beginning = 42 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 5304.603, wps = 1, since beginning = 42 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 5767.235, wps = 1, since beginning = 42 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 5672.034, wps = 1, since beginning = 42 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 5225.569, wps = 1, since beginning = 43 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 5589.683, wps = 1, since beginning = 43 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 5090.021, wps = 0, since beginning = 43 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 87\n",
            "batch no = 0 / 656, train loss = 4756.807, wps = 0, since beginning = 43 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 5043.415, wps = 0, since beginning = 43 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 5501.527, wps = 1, since beginning = 43 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 5995.692, wps = 1, since beginning = 43 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 5974.827, wps = 1, since beginning = 43 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 5543.379, wps = 1, since beginning = 43 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 5867.474, wps = 1, since beginning = 43 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 88\n",
            "batch no = 0 / 656, train loss = 6383.500, wps = 1, since beginning = 43 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 5559.095, wps = 1, since beginning = 43 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 5156.418, wps = 0, since beginning = 43 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 5257.456, wps = 1, since beginning = 43 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 4966.588, wps = 0, since beginning = 44 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 5091.793, wps = 0, since beginning = 44 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 5481.576, wps = 1, since beginning = 44 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 89\n",
            "batch no = 0 / 656, train loss = 5534.031, wps = 1, since beginning = 44 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 5255.420, wps = 1, since beginning = 44 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 5723.374, wps = 1, since beginning = 44 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 5657.717, wps = 1, since beginning = 44 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 5931.889, wps = 1, since beginning = 44 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 4357.284, wps = 0, since beginning = 44 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 4552.628, wps = 0, since beginning = 44 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 90\n",
            "batch no = 0 / 656, train loss = 6209.175, wps = 1, since beginning = 44 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 5144.334, wps = 0, since beginning = 44 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 5463.808, wps = 1, since beginning = 44 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 5798.940, wps = 1, since beginning = 44 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 5022.790, wps = 0, since beginning = 45 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 4518.021, wps = 0, since beginning = 45 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 5972.271, wps = 1, since beginning = 45 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 91\n",
            "batch no = 0 / 656, train loss = 5464.621, wps = 0, since beginning = 45 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 5453.651, wps = 0, since beginning = 45 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 5930.398, wps = 1, since beginning = 45 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 5314.893, wps = 0, since beginning = 45 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 5493.620, wps = 1, since beginning = 45 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 4677.749, wps = 0, since beginning = 45 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 5617.580, wps = 1, since beginning = 45 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 92\n",
            "batch no = 0 / 656, train loss = 5464.229, wps = 1, since beginning = 45 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 5565.992, wps = 0, since beginning = 45 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 5183.504, wps = 1, since beginning = 45 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 5177.369, wps = 0, since beginning = 45 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 5943.798, wps = 1, since beginning = 46 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 5894.494, wps = 1, since beginning = 46 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 5208.376, wps = 0, since beginning = 46 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 93\n",
            "batch no = 0 / 656, train loss = 5774.338, wps = 0, since beginning = 46 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 5460.118, wps = 1, since beginning = 46 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 6071.132, wps = 1, since beginning = 46 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 5411.550, wps = 1, since beginning = 46 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 5453.623, wps = 1, since beginning = 46 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 6332.287, wps = 1, since beginning = 46 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 5845.605, wps = 1, since beginning = 46 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 94\n",
            "batch no = 0 / 656, train loss = 5638.129, wps = 0, since beginning = 46 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 4902.873, wps = 0, since beginning = 46 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 5355.219, wps = 0, since beginning = 46 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 5324.896, wps = 0, since beginning = 46 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 5630.911, wps = 1, since beginning = 47 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 5220.487, wps = 0, since beginning = 47 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 5307.645, wps = 0, since beginning = 47 mins, cuda memory = 0.912 GBs\n",
            "Run : 2, Epoch : 95\n",
            "batch no = 0 / 656, train loss = 5673.703, wps = 0, since beginning = 47 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 5400.214, wps = 0, since beginning = 47 mins, cuda memory = 0.912 GBs\n",
            "batch no = 200 / 656, train loss = 5225.464, wps = 0, since beginning = 47 mins, cuda memory = 0.912 GBs\n",
            "batch no = 300 / 656, train loss = 5552.503, wps = 1, since beginning = 47 mins, cuda memory = 0.912 GBs\n",
            "batch no = 400 / 656, train loss = 5890.993, wps = 1, since beginning = 47 mins, cuda memory = 0.912 GBs\n",
            "batch no = 500 / 656, train loss = 5502.270, wps = 0, since beginning = 47 mins, cuda memory = 0.912 GBs\n",
            "batch no = 600 / 656, train loss = 5192.797, wps = 0, since beginning = 47 mins, cuda memory = 0.912 GBs\n",
            "Early stopping triggered with counter 2 and patience 2\n",
            "Run : 3, Epoch : 1\n",
            "batch no = 0 / 656, train loss = 11044.216, wps = 31190, since beginning = 0 mins, cuda memory = 0.912 GBs\n",
            "batch no = 100 / 656, train loss = 8966.283, wps = 289, since beginning = 0 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 9489.169, wps = 154, since beginning = 0 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 8916.116, wps = 102, since beginning = 0 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 8480.548, wps = 74, since beginning = 0 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 9254.126, wps = 64, since beginning = 0 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 8438.746, wps = 49, since beginning = 0 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 2\n",
            "batch no = 0 / 656, train loss = 8335.479, wps = 44, since beginning = 0 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 7494.234, wps = 37, since beginning = 1 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 8894.728, wps = 36, since beginning = 1 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 8565.042, wps = 33, since beginning = 1 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 7935.971, wps = 27, since beginning = 1 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 8468.764, wps = 27, since beginning = 1 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 8299.479, wps = 24, since beginning = 1 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 3\n",
            "batch no = 0 / 656, train loss = 7731.242, wps = 22, since beginning = 1 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 8013.697, wps = 21, since beginning = 1 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 7398.474, wps = 18, since beginning = 1 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 7905.601, wps = 19, since beginning = 1 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 8468.443, wps = 18, since beginning = 1 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6961.514, wps = 15, since beginning = 1 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 7830.676, wps = 16, since beginning = 1 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 4\n",
            "batch no = 0 / 656, train loss = 7778.118, wps = 15, since beginning = 1 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 7700.941, wps = 15, since beginning = 2 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 8277.692, wps = 15, since beginning = 2 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 7932.427, wps = 14, since beginning = 2 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 9885.570, wps = 16, since beginning = 2 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 7386.638, wps = 12, since beginning = 2 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 7749.076, wps = 12, since beginning = 2 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 5\n",
            "batch no = 0 / 656, train loss = 6779.759, wps = 10, since beginning = 2 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 8113.202, wps = 12, since beginning = 2 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6757.879, wps = 9, since beginning = 2 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 7457.863, wps = 10, since beginning = 2 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 7731.369, wps = 10, since beginning = 2 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 7409.147, wps = 10, since beginning = 2 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 8061.483, wps = 10, since beginning = 2 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 6\n",
            "batch no = 0 / 656, train loss = 6855.218, wps = 9, since beginning = 2 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6856.710, wps = 8, since beginning = 3 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 7448.381, wps = 9, since beginning = 3 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 7027.965, wps = 8, since beginning = 3 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 8467.760, wps = 9, since beginning = 3 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 7790.359, wps = 8, since beginning = 3 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6327.777, wps = 7, since beginning = 3 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 7\n",
            "batch no = 0 / 656, train loss = 7051.679, wps = 7, since beginning = 3 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 7350.190, wps = 7, since beginning = 3 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 7599.863, wps = 8, since beginning = 3 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 8122.064, wps = 8, since beginning = 3 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 7907.810, wps = 7, since beginning = 3 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 8469.646, wps = 8, since beginning = 3 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 7246.735, wps = 7, since beginning = 3 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 8\n",
            "batch no = 0 / 656, train loss = 8087.685, wps = 7, since beginning = 3 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 8297.278, wps = 7, since beginning = 4 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6420.314, wps = 5, since beginning = 4 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 7097.781, wps = 6, since beginning = 4 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 7025.843, wps = 6, since beginning = 4 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6690.597, wps = 6, since beginning = 4 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 7564.859, wps = 6, since beginning = 4 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 9\n",
            "batch no = 0 / 656, train loss = 7281.801, wps = 6, since beginning = 4 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 7458.177, wps = 6, since beginning = 4 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6575.073, wps = 5, since beginning = 4 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 7020.984, wps = 6, since beginning = 4 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 7295.150, wps = 5, since beginning = 4 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 7231.714, wps = 5, since beginning = 4 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6917.969, wps = 5, since beginning = 4 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 10\n",
            "batch no = 0 / 656, train loss = 7487.820, wps = 5, since beginning = 4 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6807.366, wps = 5, since beginning = 5 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 7341.993, wps = 5, since beginning = 5 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6947.347, wps = 5, since beginning = 5 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 7302.338, wps = 5, since beginning = 5 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6627.753, wps = 5, since beginning = 5 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 7742.019, wps = 5, since beginning = 5 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 11\n",
            "batch no = 0 / 656, train loss = 6818.053, wps = 4, since beginning = 5 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 7283.783, wps = 5, since beginning = 5 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 7017.124, wps = 4, since beginning = 5 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6740.768, wps = 4, since beginning = 5 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6639.268, wps = 4, since beginning = 5 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 7502.183, wps = 5, since beginning = 5 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6764.674, wps = 4, since beginning = 5 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 12\n",
            "batch no = 0 / 656, train loss = 6870.568, wps = 4, since beginning = 5 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6688.895, wps = 4, since beginning = 6 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6693.784, wps = 4, since beginning = 6 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6688.938, wps = 4, since beginning = 6 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6444.105, wps = 4, since beginning = 6 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 7318.146, wps = 4, since beginning = 6 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6120.623, wps = 3, since beginning = 6 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 13\n",
            "batch no = 0 / 656, train loss = 7066.832, wps = 4, since beginning = 6 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 7348.084, wps = 4, since beginning = 6 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6922.271, wps = 4, since beginning = 6 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 7144.912, wps = 4, since beginning = 6 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6952.131, wps = 4, since beginning = 6 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 7175.598, wps = 4, since beginning = 6 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6950.403, wps = 3, since beginning = 6 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 14\n",
            "batch no = 0 / 656, train loss = 6700.377, wps = 3, since beginning = 6 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6301.048, wps = 3, since beginning = 7 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6800.678, wps = 3, since beginning = 7 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6801.633, wps = 3, since beginning = 7 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6086.859, wps = 3, since beginning = 7 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6492.686, wps = 3, since beginning = 7 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6955.565, wps = 3, since beginning = 7 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 15\n",
            "batch no = 0 / 656, train loss = 7317.926, wps = 3, since beginning = 7 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6574.193, wps = 3, since beginning = 7 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 7167.452, wps = 3, since beginning = 7 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 7032.948, wps = 3, since beginning = 7 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 7026.009, wps = 3, since beginning = 7 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6538.616, wps = 3, since beginning = 7 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6913.339, wps = 3, since beginning = 7 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 16\n",
            "batch no = 0 / 656, train loss = 6818.162, wps = 3, since beginning = 7 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6883.002, wps = 3, since beginning = 8 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6410.508, wps = 3, since beginning = 8 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6862.818, wps = 3, since beginning = 8 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6911.917, wps = 3, since beginning = 8 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6083.699, wps = 3, since beginning = 8 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 7012.143, wps = 3, since beginning = 8 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 17\n",
            "batch no = 0 / 656, train loss = 6463.446, wps = 3, since beginning = 8 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 8236.522, wps = 3, since beginning = 8 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6348.654, wps = 3, since beginning = 8 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6940.465, wps = 3, since beginning = 8 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 7076.671, wps = 3, since beginning = 8 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6957.008, wps = 3, since beginning = 8 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5784.900, wps = 2, since beginning = 8 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 18\n",
            "batch no = 0 / 656, train loss = 6408.957, wps = 3, since beginning = 8 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6665.591, wps = 3, since beginning = 9 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6086.819, wps = 2, since beginning = 9 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6105.922, wps = 3, since beginning = 9 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6439.983, wps = 3, since beginning = 9 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6671.701, wps = 2, since beginning = 9 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6929.069, wps = 3, since beginning = 9 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 19\n",
            "batch no = 0 / 656, train loss = 6837.110, wps = 3, since beginning = 9 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6169.022, wps = 2, since beginning = 9 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 7142.514, wps = 3, since beginning = 9 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6319.100, wps = 2, since beginning = 9 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6106.885, wps = 2, since beginning = 9 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6991.117, wps = 3, since beginning = 9 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 7127.459, wps = 3, since beginning = 9 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 20\n",
            "batch no = 0 / 656, train loss = 7122.493, wps = 2, since beginning = 9 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 7216.801, wps = 3, since beginning = 10 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6322.682, wps = 2, since beginning = 10 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6600.316, wps = 2, since beginning = 10 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6765.328, wps = 2, since beginning = 10 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 7677.641, wps = 3, since beginning = 10 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6451.939, wps = 2, since beginning = 10 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 21\n",
            "batch no = 0 / 656, train loss = 6078.904, wps = 2, since beginning = 10 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6438.950, wps = 2, since beginning = 10 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6535.361, wps = 2, since beginning = 10 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6236.110, wps = 2, since beginning = 10 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6555.724, wps = 2, since beginning = 10 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6487.903, wps = 2, since beginning = 10 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6177.181, wps = 2, since beginning = 10 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 22\n",
            "batch no = 0 / 656, train loss = 6249.859, wps = 2, since beginning = 10 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 7240.703, wps = 2, since beginning = 10 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6976.222, wps = 2, since beginning = 11 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6664.093, wps = 2, since beginning = 11 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6222.068, wps = 2, since beginning = 11 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6025.813, wps = 2, since beginning = 11 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6647.536, wps = 2, since beginning = 11 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 23\n",
            "batch no = 0 / 656, train loss = 7077.154, wps = 2, since beginning = 11 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6460.206, wps = 2, since beginning = 11 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6545.798, wps = 2, since beginning = 11 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 7229.358, wps = 2, since beginning = 11 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6576.317, wps = 2, since beginning = 11 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 7404.186, wps = 2, since beginning = 11 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6508.352, wps = 2, since beginning = 11 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 24\n",
            "batch no = 0 / 656, train loss = 6506.800, wps = 2, since beginning = 11 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6611.562, wps = 2, since beginning = 11 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6857.672, wps = 2, since beginning = 12 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6373.099, wps = 2, since beginning = 12 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5943.573, wps = 2, since beginning = 12 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6243.463, wps = 2, since beginning = 12 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6383.191, wps = 2, since beginning = 12 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 25\n",
            "batch no = 0 / 656, train loss = 6148.908, wps = 2, since beginning = 12 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 7166.113, wps = 2, since beginning = 12 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6206.704, wps = 2, since beginning = 12 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6329.989, wps = 2, since beginning = 12 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6090.147, wps = 2, since beginning = 12 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6101.428, wps = 2, since beginning = 12 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6243.782, wps = 2, since beginning = 12 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 26\n",
            "batch no = 0 / 656, train loss = 6384.306, wps = 2, since beginning = 12 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6112.710, wps = 2, since beginning = 12 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6719.879, wps = 2, since beginning = 13 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5982.390, wps = 2, since beginning = 13 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6834.319, wps = 2, since beginning = 13 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6420.818, wps = 2, since beginning = 13 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6147.408, wps = 2, since beginning = 13 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 27\n",
            "batch no = 0 / 656, train loss = 6448.155, wps = 2, since beginning = 13 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 7107.906, wps = 2, since beginning = 13 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6530.971, wps = 2, since beginning = 13 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6789.637, wps = 2, since beginning = 13 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6500.476, wps = 2, since beginning = 13 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6348.195, wps = 2, since beginning = 13 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6778.850, wps = 2, since beginning = 13 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 28\n",
            "batch no = 0 / 656, train loss = 7064.358, wps = 2, since beginning = 13 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6134.119, wps = 2, since beginning = 13 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6818.224, wps = 2, since beginning = 14 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6596.438, wps = 2, since beginning = 14 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6100.435, wps = 2, since beginning = 14 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6447.477, wps = 2, since beginning = 14 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6605.861, wps = 2, since beginning = 14 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 29\n",
            "batch no = 0 / 656, train loss = 5987.662, wps = 2, since beginning = 14 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5717.926, wps = 2, since beginning = 14 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5762.290, wps = 1, since beginning = 14 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6604.042, wps = 2, since beginning = 14 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6554.388, wps = 2, since beginning = 14 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 7427.919, wps = 2, since beginning = 14 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6847.725, wps = 2, since beginning = 14 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 30\n",
            "batch no = 0 / 656, train loss = 6500.514, wps = 2, since beginning = 14 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6175.706, wps = 2, since beginning = 14 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6066.604, wps = 2, since beginning = 15 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6147.540, wps = 2, since beginning = 15 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5943.199, wps = 1, since beginning = 15 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6477.844, wps = 2, since beginning = 15 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5932.987, wps = 1, since beginning = 15 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 31\n",
            "batch no = 0 / 656, train loss = 5777.854, wps = 1, since beginning = 15 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6498.810, wps = 2, since beginning = 15 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6567.839, wps = 2, since beginning = 15 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5580.677, wps = 1, since beginning = 15 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6671.417, wps = 2, since beginning = 15 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6622.374, wps = 2, since beginning = 15 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6014.369, wps = 1, since beginning = 15 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 32\n",
            "batch no = 0 / 656, train loss = 5892.524, wps = 1, since beginning = 15 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6031.296, wps = 1, since beginning = 15 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6526.289, wps = 2, since beginning = 16 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6654.614, wps = 1, since beginning = 16 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5744.470, wps = 1, since beginning = 16 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 7082.762, wps = 2, since beginning = 16 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6008.843, wps = 1, since beginning = 16 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 33\n",
            "batch no = 0 / 656, train loss = 6482.779, wps = 2, since beginning = 16 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5069.408, wps = 1, since beginning = 16 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6886.145, wps = 2, since beginning = 16 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5875.944, wps = 1, since beginning = 16 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6166.413, wps = 1, since beginning = 16 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5961.581, wps = 1, since beginning = 16 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5547.277, wps = 1, since beginning = 16 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 34\n",
            "batch no = 0 / 656, train loss = 6233.114, wps = 1, since beginning = 16 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6420.166, wps = 1, since beginning = 16 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5854.585, wps = 1, since beginning = 17 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5840.950, wps = 1, since beginning = 17 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5581.020, wps = 1, since beginning = 17 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6226.911, wps = 1, since beginning = 17 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6437.148, wps = 1, since beginning = 17 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 35\n",
            "batch no = 0 / 656, train loss = 6418.381, wps = 1, since beginning = 17 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6492.465, wps = 1, since beginning = 17 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6265.545, wps = 1, since beginning = 17 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5885.467, wps = 1, since beginning = 17 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6159.902, wps = 1, since beginning = 17 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6195.348, wps = 1, since beginning = 17 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5597.380, wps = 1, since beginning = 17 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 36\n",
            "batch no = 0 / 656, train loss = 6732.361, wps = 1, since beginning = 17 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5292.117, wps = 1, since beginning = 17 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5485.687, wps = 1, since beginning = 18 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6007.271, wps = 1, since beginning = 18 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6270.283, wps = 1, since beginning = 18 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6196.540, wps = 1, since beginning = 18 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6133.624, wps = 1, since beginning = 18 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 37\n",
            "batch no = 0 / 656, train loss = 5808.896, wps = 1, since beginning = 18 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6155.260, wps = 1, since beginning = 18 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5489.015, wps = 1, since beginning = 18 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6490.753, wps = 1, since beginning = 18 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6052.169, wps = 1, since beginning = 18 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5829.173, wps = 1, since beginning = 18 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6158.343, wps = 1, since beginning = 18 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 38\n",
            "batch no = 0 / 656, train loss = 6181.662, wps = 1, since beginning = 18 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6324.363, wps = 1, since beginning = 18 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6044.734, wps = 1, since beginning = 19 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5955.788, wps = 1, since beginning = 19 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6346.629, wps = 1, since beginning = 19 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6609.307, wps = 1, since beginning = 19 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5528.842, wps = 1, since beginning = 19 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 39\n",
            "batch no = 0 / 656, train loss = 6498.715, wps = 1, since beginning = 19 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5554.780, wps = 1, since beginning = 19 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5697.304, wps = 1, since beginning = 19 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5853.566, wps = 1, since beginning = 19 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6208.188, wps = 1, since beginning = 19 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 4911.124, wps = 1, since beginning = 19 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6592.195, wps = 1, since beginning = 19 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 40\n",
            "batch no = 0 / 656, train loss = 6171.948, wps = 1, since beginning = 19 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6102.169, wps = 1, since beginning = 19 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6631.848, wps = 1, since beginning = 20 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6721.829, wps = 1, since beginning = 20 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5712.986, wps = 1, since beginning = 20 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5906.089, wps = 1, since beginning = 20 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5974.176, wps = 1, since beginning = 20 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 41\n",
            "batch no = 0 / 656, train loss = 6317.500, wps = 1, since beginning = 20 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6499.649, wps = 1, since beginning = 20 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6028.380, wps = 1, since beginning = 20 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5664.446, wps = 1, since beginning = 20 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6645.000, wps = 1, since beginning = 20 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6182.422, wps = 1, since beginning = 20 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6264.078, wps = 1, since beginning = 20 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 42\n",
            "batch no = 0 / 656, train loss = 6390.248, wps = 1, since beginning = 20 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6183.132, wps = 1, since beginning = 20 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6038.808, wps = 1, since beginning = 21 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5447.995, wps = 1, since beginning = 21 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5969.225, wps = 1, since beginning = 21 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5878.807, wps = 1, since beginning = 21 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5893.755, wps = 1, since beginning = 21 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 43\n",
            "batch no = 0 / 656, train loss = 6093.593, wps = 1, since beginning = 21 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 7048.381, wps = 1, since beginning = 21 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6332.588, wps = 1, since beginning = 21 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6580.286, wps = 1, since beginning = 21 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6233.344, wps = 1, since beginning = 21 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5827.388, wps = 1, since beginning = 21 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5803.720, wps = 1, since beginning = 21 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 44\n",
            "batch no = 0 / 656, train loss = 5781.833, wps = 1, since beginning = 21 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5909.581, wps = 1, since beginning = 21 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5771.900, wps = 1, since beginning = 22 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5534.700, wps = 1, since beginning = 22 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5716.911, wps = 1, since beginning = 22 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6222.285, wps = 1, since beginning = 22 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6746.323, wps = 1, since beginning = 22 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 45\n",
            "batch no = 0 / 656, train loss = 5770.592, wps = 1, since beginning = 22 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5353.086, wps = 1, since beginning = 22 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6606.031, wps = 1, since beginning = 22 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5609.794, wps = 1, since beginning = 22 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5880.755, wps = 1, since beginning = 22 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5862.973, wps = 1, since beginning = 22 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5901.424, wps = 1, since beginning = 22 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 46\n",
            "batch no = 0 / 656, train loss = 6840.584, wps = 1, since beginning = 22 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6072.625, wps = 1, since beginning = 22 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6066.233, wps = 1, since beginning = 23 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5768.317, wps = 1, since beginning = 23 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6009.341, wps = 1, since beginning = 23 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6135.670, wps = 1, since beginning = 23 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5707.469, wps = 1, since beginning = 23 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 47\n",
            "batch no = 0 / 656, train loss = 6806.617, wps = 1, since beginning = 23 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 7075.435, wps = 1, since beginning = 23 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5755.007, wps = 1, since beginning = 23 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5600.645, wps = 1, since beginning = 23 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6373.507, wps = 1, since beginning = 23 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5199.659, wps = 1, since beginning = 23 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5792.590, wps = 1, since beginning = 23 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 48\n",
            "batch no = 0 / 656, train loss = 6686.785, wps = 1, since beginning = 23 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5986.979, wps = 1, since beginning = 23 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6369.250, wps = 1, since beginning = 23 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6190.681, wps = 1, since beginning = 24 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5668.013, wps = 1, since beginning = 24 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5777.732, wps = 1, since beginning = 24 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5847.112, wps = 1, since beginning = 24 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 49\n",
            "batch no = 0 / 656, train loss = 5506.008, wps = 1, since beginning = 24 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5901.897, wps = 1, since beginning = 24 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6061.963, wps = 1, since beginning = 24 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6146.685, wps = 1, since beginning = 24 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6250.579, wps = 1, since beginning = 24 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5910.092, wps = 1, since beginning = 24 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6461.701, wps = 1, since beginning = 24 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 50\n",
            "batch no = 0 / 656, train loss = 5801.746, wps = 1, since beginning = 24 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5331.791, wps = 1, since beginning = 24 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5857.088, wps = 1, since beginning = 24 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5533.172, wps = 1, since beginning = 25 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5941.313, wps = 1, since beginning = 25 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5943.464, wps = 1, since beginning = 25 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6026.984, wps = 1, since beginning = 25 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 51\n",
            "batch no = 0 / 656, train loss = 5285.075, wps = 1, since beginning = 25 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5626.188, wps = 1, since beginning = 25 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6298.944, wps = 1, since beginning = 25 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5733.235, wps = 1, since beginning = 25 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5572.404, wps = 1, since beginning = 25 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5302.410, wps = 1, since beginning = 25 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5869.348, wps = 1, since beginning = 25 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 52\n",
            "batch no = 0 / 656, train loss = 5946.191, wps = 1, since beginning = 25 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5869.725, wps = 1, since beginning = 25 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6422.875, wps = 1, since beginning = 25 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5739.340, wps = 1, since beginning = 26 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5504.886, wps = 1, since beginning = 26 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5829.484, wps = 1, since beginning = 26 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5835.734, wps = 1, since beginning = 26 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 53\n",
            "batch no = 0 / 656, train loss = 5840.953, wps = 1, since beginning = 26 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5988.874, wps = 1, since beginning = 26 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5956.898, wps = 1, since beginning = 26 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6229.958, wps = 1, since beginning = 26 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6507.804, wps = 1, since beginning = 26 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5597.700, wps = 1, since beginning = 26 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5909.008, wps = 1, since beginning = 26 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 54\n",
            "batch no = 0 / 656, train loss = 6364.388, wps = 1, since beginning = 26 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6145.425, wps = 1, since beginning = 26 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5647.371, wps = 1, since beginning = 26 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5241.267, wps = 1, since beginning = 27 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5856.375, wps = 1, since beginning = 27 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6134.414, wps = 1, since beginning = 27 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5653.576, wps = 1, since beginning = 27 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 55\n",
            "batch no = 0 / 656, train loss = 5781.629, wps = 1, since beginning = 27 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5465.875, wps = 1, since beginning = 27 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5345.552, wps = 1, since beginning = 27 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5401.644, wps = 1, since beginning = 27 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6348.068, wps = 1, since beginning = 27 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5943.533, wps = 1, since beginning = 27 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6077.501, wps = 1, since beginning = 27 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 56\n",
            "batch no = 0 / 656, train loss = 6153.078, wps = 1, since beginning = 27 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5729.995, wps = 1, since beginning = 27 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5730.761, wps = 1, since beginning = 27 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5611.798, wps = 1, since beginning = 28 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5674.000, wps = 1, since beginning = 28 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5907.618, wps = 1, since beginning = 28 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6242.914, wps = 1, since beginning = 28 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 57\n",
            "batch no = 0 / 656, train loss = 6275.250, wps = 1, since beginning = 28 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6312.436, wps = 1, since beginning = 28 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5866.751, wps = 1, since beginning = 28 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5990.954, wps = 1, since beginning = 28 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6467.563, wps = 1, since beginning = 28 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6311.850, wps = 1, since beginning = 28 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5531.602, wps = 1, since beginning = 28 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 58\n",
            "batch no = 0 / 656, train loss = 6108.675, wps = 1, since beginning = 28 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5724.019, wps = 1, since beginning = 28 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5932.218, wps = 1, since beginning = 28 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6010.264, wps = 1, since beginning = 29 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6559.277, wps = 1, since beginning = 29 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6001.088, wps = 1, since beginning = 29 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6497.553, wps = 1, since beginning = 29 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 59\n",
            "batch no = 0 / 656, train loss = 6332.623, wps = 1, since beginning = 29 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5958.279, wps = 1, since beginning = 29 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5235.668, wps = 1, since beginning = 29 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5720.254, wps = 1, since beginning = 29 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5196.989, wps = 1, since beginning = 29 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5944.957, wps = 1, since beginning = 29 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5759.026, wps = 1, since beginning = 29 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 60\n",
            "batch no = 0 / 656, train loss = 5562.096, wps = 1, since beginning = 29 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5797.276, wps = 1, since beginning = 29 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5192.863, wps = 1, since beginning = 29 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6205.862, wps = 1, since beginning = 30 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5556.714, wps = 1, since beginning = 30 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5654.107, wps = 1, since beginning = 30 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5568.215, wps = 1, since beginning = 30 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 61\n",
            "batch no = 0 / 656, train loss = 5607.588, wps = 1, since beginning = 30 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5861.344, wps = 1, since beginning = 30 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5333.245, wps = 1, since beginning = 30 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5845.307, wps = 1, since beginning = 30 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5785.338, wps = 1, since beginning = 30 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5305.114, wps = 1, since beginning = 30 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5319.148, wps = 1, since beginning = 30 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 62\n",
            "batch no = 0 / 656, train loss = 5669.982, wps = 1, since beginning = 30 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5238.542, wps = 1, since beginning = 30 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5912.684, wps = 1, since beginning = 30 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5697.078, wps = 1, since beginning = 31 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6056.188, wps = 1, since beginning = 31 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5210.946, wps = 1, since beginning = 31 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5612.679, wps = 1, since beginning = 31 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 63\n",
            "batch no = 0 / 656, train loss = 6040.960, wps = 1, since beginning = 31 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5832.402, wps = 1, since beginning = 31 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6297.952, wps = 1, since beginning = 31 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6228.667, wps = 1, since beginning = 31 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6493.066, wps = 1, since beginning = 31 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5532.421, wps = 1, since beginning = 31 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5694.588, wps = 1, since beginning = 31 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 64\n",
            "batch no = 0 / 656, train loss = 5086.878, wps = 1, since beginning = 31 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5938.025, wps = 1, since beginning = 31 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5282.264, wps = 1, since beginning = 31 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5488.288, wps = 1, since beginning = 32 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5793.407, wps = 1, since beginning = 32 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5870.920, wps = 1, since beginning = 32 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5495.973, wps = 1, since beginning = 32 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 65\n",
            "batch no = 0 / 656, train loss = 5833.781, wps = 1, since beginning = 32 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5691.620, wps = 1, since beginning = 32 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5677.815, wps = 1, since beginning = 32 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5896.461, wps = 1, since beginning = 32 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5982.471, wps = 1, since beginning = 32 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6529.361, wps = 1, since beginning = 32 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6277.923, wps = 1, since beginning = 32 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 66\n",
            "batch no = 0 / 656, train loss = 5159.744, wps = 1, since beginning = 32 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6310.818, wps = 1, since beginning = 32 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 4693.047, wps = 1, since beginning = 32 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6333.196, wps = 1, since beginning = 33 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5721.135, wps = 1, since beginning = 33 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5483.882, wps = 1, since beginning = 33 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5800.146, wps = 1, since beginning = 33 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 67\n",
            "batch no = 0 / 656, train loss = 4790.325, wps = 1, since beginning = 33 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5705.979, wps = 1, since beginning = 33 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5522.307, wps = 1, since beginning = 33 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5952.313, wps = 1, since beginning = 33 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5141.444, wps = 1, since beginning = 33 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5224.414, wps = 1, since beginning = 33 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5588.563, wps = 1, since beginning = 33 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 68\n",
            "batch no = 0 / 656, train loss = 6039.643, wps = 1, since beginning = 33 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5150.928, wps = 1, since beginning = 33 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5328.231, wps = 1, since beginning = 33 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5866.740, wps = 1, since beginning = 34 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5729.852, wps = 1, since beginning = 34 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5269.943, wps = 1, since beginning = 34 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5850.833, wps = 1, since beginning = 34 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 69\n",
            "batch no = 0 / 656, train loss = 5689.410, wps = 1, since beginning = 34 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6079.981, wps = 1, since beginning = 34 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5699.688, wps = 1, since beginning = 34 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5803.258, wps = 1, since beginning = 34 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5927.213, wps = 1, since beginning = 34 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6147.438, wps = 1, since beginning = 34 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5242.221, wps = 1, since beginning = 34 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 70\n",
            "batch no = 0 / 656, train loss = 6179.787, wps = 1, since beginning = 34 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6308.859, wps = 1, since beginning = 34 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5427.286, wps = 1, since beginning = 34 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5375.702, wps = 1, since beginning = 34 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5614.388, wps = 1, since beginning = 35 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5469.257, wps = 1, since beginning = 35 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5425.662, wps = 1, since beginning = 35 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 71\n",
            "batch no = 0 / 656, train loss = 5359.539, wps = 1, since beginning = 35 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5048.273, wps = 1, since beginning = 35 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5762.285, wps = 1, since beginning = 35 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5293.993, wps = 1, since beginning = 35 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5438.881, wps = 1, since beginning = 35 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5304.513, wps = 1, since beginning = 35 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5683.920, wps = 1, since beginning = 35 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 72\n",
            "batch no = 0 / 656, train loss = 5737.185, wps = 1, since beginning = 35 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6946.521, wps = 1, since beginning = 35 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5579.451, wps = 1, since beginning = 35 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5704.131, wps = 1, since beginning = 35 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5594.607, wps = 1, since beginning = 36 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5068.485, wps = 1, since beginning = 36 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5722.084, wps = 1, since beginning = 36 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 73\n",
            "batch no = 0 / 656, train loss = 5794.381, wps = 1, since beginning = 36 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5572.130, wps = 1, since beginning = 36 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5598.226, wps = 1, since beginning = 36 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6020.042, wps = 1, since beginning = 36 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5938.117, wps = 1, since beginning = 36 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5301.255, wps = 1, since beginning = 36 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5829.289, wps = 1, since beginning = 36 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 74\n",
            "batch no = 0 / 656, train loss = 6114.856, wps = 1, since beginning = 36 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5147.965, wps = 1, since beginning = 36 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5727.133, wps = 1, since beginning = 36 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5707.503, wps = 1, since beginning = 36 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5257.964, wps = 1, since beginning = 37 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5722.191, wps = 1, since beginning = 37 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5638.995, wps = 1, since beginning = 37 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 75\n",
            "batch no = 0 / 656, train loss = 5355.622, wps = 1, since beginning = 37 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5285.795, wps = 1, since beginning = 37 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5465.975, wps = 1, since beginning = 37 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5022.311, wps = 1, since beginning = 37 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5151.490, wps = 1, since beginning = 37 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5459.425, wps = 1, since beginning = 37 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6220.167, wps = 1, since beginning = 37 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 76\n",
            "batch no = 0 / 656, train loss = 5336.910, wps = 1, since beginning = 37 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5566.127, wps = 1, since beginning = 37 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5608.137, wps = 1, since beginning = 37 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5801.729, wps = 1, since beginning = 37 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 4977.104, wps = 1, since beginning = 38 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6141.883, wps = 1, since beginning = 38 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5307.748, wps = 1, since beginning = 38 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 77\n",
            "batch no = 0 / 656, train loss = 4967.220, wps = 1, since beginning = 38 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5873.755, wps = 1, since beginning = 38 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5151.300, wps = 1, since beginning = 38 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5848.853, wps = 1, since beginning = 38 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5840.115, wps = 1, since beginning = 38 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5609.129, wps = 1, since beginning = 38 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6199.193, wps = 1, since beginning = 38 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 78\n",
            "batch no = 0 / 656, train loss = 5597.753, wps = 1, since beginning = 38 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5605.065, wps = 1, since beginning = 38 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5424.191, wps = 1, since beginning = 38 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5561.174, wps = 1, since beginning = 38 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5519.013, wps = 1, since beginning = 39 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5266.150, wps = 1, since beginning = 39 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5828.792, wps = 1, since beginning = 39 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 79\n",
            "batch no = 0 / 656, train loss = 5444.760, wps = 1, since beginning = 39 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5086.057, wps = 1, since beginning = 39 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6434.144, wps = 1, since beginning = 39 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5604.978, wps = 1, since beginning = 39 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5367.389, wps = 1, since beginning = 39 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5469.577, wps = 1, since beginning = 39 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5225.816, wps = 1, since beginning = 39 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 80\n",
            "batch no = 0 / 656, train loss = 5835.162, wps = 1, since beginning = 39 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5779.255, wps = 1, since beginning = 39 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5237.757, wps = 1, since beginning = 39 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5887.276, wps = 1, since beginning = 39 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6500.592, wps = 1, since beginning = 40 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6013.506, wps = 1, since beginning = 40 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5801.703, wps = 1, since beginning = 40 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 81\n",
            "batch no = 0 / 656, train loss = 5579.151, wps = 1, since beginning = 40 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5811.886, wps = 1, since beginning = 40 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 4559.597, wps = 0, since beginning = 40 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6172.277, wps = 1, since beginning = 40 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5572.995, wps = 1, since beginning = 40 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5482.216, wps = 1, since beginning = 40 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5168.805, wps = 1, since beginning = 40 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 82\n",
            "batch no = 0 / 656, train loss = 5668.633, wps = 1, since beginning = 40 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5580.712, wps = 1, since beginning = 40 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5316.111, wps = 1, since beginning = 40 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5388.467, wps = 1, since beginning = 40 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5759.661, wps = 1, since beginning = 41 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5802.686, wps = 1, since beginning = 41 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5503.588, wps = 1, since beginning = 41 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 83\n",
            "batch no = 0 / 656, train loss = 5175.086, wps = 1, since beginning = 41 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5192.528, wps = 1, since beginning = 41 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5600.662, wps = 1, since beginning = 41 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5807.138, wps = 1, since beginning = 41 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5418.884, wps = 1, since beginning = 41 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5603.713, wps = 1, since beginning = 41 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5518.974, wps = 1, since beginning = 41 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 84\n",
            "batch no = 0 / 656, train loss = 5520.742, wps = 1, since beginning = 41 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6245.914, wps = 1, since beginning = 41 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5889.140, wps = 1, since beginning = 41 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 4819.494, wps = 1, since beginning = 41 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5722.701, wps = 1, since beginning = 42 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 4889.238, wps = 0, since beginning = 42 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5888.515, wps = 1, since beginning = 42 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 85\n",
            "batch no = 0 / 656, train loss = 5550.062, wps = 1, since beginning = 42 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5682.838, wps = 1, since beginning = 42 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6189.656, wps = 1, since beginning = 42 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5264.078, wps = 1, since beginning = 42 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5305.371, wps = 1, since beginning = 42 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 4927.186, wps = 0, since beginning = 42 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6109.004, wps = 1, since beginning = 42 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 86\n",
            "batch no = 0 / 656, train loss = 5531.986, wps = 1, since beginning = 42 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5691.502, wps = 1, since beginning = 42 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5417.045, wps = 1, since beginning = 42 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5534.810, wps = 1, since beginning = 42 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5859.925, wps = 1, since beginning = 43 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5781.791, wps = 1, since beginning = 43 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6138.453, wps = 1, since beginning = 43 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 87\n",
            "batch no = 0 / 656, train loss = 4742.450, wps = 0, since beginning = 43 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5535.599, wps = 1, since beginning = 43 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5726.682, wps = 1, since beginning = 43 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5577.796, wps = 1, since beginning = 43 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5525.310, wps = 1, since beginning = 43 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5300.933, wps = 1, since beginning = 43 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5489.580, wps = 1, since beginning = 43 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 88\n",
            "batch no = 0 / 656, train loss = 5127.772, wps = 1, since beginning = 43 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5672.769, wps = 1, since beginning = 43 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5420.631, wps = 1, since beginning = 43 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5671.129, wps = 1, since beginning = 43 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5646.037, wps = 1, since beginning = 44 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5319.134, wps = 1, since beginning = 44 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5275.169, wps = 1, since beginning = 44 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 89\n",
            "batch no = 0 / 656, train loss = 5326.733, wps = 1, since beginning = 44 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5305.271, wps = 0, since beginning = 44 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5050.595, wps = 0, since beginning = 44 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5654.917, wps = 1, since beginning = 44 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5712.303, wps = 1, since beginning = 44 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 4948.246, wps = 0, since beginning = 44 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5623.462, wps = 0, since beginning = 44 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 90\n",
            "batch no = 0 / 656, train loss = 5365.297, wps = 0, since beginning = 44 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5723.089, wps = 1, since beginning = 44 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5619.446, wps = 1, since beginning = 44 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5208.188, wps = 0, since beginning = 44 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5916.986, wps = 1, since beginning = 45 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5536.244, wps = 1, since beginning = 45 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5238.457, wps = 0, since beginning = 45 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 91\n",
            "batch no = 0 / 656, train loss = 5442.333, wps = 1, since beginning = 45 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5555.214, wps = 1, since beginning = 45 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5323.130, wps = 1, since beginning = 45 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5886.119, wps = 1, since beginning = 45 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6083.369, wps = 1, since beginning = 45 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5301.011, wps = 0, since beginning = 45 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5806.677, wps = 1, since beginning = 45 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 92\n",
            "batch no = 0 / 656, train loss = 5263.329, wps = 0, since beginning = 45 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6069.429, wps = 1, since beginning = 45 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5958.180, wps = 1, since beginning = 45 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5849.548, wps = 1, since beginning = 45 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 4944.466, wps = 0, since beginning = 46 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5359.809, wps = 0, since beginning = 46 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5475.719, wps = 1, since beginning = 46 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 93\n",
            "batch no = 0 / 656, train loss = 5824.657, wps = 1, since beginning = 46 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5541.366, wps = 1, since beginning = 46 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5621.380, wps = 1, since beginning = 46 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5353.645, wps = 0, since beginning = 46 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5201.327, wps = 0, since beginning = 46 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5070.500, wps = 0, since beginning = 46 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6059.099, wps = 1, since beginning = 46 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 94\n",
            "batch no = 0 / 656, train loss = 4813.565, wps = 0, since beginning = 46 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5667.546, wps = 1, since beginning = 46 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6096.172, wps = 1, since beginning = 46 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5880.982, wps = 0, since beginning = 46 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5257.593, wps = 0, since beginning = 47 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5802.378, wps = 1, since beginning = 47 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 4704.136, wps = 0, since beginning = 47 mins, cuda memory = 0.913 GBs\n",
            "Run : 3, Epoch : 95\n",
            "batch no = 0 / 656, train loss = 5507.366, wps = 0, since beginning = 47 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5194.864, wps = 0, since beginning = 47 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5472.093, wps = 0, since beginning = 47 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 4460.147, wps = 0, since beginning = 47 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5842.812, wps = 0, since beginning = 47 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5472.962, wps = 0, since beginning = 47 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5263.127, wps = 0, since beginning = 47 mins, cuda memory = 0.913 GBs\n",
            "Early stopping triggered with counter 2 and patience 2\n",
            "Run : 4, Epoch : 1\n",
            "batch no = 0 / 656, train loss = 12314.745, wps = 27866, since beginning = 0 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 9297.828, wps = 293, since beginning = 0 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 10405.854, wps = 168, since beginning = 0 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 8712.457, wps = 95, since beginning = 0 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 8553.509, wps = 72, since beginning = 0 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 8842.257, wps = 61, since beginning = 0 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 8800.666, wps = 52, since beginning = 0 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 2\n",
            "batch no = 0 / 656, train loss = 8767.400, wps = 46, since beginning = 0 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 9083.980, wps = 42, since beginning = 1 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 9537.213, wps = 39, since beginning = 1 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 7799.400, wps = 29, since beginning = 1 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 9174.942, wps = 31, since beginning = 1 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 7743.963, wps = 25, since beginning = 1 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 8017.311, wps = 23, since beginning = 1 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 3\n",
            "batch no = 0 / 656, train loss = 8684.499, wps = 24, since beginning = 1 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 7128.159, wps = 19, since beginning = 1 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 8104.527, wps = 20, since beginning = 1 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 7590.988, wps = 18, since beginning = 1 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 8087.858, wps = 18, since beginning = 1 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 8776.396, wps = 18, since beginning = 1 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 7540.842, wps = 16, since beginning = 1 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 4\n",
            "batch no = 0 / 656, train loss = 7919.308, wps = 15, since beginning = 1 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 7595.144, wps = 14, since beginning = 2 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 7641.543, wps = 14, since beginning = 2 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 8040.052, wps = 14, since beginning = 2 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 7617.377, wps = 12, since beginning = 2 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6688.896, wps = 11, since beginning = 2 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 7805.743, wps = 12, since beginning = 2 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 5\n",
            "batch no = 0 / 656, train loss = 7679.508, wps = 11, since beginning = 2 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6972.523, wps = 10, since beginning = 2 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 7276.234, wps = 10, since beginning = 2 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 8239.730, wps = 11, since beginning = 2 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 7048.529, wps = 9, since beginning = 2 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 7477.410, wps = 10, since beginning = 2 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 7495.537, wps = 9, since beginning = 2 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 6\n",
            "batch no = 0 / 656, train loss = 7542.659, wps = 9, since beginning = 2 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 7790.013, wps = 9, since beginning = 3 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 7759.372, wps = 9, since beginning = 3 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 8073.186, wps = 9, since beginning = 3 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 7394.784, wps = 8, since beginning = 3 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 7774.998, wps = 8, since beginning = 3 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 7086.305, wps = 8, since beginning = 3 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 7\n",
            "batch no = 0 / 656, train loss = 7082.922, wps = 7, since beginning = 3 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 7652.885, wps = 8, since beginning = 3 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 7913.884, wps = 8, since beginning = 3 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6922.731, wps = 7, since beginning = 3 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 7226.735, wps = 6, since beginning = 3 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 8270.078, wps = 7, since beginning = 3 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 8076.983, wps = 7, since beginning = 3 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 8\n",
            "batch no = 0 / 656, train loss = 6228.858, wps = 6, since beginning = 3 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6988.734, wps = 6, since beginning = 4 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 7704.636, wps = 7, since beginning = 4 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 8166.777, wps = 7, since beginning = 4 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6921.731, wps = 6, since beginning = 4 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6241.798, wps = 5, since beginning = 4 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 7384.624, wps = 6, since beginning = 4 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 9\n",
            "batch no = 0 / 656, train loss = 8129.220, wps = 6, since beginning = 4 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 7292.663, wps = 5, since beginning = 4 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6462.768, wps = 5, since beginning = 4 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 7189.194, wps = 5, since beginning = 4 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 7472.237, wps = 5, since beginning = 4 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 7787.712, wps = 6, since beginning = 4 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 7644.741, wps = 5, since beginning = 4 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 10\n",
            "batch no = 0 / 656, train loss = 6534.894, wps = 4, since beginning = 4 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6527.425, wps = 5, since beginning = 5 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6725.969, wps = 5, since beginning = 5 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6741.562, wps = 5, since beginning = 5 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 7218.083, wps = 5, since beginning = 5 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 7061.886, wps = 5, since beginning = 5 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6691.824, wps = 4, since beginning = 5 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 11\n",
            "batch no = 0 / 656, train loss = 7554.335, wps = 5, since beginning = 5 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6575.424, wps = 4, since beginning = 5 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 7937.312, wps = 5, since beginning = 5 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 7123.039, wps = 4, since beginning = 5 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 7135.892, wps = 4, since beginning = 5 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 7118.641, wps = 4, since beginning = 5 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 7109.243, wps = 4, since beginning = 5 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 12\n",
            "batch no = 0 / 656, train loss = 7267.718, wps = 4, since beginning = 5 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6167.885, wps = 4, since beginning = 6 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 7121.783, wps = 4, since beginning = 6 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6804.697, wps = 4, since beginning = 6 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6980.275, wps = 4, since beginning = 6 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6692.986, wps = 4, since beginning = 6 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 7355.823, wps = 4, since beginning = 6 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 13\n",
            "batch no = 0 / 656, train loss = 6631.464, wps = 4, since beginning = 6 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 7290.960, wps = 4, since beginning = 6 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6682.577, wps = 4, since beginning = 6 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 7637.406, wps = 4, since beginning = 6 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6333.757, wps = 3, since beginning = 6 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6799.541, wps = 4, since beginning = 6 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6720.853, wps = 3, since beginning = 6 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 14\n",
            "batch no = 0 / 656, train loss = 6856.212, wps = 4, since beginning = 6 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 7327.037, wps = 4, since beginning = 7 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6998.350, wps = 3, since beginning = 7 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 7641.458, wps = 4, since beginning = 7 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6662.627, wps = 3, since beginning = 7 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6605.685, wps = 3, since beginning = 7 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6458.193, wps = 3, since beginning = 7 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 15\n",
            "batch no = 0 / 656, train loss = 6321.724, wps = 3, since beginning = 7 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6224.312, wps = 3, since beginning = 7 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 7039.513, wps = 3, since beginning = 7 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6759.198, wps = 3, since beginning = 7 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 7420.549, wps = 3, since beginning = 7 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6562.821, wps = 3, since beginning = 7 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6915.111, wps = 3, since beginning = 7 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 16\n",
            "batch no = 0 / 656, train loss = 6349.173, wps = 3, since beginning = 7 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6782.395, wps = 3, since beginning = 8 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 7258.607, wps = 3, since beginning = 8 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 7176.113, wps = 3, since beginning = 8 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6262.261, wps = 3, since beginning = 8 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6199.550, wps = 3, since beginning = 8 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6662.896, wps = 3, since beginning = 8 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 17\n",
            "batch no = 0 / 656, train loss = 7134.397, wps = 3, since beginning = 8 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6672.657, wps = 3, since beginning = 8 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6700.469, wps = 3, since beginning = 8 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 7250.169, wps = 3, since beginning = 8 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 7188.408, wps = 3, since beginning = 8 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 7464.499, wps = 3, since beginning = 8 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6073.991, wps = 2, since beginning = 8 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 18\n",
            "batch no = 0 / 656, train loss = 6308.138, wps = 2, since beginning = 8 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 7449.270, wps = 3, since beginning = 9 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6414.799, wps = 3, since beginning = 9 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6728.973, wps = 3, since beginning = 9 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6676.083, wps = 3, since beginning = 9 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6370.308, wps = 2, since beginning = 9 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6790.896, wps = 3, since beginning = 9 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 19\n",
            "batch no = 0 / 656, train loss = 6619.593, wps = 3, since beginning = 9 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6341.670, wps = 2, since beginning = 9 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6626.346, wps = 2, since beginning = 9 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6928.824, wps = 3, since beginning = 9 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6359.594, wps = 2, since beginning = 9 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6362.812, wps = 2, since beginning = 9 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6963.351, wps = 2, since beginning = 9 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 20\n",
            "batch no = 0 / 656, train loss = 6314.933, wps = 2, since beginning = 9 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6673.554, wps = 2, since beginning = 10 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6310.516, wps = 2, since beginning = 10 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 7035.551, wps = 3, since beginning = 10 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6202.219, wps = 2, since beginning = 10 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6404.919, wps = 2, since beginning = 10 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6643.638, wps = 2, since beginning = 10 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 21\n",
            "batch no = 0 / 656, train loss = 6486.117, wps = 2, since beginning = 10 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6341.832, wps = 2, since beginning = 10 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6199.204, wps = 2, since beginning = 10 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6770.848, wps = 2, since beginning = 10 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6231.237, wps = 2, since beginning = 10 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6979.076, wps = 2, since beginning = 10 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5935.999, wps = 2, since beginning = 10 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 22\n",
            "batch no = 0 / 656, train loss = 6552.378, wps = 2, since beginning = 10 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6516.793, wps = 2, since beginning = 11 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6599.326, wps = 2, since beginning = 11 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6630.066, wps = 2, since beginning = 11 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6482.854, wps = 2, since beginning = 11 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 7197.423, wps = 2, since beginning = 11 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6489.687, wps = 2, since beginning = 11 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 23\n",
            "batch no = 0 / 656, train loss = 6777.188, wps = 2, since beginning = 11 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6156.739, wps = 2, since beginning = 11 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6587.108, wps = 2, since beginning = 11 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6698.008, wps = 2, since beginning = 11 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6310.727, wps = 2, since beginning = 11 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6498.557, wps = 2, since beginning = 11 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6361.563, wps = 2, since beginning = 11 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 24\n",
            "batch no = 0 / 656, train loss = 6168.913, wps = 2, since beginning = 11 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6733.476, wps = 2, since beginning = 11 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6279.957, wps = 2, since beginning = 12 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6219.607, wps = 2, since beginning = 12 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5610.582, wps = 2, since beginning = 12 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6176.748, wps = 2, since beginning = 12 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6803.633, wps = 2, since beginning = 12 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 25\n",
            "batch no = 0 / 656, train loss = 6670.556, wps = 2, since beginning = 12 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6031.472, wps = 2, since beginning = 12 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6040.382, wps = 2, since beginning = 12 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6225.930, wps = 2, since beginning = 12 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6257.302, wps = 2, since beginning = 12 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6583.659, wps = 2, since beginning = 12 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6953.471, wps = 2, since beginning = 12 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 26\n",
            "batch no = 0 / 656, train loss = 6310.309, wps = 2, since beginning = 12 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6054.896, wps = 2, since beginning = 13 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6837.164, wps = 2, since beginning = 13 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 7322.430, wps = 2, since beginning = 13 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6787.337, wps = 2, since beginning = 13 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5811.169, wps = 2, since beginning = 13 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6943.716, wps = 2, since beginning = 13 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 27\n",
            "batch no = 0 / 656, train loss = 6427.600, wps = 2, since beginning = 13 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5747.417, wps = 2, since beginning = 13 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5970.525, wps = 2, since beginning = 13 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6270.929, wps = 2, since beginning = 13 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6098.915, wps = 2, since beginning = 13 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5984.940, wps = 2, since beginning = 13 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6671.246, wps = 2, since beginning = 13 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 28\n",
            "batch no = 0 / 656, train loss = 7109.637, wps = 2, since beginning = 13 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 7225.149, wps = 2, since beginning = 13 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5617.807, wps = 2, since beginning = 14 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6174.640, wps = 2, since beginning = 14 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6024.493, wps = 2, since beginning = 14 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6639.399, wps = 2, since beginning = 14 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6617.441, wps = 2, since beginning = 14 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 29\n",
            "batch no = 0 / 656, train loss = 6460.562, wps = 2, since beginning = 14 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6377.577, wps = 2, since beginning = 14 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6293.635, wps = 2, since beginning = 14 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5936.542, wps = 2, since beginning = 14 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6663.512, wps = 2, since beginning = 14 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6161.469, wps = 2, since beginning = 14 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 7638.715, wps = 2, since beginning = 14 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 30\n",
            "batch no = 0 / 656, train loss = 6769.011, wps = 2, since beginning = 14 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6302.552, wps = 2, since beginning = 14 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6401.259, wps = 2, since beginning = 15 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6042.686, wps = 1, since beginning = 15 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6396.112, wps = 2, since beginning = 15 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6400.193, wps = 2, since beginning = 15 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6255.871, wps = 2, since beginning = 15 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 31\n",
            "batch no = 0 / 656, train loss = 5861.632, wps = 2, since beginning = 15 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5162.897, wps = 1, since beginning = 15 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6017.080, wps = 1, since beginning = 15 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 7023.338, wps = 2, since beginning = 15 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5895.398, wps = 1, since beginning = 15 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6559.921, wps = 1, since beginning = 15 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5456.819, wps = 1, since beginning = 15 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 32\n",
            "batch no = 0 / 656, train loss = 5848.557, wps = 1, since beginning = 15 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6022.549, wps = 1, since beginning = 15 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5834.700, wps = 1, since beginning = 16 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6004.942, wps = 1, since beginning = 16 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6901.640, wps = 2, since beginning = 16 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6108.396, wps = 1, since beginning = 16 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5874.519, wps = 1, since beginning = 16 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 33\n",
            "batch no = 0 / 656, train loss = 6833.003, wps = 2, since beginning = 16 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5991.219, wps = 1, since beginning = 16 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6293.576, wps = 1, since beginning = 16 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 7068.903, wps = 2, since beginning = 16 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6047.719, wps = 1, since beginning = 16 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6668.971, wps = 1, since beginning = 16 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5986.987, wps = 1, since beginning = 16 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 34\n",
            "batch no = 0 / 656, train loss = 6047.837, wps = 1, since beginning = 16 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5904.253, wps = 1, since beginning = 16 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6625.252, wps = 1, since beginning = 17 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5875.354, wps = 1, since beginning = 17 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5577.915, wps = 1, since beginning = 17 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6611.656, wps = 1, since beginning = 17 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5921.064, wps = 1, since beginning = 17 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 35\n",
            "batch no = 0 / 656, train loss = 6854.877, wps = 1, since beginning = 17 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5369.224, wps = 1, since beginning = 17 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 7073.040, wps = 1, since beginning = 17 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5496.470, wps = 1, since beginning = 17 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6097.602, wps = 1, since beginning = 17 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5843.756, wps = 1, since beginning = 17 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5908.737, wps = 1, since beginning = 17 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 36\n",
            "batch no = 0 / 656, train loss = 5735.375, wps = 1, since beginning = 17 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5874.225, wps = 1, since beginning = 17 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5932.442, wps = 1, since beginning = 18 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5529.823, wps = 1, since beginning = 18 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6011.886, wps = 1, since beginning = 18 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6060.017, wps = 1, since beginning = 18 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5916.512, wps = 1, since beginning = 18 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 37\n",
            "batch no = 0 / 656, train loss = 5782.281, wps = 1, since beginning = 18 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6682.904, wps = 1, since beginning = 18 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5563.116, wps = 1, since beginning = 18 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5661.988, wps = 1, since beginning = 18 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5967.917, wps = 1, since beginning = 18 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5468.742, wps = 1, since beginning = 18 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5542.700, wps = 1, since beginning = 18 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 38\n",
            "batch no = 0 / 656, train loss = 6315.795, wps = 1, since beginning = 18 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5959.786, wps = 1, since beginning = 18 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6126.820, wps = 1, since beginning = 19 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6040.867, wps = 1, since beginning = 19 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6090.947, wps = 1, since beginning = 19 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6763.532, wps = 1, since beginning = 19 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5943.409, wps = 1, since beginning = 19 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 39\n",
            "batch no = 0 / 656, train loss = 5908.394, wps = 1, since beginning = 19 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6045.431, wps = 1, since beginning = 19 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5916.567, wps = 1, since beginning = 19 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5379.922, wps = 1, since beginning = 19 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5926.024, wps = 1, since beginning = 19 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6033.822, wps = 1, since beginning = 19 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6358.448, wps = 1, since beginning = 19 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 40\n",
            "batch no = 0 / 656, train loss = 5577.890, wps = 1, since beginning = 19 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5949.717, wps = 1, since beginning = 19 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5679.414, wps = 1, since beginning = 20 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5780.919, wps = 1, since beginning = 20 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6092.445, wps = 1, since beginning = 20 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6130.697, wps = 1, since beginning = 20 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6962.670, wps = 1, since beginning = 20 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 41\n",
            "batch no = 0 / 656, train loss = 5984.298, wps = 1, since beginning = 20 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5599.479, wps = 1, since beginning = 20 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6218.892, wps = 1, since beginning = 20 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6209.411, wps = 1, since beginning = 20 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6340.220, wps = 1, since beginning = 20 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6642.137, wps = 1, since beginning = 20 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5579.593, wps = 1, since beginning = 20 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 42\n",
            "batch no = 0 / 656, train loss = 6214.526, wps = 1, since beginning = 20 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5595.444, wps = 1, since beginning = 20 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6413.708, wps = 1, since beginning = 21 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5431.269, wps = 1, since beginning = 21 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6668.640, wps = 1, since beginning = 21 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6046.499, wps = 1, since beginning = 21 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 7256.640, wps = 1, since beginning = 21 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 43\n",
            "batch no = 0 / 656, train loss = 6190.648, wps = 1, since beginning = 21 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6441.777, wps = 1, since beginning = 21 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5716.784, wps = 1, since beginning = 21 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5686.793, wps = 1, since beginning = 21 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6493.638, wps = 1, since beginning = 21 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5803.888, wps = 1, since beginning = 21 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5621.883, wps = 1, since beginning = 21 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 44\n",
            "batch no = 0 / 656, train loss = 5309.242, wps = 1, since beginning = 21 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6909.338, wps = 1, since beginning = 21 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5689.604, wps = 1, since beginning = 21 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6473.750, wps = 1, since beginning = 22 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5457.243, wps = 1, since beginning = 22 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5427.558, wps = 1, since beginning = 22 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6035.474, wps = 1, since beginning = 22 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 45\n",
            "batch no = 0 / 656, train loss = 5194.499, wps = 1, since beginning = 22 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5924.086, wps = 1, since beginning = 22 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6154.445, wps = 1, since beginning = 22 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5763.938, wps = 1, since beginning = 22 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6186.671, wps = 1, since beginning = 22 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6083.573, wps = 1, since beginning = 22 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6341.347, wps = 1, since beginning = 22 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 46\n",
            "batch no = 0 / 656, train loss = 6258.837, wps = 1, since beginning = 22 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5445.485, wps = 1, since beginning = 22 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6465.024, wps = 1, since beginning = 22 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6114.461, wps = 1, since beginning = 23 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5715.249, wps = 1, since beginning = 23 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6143.906, wps = 1, since beginning = 23 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6554.763, wps = 1, since beginning = 23 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 47\n",
            "batch no = 0 / 656, train loss = 6036.754, wps = 1, since beginning = 23 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 4626.669, wps = 1, since beginning = 23 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6437.483, wps = 1, since beginning = 23 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5689.308, wps = 1, since beginning = 23 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5948.476, wps = 1, since beginning = 23 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5471.571, wps = 1, since beginning = 23 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5598.015, wps = 1, since beginning = 23 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 48\n",
            "batch no = 0 / 656, train loss = 5434.891, wps = 1, since beginning = 23 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5855.913, wps = 1, since beginning = 23 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6246.245, wps = 1, since beginning = 23 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5947.711, wps = 1, since beginning = 24 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5918.142, wps = 1, since beginning = 24 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6530.490, wps = 1, since beginning = 24 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5629.574, wps = 1, since beginning = 24 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 49\n",
            "batch no = 0 / 656, train loss = 6453.027, wps = 1, since beginning = 24 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6051.750, wps = 1, since beginning = 24 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5832.689, wps = 1, since beginning = 24 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5772.986, wps = 1, since beginning = 24 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6189.288, wps = 1, since beginning = 24 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5302.379, wps = 1, since beginning = 24 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5779.913, wps = 1, since beginning = 24 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 50\n",
            "batch no = 0 / 656, train loss = 5573.213, wps = 1, since beginning = 24 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5989.684, wps = 1, since beginning = 24 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5951.954, wps = 1, since beginning = 24 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5721.290, wps = 1, since beginning = 25 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5654.292, wps = 1, since beginning = 25 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5758.798, wps = 1, since beginning = 25 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6080.560, wps = 1, since beginning = 25 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 51\n",
            "batch no = 0 / 656, train loss = 5510.758, wps = 1, since beginning = 25 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5946.357, wps = 1, since beginning = 25 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6005.134, wps = 1, since beginning = 25 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5516.600, wps = 1, since beginning = 25 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5528.768, wps = 1, since beginning = 25 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6235.050, wps = 1, since beginning = 25 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6082.790, wps = 1, since beginning = 25 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 52\n",
            "batch no = 0 / 656, train loss = 5075.381, wps = 1, since beginning = 25 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5732.665, wps = 1, since beginning = 25 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6095.121, wps = 1, since beginning = 25 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5773.519, wps = 1, since beginning = 26 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6485.010, wps = 1, since beginning = 26 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5691.632, wps = 1, since beginning = 26 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6293.529, wps = 1, since beginning = 26 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 53\n",
            "batch no = 0 / 656, train loss = 6485.316, wps = 1, since beginning = 26 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6085.356, wps = 1, since beginning = 26 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5915.762, wps = 1, since beginning = 26 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5701.642, wps = 1, since beginning = 26 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5496.900, wps = 1, since beginning = 26 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5562.938, wps = 1, since beginning = 26 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6009.877, wps = 1, since beginning = 26 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 54\n",
            "batch no = 0 / 656, train loss = 5337.280, wps = 1, since beginning = 26 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5752.713, wps = 1, since beginning = 26 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5598.109, wps = 1, since beginning = 26 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5552.580, wps = 1, since beginning = 27 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6078.551, wps = 1, since beginning = 27 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 4769.134, wps = 1, since beginning = 27 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5425.108, wps = 1, since beginning = 27 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 55\n",
            "batch no = 0 / 656, train loss = 5835.755, wps = 1, since beginning = 27 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6173.069, wps = 1, since beginning = 27 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5832.250, wps = 1, since beginning = 27 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6157.833, wps = 1, since beginning = 27 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6210.496, wps = 1, since beginning = 27 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5544.304, wps = 1, since beginning = 27 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5709.595, wps = 1, since beginning = 27 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 56\n",
            "batch no = 0 / 656, train loss = 5049.000, wps = 1, since beginning = 27 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5891.506, wps = 1, since beginning = 27 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6053.322, wps = 1, since beginning = 27 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5232.435, wps = 1, since beginning = 28 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5867.644, wps = 1, since beginning = 28 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 4674.961, wps = 1, since beginning = 28 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6906.754, wps = 1, since beginning = 28 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 57\n",
            "batch no = 0 / 656, train loss = 5829.267, wps = 1, since beginning = 28 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5392.961, wps = 1, since beginning = 28 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5853.125, wps = 1, since beginning = 28 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5238.463, wps = 1, since beginning = 28 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5398.274, wps = 1, since beginning = 28 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6276.111, wps = 1, since beginning = 28 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5665.629, wps = 1, since beginning = 28 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 58\n",
            "batch no = 0 / 656, train loss = 6238.541, wps = 1, since beginning = 28 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5816.632, wps = 1, since beginning = 28 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6156.804, wps = 1, since beginning = 28 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6020.650, wps = 1, since beginning = 29 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 4926.004, wps = 1, since beginning = 29 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5749.954, wps = 1, since beginning = 29 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5851.384, wps = 1, since beginning = 29 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 59\n",
            "batch no = 0 / 656, train loss = 5933.464, wps = 1, since beginning = 29 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 6273.152, wps = 1, since beginning = 29 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5879.796, wps = 1, since beginning = 29 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5567.866, wps = 1, since beginning = 29 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5408.299, wps = 1, since beginning = 29 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5753.401, wps = 1, since beginning = 29 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5911.796, wps = 1, since beginning = 29 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 60\n",
            "batch no = 0 / 656, train loss = 6036.053, wps = 1, since beginning = 29 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 4984.906, wps = 1, since beginning = 29 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5827.769, wps = 1, since beginning = 29 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6275.909, wps = 1, since beginning = 30 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5979.624, wps = 1, since beginning = 30 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6024.898, wps = 1, since beginning = 30 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5723.067, wps = 1, since beginning = 30 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 61\n",
            "batch no = 0 / 656, train loss = 5490.872, wps = 1, since beginning = 30 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5886.267, wps = 1, since beginning = 30 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5897.873, wps = 1, since beginning = 30 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5019.278, wps = 1, since beginning = 30 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5632.583, wps = 1, since beginning = 30 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5718.875, wps = 1, since beginning = 30 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5476.164, wps = 1, since beginning = 30 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 62\n",
            "batch no = 0 / 656, train loss = 6695.418, wps = 1, since beginning = 30 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5790.746, wps = 1, since beginning = 30 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5944.909, wps = 1, since beginning = 30 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5856.219, wps = 1, since beginning = 31 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5213.951, wps = 1, since beginning = 31 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5618.772, wps = 1, since beginning = 31 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5739.776, wps = 1, since beginning = 31 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 63\n",
            "batch no = 0 / 656, train loss = 5964.979, wps = 1, since beginning = 31 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5896.464, wps = 1, since beginning = 31 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5167.594, wps = 1, since beginning = 31 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6199.292, wps = 1, since beginning = 31 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5629.333, wps = 1, since beginning = 31 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5884.648, wps = 1, since beginning = 31 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5836.765, wps = 1, since beginning = 31 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 64\n",
            "batch no = 0 / 656, train loss = 5448.475, wps = 1, since beginning = 31 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5041.949, wps = 1, since beginning = 31 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5836.586, wps = 1, since beginning = 31 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5228.640, wps = 1, since beginning = 32 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5216.060, wps = 1, since beginning = 32 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5060.387, wps = 1, since beginning = 32 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5370.509, wps = 1, since beginning = 32 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 65\n",
            "batch no = 0 / 656, train loss = 5446.207, wps = 1, since beginning = 32 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5369.914, wps = 1, since beginning = 32 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5234.399, wps = 1, since beginning = 32 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5872.342, wps = 1, since beginning = 32 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5774.531, wps = 1, since beginning = 32 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5626.581, wps = 1, since beginning = 32 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5862.632, wps = 1, since beginning = 32 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 66\n",
            "batch no = 0 / 656, train loss = 5299.017, wps = 1, since beginning = 32 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5805.979, wps = 1, since beginning = 32 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5643.948, wps = 1, since beginning = 32 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5834.646, wps = 1, since beginning = 33 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5790.147, wps = 1, since beginning = 33 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5914.021, wps = 1, since beginning = 33 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5750.901, wps = 1, since beginning = 33 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 67\n",
            "batch no = 0 / 656, train loss = 5753.142, wps = 1, since beginning = 33 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5977.129, wps = 1, since beginning = 33 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5700.747, wps = 1, since beginning = 33 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5396.166, wps = 1, since beginning = 33 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5977.081, wps = 1, since beginning = 33 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5447.950, wps = 1, since beginning = 33 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5613.037, wps = 1, since beginning = 33 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 68\n",
            "batch no = 0 / 656, train loss = 5708.628, wps = 1, since beginning = 33 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5234.631, wps = 1, since beginning = 33 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5289.225, wps = 1, since beginning = 33 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5744.656, wps = 1, since beginning = 33 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5923.399, wps = 1, since beginning = 34 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5583.481, wps = 1, since beginning = 34 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6366.783, wps = 1, since beginning = 34 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 69\n",
            "batch no = 0 / 656, train loss = 5529.961, wps = 1, since beginning = 34 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5915.002, wps = 1, since beginning = 34 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5708.591, wps = 1, since beginning = 34 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5981.480, wps = 1, since beginning = 34 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5553.858, wps = 1, since beginning = 34 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5592.209, wps = 1, since beginning = 34 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6734.105, wps = 1, since beginning = 34 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 70\n",
            "batch no = 0 / 656, train loss = 5530.701, wps = 1, since beginning = 34 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5718.436, wps = 1, since beginning = 34 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5724.838, wps = 1, since beginning = 34 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5812.577, wps = 1, since beginning = 34 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5974.552, wps = 1, since beginning = 35 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5103.743, wps = 1, since beginning = 35 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5301.900, wps = 1, since beginning = 35 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 71\n",
            "batch no = 0 / 656, train loss = 5330.102, wps = 1, since beginning = 35 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5281.406, wps = 1, since beginning = 35 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5566.121, wps = 1, since beginning = 35 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5788.211, wps = 1, since beginning = 35 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5323.678, wps = 1, since beginning = 35 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 4964.676, wps = 1, since beginning = 35 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5230.145, wps = 1, since beginning = 35 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 72\n",
            "batch no = 0 / 656, train loss = 5399.406, wps = 1, since beginning = 35 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5425.578, wps = 1, since beginning = 35 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5223.313, wps = 1, since beginning = 35 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5630.916, wps = 1, since beginning = 35 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5434.062, wps = 1, since beginning = 36 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 4803.794, wps = 1, since beginning = 36 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5709.761, wps = 1, since beginning = 36 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 73\n",
            "batch no = 0 / 656, train loss = 5232.612, wps = 1, since beginning = 36 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5800.048, wps = 1, since beginning = 36 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 4690.992, wps = 1, since beginning = 36 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5719.607, wps = 1, since beginning = 36 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5602.873, wps = 1, since beginning = 36 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5913.606, wps = 1, since beginning = 36 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5709.791, wps = 1, since beginning = 36 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 74\n",
            "batch no = 0 / 656, train loss = 5778.997, wps = 1, since beginning = 36 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5525.767, wps = 1, since beginning = 36 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5377.638, wps = 1, since beginning = 36 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5571.725, wps = 1, since beginning = 36 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6307.212, wps = 1, since beginning = 37 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5388.807, wps = 1, since beginning = 37 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5540.022, wps = 1, since beginning = 37 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 75\n",
            "batch no = 0 / 656, train loss = 5585.680, wps = 1, since beginning = 37 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5517.956, wps = 1, since beginning = 37 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5301.937, wps = 1, since beginning = 37 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5724.315, wps = 1, since beginning = 37 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5696.402, wps = 1, since beginning = 37 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5823.865, wps = 1, since beginning = 37 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5501.708, wps = 1, since beginning = 37 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 76\n",
            "batch no = 0 / 656, train loss = 5833.713, wps = 1, since beginning = 37 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5024.766, wps = 1, since beginning = 37 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5768.204, wps = 1, since beginning = 37 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5714.968, wps = 1, since beginning = 37 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6051.687, wps = 1, since beginning = 38 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 4735.133, wps = 1, since beginning = 38 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5557.684, wps = 1, since beginning = 38 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 77\n",
            "batch no = 0 / 656, train loss = 5523.463, wps = 1, since beginning = 38 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5403.573, wps = 1, since beginning = 38 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6042.581, wps = 1, since beginning = 38 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5457.124, wps = 1, since beginning = 38 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5523.288, wps = 1, since beginning = 38 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5634.489, wps = 1, since beginning = 38 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6006.904, wps = 1, since beginning = 38 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 78\n",
            "batch no = 0 / 656, train loss = 5293.009, wps = 1, since beginning = 38 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5497.243, wps = 1, since beginning = 38 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5660.479, wps = 1, since beginning = 38 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5506.800, wps = 1, since beginning = 38 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5092.657, wps = 1, since beginning = 39 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5150.596, wps = 1, since beginning = 39 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5746.183, wps = 1, since beginning = 39 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 79\n",
            "batch no = 0 / 656, train loss = 5469.780, wps = 1, since beginning = 39 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5922.663, wps = 1, since beginning = 39 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6288.107, wps = 1, since beginning = 39 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5731.800, wps = 1, since beginning = 39 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5309.150, wps = 1, since beginning = 39 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5261.514, wps = 1, since beginning = 39 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 4897.143, wps = 1, since beginning = 39 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 80\n",
            "batch no = 0 / 656, train loss = 5477.767, wps = 1, since beginning = 39 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5750.457, wps = 1, since beginning = 39 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5602.952, wps = 1, since beginning = 39 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5149.469, wps = 1, since beginning = 39 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5409.246, wps = 1, since beginning = 40 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5496.688, wps = 1, since beginning = 40 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5616.884, wps = 1, since beginning = 40 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 81\n",
            "batch no = 0 / 656, train loss = 5476.865, wps = 1, since beginning = 40 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5137.511, wps = 1, since beginning = 40 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5412.062, wps = 1, since beginning = 40 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5545.993, wps = 1, since beginning = 40 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5424.712, wps = 1, since beginning = 40 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5695.314, wps = 1, since beginning = 40 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5828.924, wps = 1, since beginning = 40 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 82\n",
            "batch no = 0 / 656, train loss = 5292.744, wps = 1, since beginning = 40 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5316.267, wps = 1, since beginning = 40 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6106.742, wps = 1, since beginning = 40 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5564.305, wps = 1, since beginning = 40 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5352.642, wps = 1, since beginning = 41 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5607.306, wps = 1, since beginning = 41 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5603.080, wps = 1, since beginning = 41 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 83\n",
            "batch no = 0 / 656, train loss = 5457.192, wps = 1, since beginning = 41 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5329.051, wps = 1, since beginning = 41 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 4743.664, wps = 0, since beginning = 41 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5989.898, wps = 1, since beginning = 41 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5377.665, wps = 1, since beginning = 41 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6044.314, wps = 1, since beginning = 41 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5335.867, wps = 1, since beginning = 41 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 84\n",
            "batch no = 0 / 656, train loss = 5848.708, wps = 1, since beginning = 41 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5300.826, wps = 1, since beginning = 41 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5633.662, wps = 1, since beginning = 41 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5283.515, wps = 1, since beginning = 41 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6206.136, wps = 1, since beginning = 42 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5385.033, wps = 1, since beginning = 42 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6115.225, wps = 1, since beginning = 42 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 85\n",
            "batch no = 0 / 656, train loss = 5632.585, wps = 1, since beginning = 42 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5405.179, wps = 1, since beginning = 42 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6323.506, wps = 1, since beginning = 42 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5483.651, wps = 1, since beginning = 42 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 4852.746, wps = 1, since beginning = 42 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6024.213, wps = 1, since beginning = 42 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5319.333, wps = 1, since beginning = 42 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 86\n",
            "batch no = 0 / 656, train loss = 6284.750, wps = 1, since beginning = 42 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5967.795, wps = 1, since beginning = 42 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5625.336, wps = 1, since beginning = 42 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5663.961, wps = 1, since beginning = 42 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5199.863, wps = 0, since beginning = 43 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5466.872, wps = 1, since beginning = 43 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5849.435, wps = 1, since beginning = 43 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 87\n",
            "batch no = 0 / 656, train loss = 5618.863, wps = 1, since beginning = 43 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5872.241, wps = 1, since beginning = 43 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5381.727, wps = 1, since beginning = 43 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5485.131, wps = 1, since beginning = 43 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5908.464, wps = 1, since beginning = 43 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 4999.622, wps = 0, since beginning = 43 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 6138.939, wps = 1, since beginning = 43 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 88\n",
            "batch no = 0 / 656, train loss = 6166.149, wps = 1, since beginning = 43 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5312.722, wps = 1, since beginning = 43 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5464.329, wps = 1, since beginning = 43 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5204.624, wps = 0, since beginning = 43 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5363.373, wps = 1, since beginning = 43 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5537.036, wps = 1, since beginning = 44 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5669.825, wps = 0, since beginning = 44 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 89\n",
            "batch no = 0 / 656, train loss = 5470.659, wps = 1, since beginning = 44 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5536.600, wps = 1, since beginning = 44 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6324.136, wps = 1, since beginning = 44 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5597.560, wps = 1, since beginning = 44 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5920.490, wps = 1, since beginning = 44 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5308.925, wps = 1, since beginning = 44 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5663.323, wps = 1, since beginning = 44 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 90\n",
            "batch no = 0 / 656, train loss = 5021.645, wps = 0, since beginning = 44 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5072.602, wps = 0, since beginning = 44 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5148.949, wps = 0, since beginning = 44 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6071.279, wps = 1, since beginning = 44 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5602.612, wps = 1, since beginning = 44 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5732.689, wps = 1, since beginning = 45 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5761.910, wps = 1, since beginning = 45 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 91\n",
            "batch no = 0 / 656, train loss = 5403.059, wps = 0, since beginning = 45 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5286.470, wps = 0, since beginning = 45 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 4999.156, wps = 0, since beginning = 45 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5256.043, wps = 0, since beginning = 45 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5507.672, wps = 1, since beginning = 45 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5808.659, wps = 1, since beginning = 45 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5839.052, wps = 1, since beginning = 45 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 92\n",
            "batch no = 0 / 656, train loss = 4995.915, wps = 0, since beginning = 45 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5389.542, wps = 1, since beginning = 45 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6170.100, wps = 1, since beginning = 45 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6021.049, wps = 1, since beginning = 45 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6263.707, wps = 1, since beginning = 45 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5632.257, wps = 0, since beginning = 46 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5690.775, wps = 1, since beginning = 46 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 93\n",
            "batch no = 0 / 656, train loss = 5041.597, wps = 0, since beginning = 46 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5682.874, wps = 1, since beginning = 46 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5752.080, wps = 1, since beginning = 46 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5823.584, wps = 1, since beginning = 46 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6237.864, wps = 1, since beginning = 46 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5424.028, wps = 0, since beginning = 46 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5229.175, wps = 0, since beginning = 46 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 94\n",
            "batch no = 0 / 656, train loss = 5518.232, wps = 0, since beginning = 46 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5954.781, wps = 1, since beginning = 46 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5169.205, wps = 0, since beginning = 46 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5528.143, wps = 0, since beginning = 46 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 6383.604, wps = 1, since beginning = 46 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 4939.058, wps = 0, since beginning = 47 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5162.552, wps = 0, since beginning = 47 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 95\n",
            "batch no = 0 / 656, train loss = 6016.214, wps = 1, since beginning = 47 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5873.074, wps = 1, since beginning = 47 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 4877.701, wps = 0, since beginning = 47 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5198.689, wps = 0, since beginning = 47 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5297.371, wps = 0, since beginning = 47 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5339.479, wps = 0, since beginning = 47 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 4877.302, wps = 0, since beginning = 47 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 96\n",
            "batch no = 0 / 656, train loss = 5135.948, wps = 0, since beginning = 47 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5058.589, wps = 0, since beginning = 47 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5260.027, wps = 0, since beginning = 47 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 4756.355, wps = 0, since beginning = 47 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5392.796, wps = 0, since beginning = 47 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 4973.321, wps = 0, since beginning = 48 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5555.231, wps = 1, since beginning = 48 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 97\n",
            "batch no = 0 / 656, train loss = 4708.917, wps = 0, since beginning = 48 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5033.426, wps = 0, since beginning = 48 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 6146.833, wps = 1, since beginning = 48 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5558.928, wps = 0, since beginning = 48 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5859.295, wps = 0, since beginning = 48 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6217.501, wps = 1, since beginning = 48 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5778.684, wps = 1, since beginning = 48 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 98\n",
            "batch no = 0 / 656, train loss = 5479.832, wps = 0, since beginning = 48 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 4564.313, wps = 0, since beginning = 48 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5116.174, wps = 0, since beginning = 48 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5532.059, wps = 0, since beginning = 48 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5087.716, wps = 0, since beginning = 48 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5671.937, wps = 0, since beginning = 49 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5678.387, wps = 0, since beginning = 49 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 99\n",
            "batch no = 0 / 656, train loss = 5404.241, wps = 0, since beginning = 49 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5632.039, wps = 0, since beginning = 49 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 5136.455, wps = 0, since beginning = 49 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 6004.353, wps = 0, since beginning = 49 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5479.291, wps = 0, since beginning = 49 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 5091.642, wps = 0, since beginning = 49 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5468.487, wps = 0, since beginning = 49 mins, cuda memory = 0.913 GBs\n",
            "Run : 4, Epoch : 100\n",
            "batch no = 0 / 656, train loss = 5148.224, wps = 0, since beginning = 49 mins, cuda memory = 0.913 GBs\n",
            "batch no = 100 / 656, train loss = 5388.184, wps = 0, since beginning = 49 mins, cuda memory = 0.913 GBs\n",
            "batch no = 200 / 656, train loss = 4755.410, wps = 0, since beginning = 49 mins, cuda memory = 0.913 GBs\n",
            "batch no = 300 / 656, train loss = 5606.619, wps = 0, since beginning = 49 mins, cuda memory = 0.913 GBs\n",
            "batch no = 400 / 656, train loss = 5194.409, wps = 0, since beginning = 49 mins, cuda memory = 0.913 GBs\n",
            "batch no = 500 / 656, train loss = 6098.919, wps = 0, since beginning = 50 mins, cuda memory = 0.913 GBs\n",
            "batch no = 600 / 656, train loss = 5922.786, wps = 0, since beginning = 50 mins, cuda memory = 0.913 GBs\n",
            "Early stopping triggered with counter 2 and patience 2\n",
            "Test loss 4.554 +- 0.001\n",
            "Test perplexity 95.019 +- 0.142\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Error Analysis**"
      ],
      "metadata": {
        "id": "A0OWvW_khwM0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we will try to draw conclusions about the functioning of the implemented model, attempting to give an explanation for any difficulties encountered."
      ],
      "metadata": {
        "id": "-pJo_vg3jdi6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Words with the highest frequency in both the prediction and the target set**"
      ],
      "metadata": {
        "id": "gVwsl2tVh37U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each example in the test dataset, the model's predictions are obtained, the loss is calculated, and the targets and predictions are converted to lists.\n",
        "\n",
        "A dictionary is created to store information about the current example, including the first word of the input data, the target and predicted labels, the length of the input data, and the loss. The code then creates a mask to check if the target and prediction match and counts the number of matches. The code also filters out the matching labels from the target and predictions and checks if the predictions are in the set of non-matching targets.\n",
        "\n",
        "Finally, the code creates a list of dictionaries containing information about each prediction, a list of tuples (target_label, count) and a list of tuples (predicted label, count) as the output.\n",
        "\n",
        "The information stored in the list of dictionaries includes:\n",
        "\n",
        "*    The first word of the input data;\n",
        "*    The target labels of the input data;\n",
        "*    The predicted labels of the input data;\n",
        "*    The loss value for the current example;\n",
        "*    The length of the input data;\n",
        "*    The number of correct predictions;\n",
        "*    The number of predictions in the set of non-matching targets.\n",
        "\n",
        "This list, along with the two lists of tuples, can be used to evaluate the performance of the model on the test dataset. The list of tuples (target_label, count) represents the frequency of each target label in the test dataset, while the list of tuples (predicted_label, count) represents the frequency of each predicted label."
      ],
      "metadata": {
        "id": "HWRfPPs2UvT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions\n",
        "test_stat = []\n",
        "target_freq = []\n",
        "predict_freq = []\n",
        "field0 = {'1stw': '', 'target': [], 'predict': [], 'loss': 0.0, 'length': 0, 'correct': 0, 'in_set': 0 }\n",
        "\n",
        "hid_size = 500      \n",
        "emb_size = 500 \n",
        "\n",
        "# Load the model's state_dict\n",
        "checkpoint = torch.load(fold_path+\"RegModel\")\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Create a DataLoader for the test dataset\n",
        "test_loader = DataLoader(test_dataset, batch_size = 1, collate_fn=collate_fn)\n",
        "\n",
        "# Define the score function (CrossEntropyLoss in this case)\n",
        "score_function = nn.CrossEntropyLoss(ignore_index = PAD_TOKEN)\n",
        "\n",
        "# Iterate over the test data without keeping track of gradients\n",
        "with torch.no_grad():\n",
        "\n",
        "  for i, (inputs, targets, length) in enumerate(test_loader):\n",
        "      \"\"\"\n",
        "        Iterate over the test dataset and make predictions\n",
        "        \n",
        "        Parameters:\n",
        "            inputs (torch.Tensor): the input data\n",
        "            targets (torch.Tensor): the ground truth labels\n",
        "            length (torch.Tensor): the length of the input data\n",
        "        \n",
        "        Returns:\n",
        "            test_stat (list): a list of dictionaries containing information about the predictions\n",
        "            target_freq (list): a list of tuples (target_label, count)\n",
        "            predict_freq (list): a list of tuples (predicted_label, count)\n",
        "      \"\"\"   \n",
        "      inputs = inputs.to(device)\n",
        "      targets = targets.to(device)\n",
        "      length = length.to(device)\n",
        "        \n",
        "      # Get the model's predictions\n",
        "      scores = model(inputs, length)\n",
        "\n",
        "      predicted_id = scores.argmax(dim=-1) \n",
        "\n",
        "      # Calculate the loss\n",
        "      loss = score_function(scores, targets.view(-1))\n",
        "      # Convert the targets and predictions to lists\n",
        "      target_list = targets.reshape(-1).cpu().numpy().tolist()\n",
        "      predict_list = predicted_id.cpu().numpy().tolist()\n",
        "\n",
        "      # Create a dictionary to store information about the current example\n",
        "      field = dict(field0)\n",
        "      field['1stw'] = inputs[0][0].item()\n",
        "      field['target'] = list(target_list)\n",
        "      field['predict'] = list(predict_list)\n",
        "      field['length'] = length.item()\n",
        "      field['loss'] = loss.item()\n",
        "\n",
        "      # Create a mask to check if the target and prediction match\n",
        "      mask = [1 if x == y else 0 for x, y in zip(target_list, predict_list)]\n",
        "      field['correct'] = mask.count(1)\n",
        "\n",
        "      # Filter out the matching labels from the target and predictions\n",
        "      target_list = list(filter(lambda x_m: x_m[0] if x_m[1] == 0 else None, zip(target_list, mask)))\n",
        "      predict_list = list(filter(lambda x_m: x_m[0] if x_m[1] == 0 else None, zip(predict_list, mask)))\n",
        " \n",
        "      # Check if the predictions are in the set of non-matching targets\n",
        "      for s in range(len(predict_list)):\n",
        "        if predict_list[s] in target_list:\n",
        "          index = target_list.index(predict_list[s])\n",
        "          target_list.pop(index)\n",
        "          field['in_set']+=1\n",
        "      test_stat.append(field)\n",
        "\n",
        "\n",
        "  target_words = [f['target'] for f in test_stat]\n",
        "  target_words = sum(target_words, [])\n",
        "  target_words = list(set(target_words))\n",
        "\n",
        "  for w in range(len(target_words)):\n",
        "      count = sum(t['target'].count(target_words[w]) for t in test_stat)\n",
        "      target_freq.append((target_words[w], count))\n",
        "  target_freq = sorted(target_freq, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "  n_elem = 10\n",
        "  y = [vocab.id2word[t[0]] for t in target_freq[:n_elem]]\n",
        "  x = [t[1] for t in target_freq[:n_elem]]\n",
        "  plt.title('Words with the highest frequency in the target set')\n",
        "  plt.barh(range(1,n_elem+1), x, 0.05, color='b')\n",
        "  plt.yticks(range(1,n_elem+1), y)\n",
        "  plt.ylabel('Labels')\n",
        "  plt.xlabel('Number of occurrencies')\n",
        "  plt.show()\n",
        "\n",
        "  predict_words = [f['predict'] for f in test_stat]\n",
        "  predict_words = sum(predict_words, [])\n",
        "  predict_words = list(set(predict_words))\n",
        "\n",
        "  for w in range(len(predict_words)):\n",
        "      count = sum(t['predict'].count(predict_words[w]) for t in test_stat)\n",
        "      predict_freq.append((predict_words[w], count))\n",
        "  predict_freq = sorted(predict_freq, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "  n_elem = 10\n",
        "  y = [vocab.id2word[t[0]] for t in predict_freq[:n_elem]]\n",
        "  x = [t[1] for t in predict_freq[:n_elem]]\n",
        "  plt.title('Words with the highest frequency in the prediction set')\n",
        "  plt.barh(range(1,n_elem+1), x, 0.05, color='b')\n",
        "  plt.yticks(range(1,n_elem+1), y)\n",
        "  plt.ylabel('Labels')\n",
        "  plt.xlabel('Number of occurrencies')\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "EsQz1U-c0emP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "outputId": "8196c227-51df-4847-b908-5f0aedf6db49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEWCAYAAACufwpNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de5wcVZn/8c+XJBAJGALJIgghgLLclEtGBQkYXVRWV1EWDQgq16yyincE3dWwC7sowk8UV4hBQGERRG7iIpddElhuIQMkBCFyjYEEhHBLwF+4PfvHOR1qOt0znUn1dPfM9/169WuqTlWdfupUdT1dp3qqFBGYmZmVYa1WB2BmZoOHk4qZmZXGScXMzErjpGJmZqVxUjEzs9I4qZiZWWmcVFpI0jRJ5zWx/uWStupl+iOS9l6D+s+RdEJ/l6+qa6akI+pMG5/XZVgD9UyQFJKGlxHX6pD0cUmLcqy7DPT7t1pf+9tq1nWIpP8to65mK36OV2dfrVHPtyTNKD/CgeWkUiDpOElXVZXdX6fsgIGNbvVFxHoR8RCseQJo5Yc8Iv6U1+XVVrw/9J70Cn4AfCHHeudAxNVOivvb6mjlF4GyNbqvSpos6dGqZf8tIvraxwZMf48ZTio93QC8u/ItQ9ImwAhgl6qyt+R5GzYYPjDWpy2Ae2pN8PbvDN5Oa85JpafbSUlk5zy+J3A9sKCq7MGIWCxpU0lXSHpa0gOSjqxUlE+JL5Z0nqTngUMkbSlplqRlkq4FxhbmH5nnXSrpWUm3S9q4OkBJh0r6bWH8fkm/LowvkrRzHg5Jb5E0FTgIOCafmv+2UOXOkuZJek7ShZJG1njP7YAzgN3z8s8WJo+R9Lu8TrdJ2rqw3LaSrs3ts0DSJ3tvfraQdFOu6xpJY3M9Pb7J5na8Ic93naSf1OhGPEjSnyQ9JenbhZjWknSspAdzW18kacPetoGkE0nb/fS8/qdXtc86kpYDw4C5kh7M5Y9I+qakecALkoZL2k3Szbn+uZImF+rpsX9IOl2vd6us8s1Whe7LPtar0n6frdMmw5S6Xh7M790tafPcrqdUvecVkr5Sa+NV9rc8fE5evua+UaXyBe3Z3L67F+r8gaRnJD0s6W8L5aMlnSVpiaTHJJ2gOl1Oev2zeGGO5Q5JO1W1Y7+3Ez0/x9X76oaSzpa0OK/HZZJGAVcBm+b1Xa50LOnRHS7po5LuyTHMVPocFmP+uvr47OZ535LjfS5v+wsL02p+RtX7MaN3EeFX4UVKIl/Jw6cDhwEnVpX9PA/fAPwHMJKUdJ4E3penTQNeBj5GSt5vAG4BTgXWAfYClgHn5fn/AfgtsC7p4DQReGON+LYCns11bgosBB4tTHsGWCuPB/CWPHwOcEJVXY8As3M9GwL3Ap+r0y6HAP9bVXYOsBR4JzAcOB/4VZ42ClgEHJqn7QI8BWxfp/6ZwIPANrmtZgIn5WkT8roMz+O3kLqa1gYmAc8X2rEy789yPTsBK4Dt8vQvAbcCm+XtcCZwQV/bIMdzRB/7zsr2LrTvXcDmOZY35/b6UN5+78/j4wrrVW//mFzZzlX1793AevXVJt8A7gb+GlCevlHerot5fX8aC7wIbNzX+ve2b9RYrsf2LexvLwNH5m3x+RyL8vRL8zqOAv6KtB//Q536p+W69id9afw68DAwognbqce6AL8DLgTG5Pd+Ty/bc1qhnm2AF/J7jwCOAR4A1u7HZ/cC4Nt5XUYCkxr5jFLjmNHQMXSgDtad8sob9tI8PBd4K7BPVdln8w74KrB+Ydl/B84p1HNDYdp44BVgVKHsPws70WHAzcDbG4hxEbArcAAwPe9c2+ad44rCfI0klYML498HzqjznodQO6nMKIx/CLgvD08Bbqya/0zgu3Xqnwn8U2H8KOD3eXhCXpfhhXZctzDveaz6od6sMH02cEAevhf4m8K0TUgHnOG9bQP6n1QOK4x/E/hl1TJX5/2pr/1jMr0nld7Wq682WQDsW2ed7gXen4e/APxXI+vf275RY7mV27dqf3ugML5unudNwMakpPiGwvQDget7+UzfWhhfC1gC7NmE7bRyXfI2eA0YUyOmWttzWqGefwYuqor5MWByPz67vyAdJzarKu/1M0o/k4q7v1Z1AzApdx2Mi4j7SQead+eyHfM8mwJPR8SywrILSd9yKhYVhjcFnomIF6rmr/glacf9VT5V/r6kEXVinEXaKffKwzOB9+TXrNVYV4DHC8MvAuuVtPwWwLvyqfuzSl1mB5EOCmsSS6XdXyyULaoxX29xXVqI6V7Sl4ONWb1t0KhibFsAn6hqk0mkg09f+0dfeluvinptsjnpLLGWc4GD8/DBpDZqVGn7VmF7r0da1xHAksL6nkk6Y6ln5XaIiNeAR0ltvsp0yttOm5P21Wd6iaueSi9EMeZF9Dy+NNq+x5DOQGfn7rTDcnl/PqN98kWpVd0CjCaddt8EEBHPS1qcyxZHxMOSXgE2lLR+IbGMJ32bqIjC8BLS9YdRhR1yfGWeiHgZOB44XtIE4L9I3yDPqhHjLOAjwJbAv5G6ww4Cdid1z9USdcobtbrLLwJmRcT71/B9qy0htfu6hQPN5qsZ12ERcVOd6fW2QX/br7jcItI34COrZ5K0Bb3sH6SukHUL8w8DxlXVXXO98rr0ZhGwNTC/xrTzgPn5GsR2wGV91NUf/dm3VgBjI+KVBpdZuY9IWovUTbi4Tgxrsp2q49xQ0gYR8WzVtL7WeTHwtsL7Kq/DY3WXqCMiHicdu5A0CbhO0g30/Rnt1z7vM5UqEfEXYA7wVeDGwqT/zWU35PkWkc5g/l3pAu/bgcNJH8Ja9S7M9R4vae28cT9SmS7pvZLelg8Wz5O6Ll6rE+Ys4L2k0/9Hc5z7kPrB6/2U9QnSNZf+egLYTNLaDc5/JbCNpE9LGpFf7yhebOyPQjtOy+24O4V2bMAZwIn54ICkcZL2zcO9bYM1bT9I+8ZHJH1Q6eL4SKUL8Jv1tX8AfwRGSvpwPnv6J1Kffp/r1YAZwL9KequSt0vaCCDvX7eTzlB+kz8fZXuS1M4NtW9ELAGuAU6R9EalHylsLek9vSw2UdJ+ShfQv0xKSrfWmXdNtlN1nFcB/yFpTP4M7JUnPwFsJGl0nRguAj4s6W/y9v5ajvnmXtaxJkmfkLRZHn2GlCxeo+/PaL/2eSeV2maRTqWL/5dxYy4r/pT4QFIf6mLShcPvRsR1vdT7KeBdwNPAd0l9nRVvAi4mHczuzTHU7GqIiD8Cy3NMRMTzwEPATVH/9/FnAdvn09z+fNv8H9LPZR+X9FRfM+eztw+QrvssJp2qf4+eB8L+qpyVLQVOIF0IXdHgsqcBVwDXSFpGOrC8K0/rbRucBuyv9AueH/Un6PxFZF/gW6QD6SLSRfLK57Du/hERz5GuM80gfVt9gdSF08h69eVU0kHsGtK6n0W6YF1xLulb8+p0fTUsn3GeCNyU98/dGljsM6QfavyBdKC8mNQ9Vc/lpGsIzwCfBvbLvQO14un3dqrh06QvJ/cBfyYlNCLiPtIF9IfyOhe74oiIBaTuxh+TLp5/BPhIRLzUy3vV8w7gNqVfKF4BfCkiHmrgM9qvY0bllxRmHUvpJ5L3RcR3Wx1LmSRNI134PriveZscx16kb+9bRAceMNqlHYcKn6lYx8mn6Fvnbo99SN8qm9HXP+TlrpcvkX7J1XEJxQaeL9RbJ3oTcAnpGtKjwOdjCN4Wpdly3/oc0s/oD21xONYh3P1lZmalcfeXmZmVZkh3f40dOzYmTJjQ6jDMzDpKd3f3UxExrta0IZ1UJkyYwJw5c1odhplZR5FU924P7v4yM7PSOKmYmVlpnFTMzKw0TipmZlYaJxUzMyuNk4qZmZWm45OKpNW+FbSZmTVHxyeViHh3q2MwM7Ok4//5UdLyiFhP0mTSM56fIj3yt5v0DOe6Nzfr7gZpQMLsk2/BZmaDQccnlSq7ADuQHjhzE7AHPR+0haSpwFSA8ePHs3B1ngJuZma96vjuryqzI+LRiHgNuIv0VMYeImJ6RHRFRNe4cTVvXWNmZv002JJK8ZGyrzL4zsTMzNraYEsqZmbWQk4qZmZWmo7vHoqI9fLfmcDMQvkXWhSSmdmQ5TMVMzMrjZOKmZmVxknFzMxK46RiZmalcVIxM7PSOKmYmVlpnFTMzKw0TipmZlYaJxUzMyuNk4qZmZXGScXMzErjpGJmZqUZtElF0vJWx2BmNtQM2qRiZmYDr62TiqTLJHVLuic/Wx5JyyWdKGmupFslbZzLt5R0i6S7JZ3Q2sjNzIamtk4qwGERMRHoAo6WtBEwCrg1InYCbgCOzPOeBvw0It4GLGmk8u5ukNrnZWbW6do9qRwtaS5wK7A58FbgJeDKPL0bmJCH9wAuyMO/rFehpKmS5kiaM378k0TQNi8zs07XtklF0mRgb2D3fFZyJzASeDli5SH4VXo+vbLPQ3NETI+IrojoGjduXMlRm5kNbW2bVIDRwDMR8aKkbYHd+pj/JuCAPHxQUyMzM7Oa2jmp/B4YLule4CRSF1hvvgT8o6S7gTc3OzgzM1uVYgh35nd1dcWcOXNaHYaZWUeR1B0RXbWmtfOZipmZdRgnFTMzK42TipmZlcZJxczMSuOkYmZmpXFSMTOz0jipmJlZaZxUzMysNE4qZmZWGicVMzMrjZOKmZmVxknFzMxK46RiZmalcVIxM7PSOKmYmVlpBl1SkXSZpG5J90ia2up4zMyGkuF9z9JxDouIpyW9Abhd0m8iYmllYk40OdmMR2pNkLUM4eelmdkgMejOVICjJc0lPX54c+CtxYkRMT0iuiKia+LEcUTQNi8zs043qM5UJE0G9gZ2j4gXJc0ERrY0KDOzIWSwnamMBp7JCWVbYLdWB2RmNpQMtqTye2C4pHuBk0hdYGZmNkAGVfdXRKwA/rbVcZiZDVWD7UzFzMxayEnFzMxK46RiZmalcVIxM7PSOKmYmVlpnFTMzKw0TipmZlYaJxUzMyuNk4qZmZXGScXMzErjpGJmZqVxUjEzs9I4qZiZWWkGRVKRdHOrYzAzs0GSVCLi3a2OwczMBklSkbQ8/91E0g2S7pI0X9KerY7NzGwoGVQP6QI+BVwdESdKGgasWz2DpKnA1DQ2HmlA4+tTRKsjMDPrv0FxplJwO3CopGnA2yJiWfUMETE9IroiomvixHFE0FYvM7NONqiSSkTcAOwFPAacI+kzLQ7JzGxIGVRJRdIWwBMR8TNgBrBri0MyMxtSBts1lcnANyS9DCwHfKZiZjaABkVSiYj18t9zgXNbHI6Z2ZA1qLq/zMystZxUzMysNE4qZmZWGicVMzMrjZOKmZmVxknFzMxK46RiZmalcVIxM7PSOKmYmVlpnFTMzKw0TipmZlYaJxUzMyuNk4qZmZWmo5OKpA0kHdXqOMzMLGkoqUjaQ9KoPHywpFPzA7FabQPAScXMrE00eqbyU+BFSTsBXwMeBH7RtKgadxKwtaS7JJ2cX/Ml3S1pSquDMzMbahp9SNcrERGS9gVOj4izJB3ezMAadCywY0TsLOnvgc8BOwFjgdsl3RARS4oLSJoKTE1j45EGNuB2FdHqCMxsMGj0TGWZpOOAg4HfSVoLGNG8sPplEnBBRLwaEU8As4B3VM8UEdMjoisiuiZOHEcEfjmhmFlJGk0qU4AVwOER8TiwGXBy06IyM7OO1FBSiYjHI+LUiLgxj/8pItrhmsoyYP08fCMwRdIwSeOAvYDZLYvMzGwI6vWaiqRlQK3OEQEREW9sSlQNioilkm6SNB+4CpgHzCXFfEw+qzIzswHSa1KJiPV7m94OIuJTVUXfaEkgZmbW+D8/Spok6dA8PFbSls0Ly8zMOlGj//z4XeCbwHG5aG3gvGYFZWZmnanRM5WPAx8FXgCIiMW8foHczMwMaDypvBQRQb5oX7lli5mZWVGjSeUiSWcCG0g6ErgO+FnzwjIzs07U0G1aIuIHkt4PPA9sA3wnIq5tamRmZtZxGr33F8DdwBtIXWB3NyccMzPrZI3++usI0n+n7wfsD9wq6bBmBmZmZp2n0TOVbwC7RMRSAEkbATcDP29WYGZm1nkavVC/lHSfrYpluczMzGylvu799dU8+ABwm6TLSddU9iXdZ8vMzGylvrq/Kv/g+GB+VVzenHDMzKyT9XVDyeMHKhAzM+t8DV2oz88nOQbYARhZKY+I9zUprjUi6Wjg88AdEXFQq+MxMxsqGr1Qfz5wH7AlcDzwCHB7k2Iqw1HA+51QzMwGVqM/Kd4oIs6S9KWImAXMktQWSSX/mKDyPzMzgG2BrYCrJP08Iv5fvWW7u0EagCAHAT/H3swa0WhSeTn/XSLpw8BiYMPmhNQ4SROBQ4F3kZ5GeRtwMLAP8N6IeKrGMlOBqWls/ECFamY2JDSaVE6QNBr4GvBj4I3Al5sWVeMmAZdGxAsAki4B9uxtgYiYDkwH6Orqijlzmh6jmdmQ0egNJa/Mg88B7wWQ1A5JxczM2kjDjxOu4at9z9J0NwIfk7RufsbLx3OZmZm1wOrcpbhayy9xR8Qdks4h3ewSYEZE3ClffTcza4k1SSpt8XugiDgVOLWqbEJrojEzG9r6uvfXMmonD5GerWJmZrZSX7dpWb+36WZmZkVrcqHezMysBycVMzMrjZOKmZmVxknFzMxK46RiZmalcVIxM7PSOKmYmVlpnFTMzKw0TipmZlYaJxUzMyuNk4qZmZVm0CQVSSHplML41yVNa2FIZmZDzqBJKsAKYD9JY1sdiJnZULUmz1NpN6+Qnj3/FeDbjSzQ3Q1+nteaibZ4qo6ZtYvBlFQAfgLMk/T9ejNImgpMBRg/fjwLFw5UaGZmg99g6v4iIp4HfgEc3cs80yOiKyK6xo0bN3DBmZkNAYMqqWQ/BA4HRrU6EDOzoWbQJZWIeBq4iJRYzMxsAA26pJKdAvhXYGZmA2zQXKiPiPUKw08A67YwHDOzIWmwnqmYmVkLOKmYmVlpnFTMzKw0TipmZlYaJxUzMyuNk4qZmZXGScXMzErjpGJmZqVxUjEzs9I4qZiZWWmcVMzMrDROKmZmVhonFTMzK42TipmZlWbAk4qktSWV9lRGSaMlOTmambWBATsYS9pO0inAAmCbXDZR0ixJ3ZKulrRJLt9Z0q2S5km6VNKYXH60pD/k8l/lqicBCyRNkzR+dWLq7gbJL78G58usFRQRzas8nZF8ktcf7Xs2cFFELJM0ApgF7BsRT0qaAnwwIg6TNA/4YkTMkvQvwBsj4suSFgNbRsQKSRtExLP5fcYCnwY+CzwOnAVcHhEv1YhpKjAVYPz48RMXLlzYtPU3MxuMJHVHRFfNaU1OKs8D84AjIuK+qmk7AjcDD+WiYcAS4BPA3RExPs+3NfDriNhV0u+B5cBlwGURsbzGe+4O/Bx4OSLe3lt8XV1dMWfOnDVZRTOzIae3pNLs7q/9gceASyR9R9IWxbiAeyJi5/x6W0R8oI/6Pgz8BNgVuF3SyschS9pe0snAL4CbgCNLXRMzM+tTU5NKRFwTEVOAPYHngMslXSdpAunayrh8ZoGkEZJ2iIjngGck7Zmr+TQwK1+M3zwirge+CYwG1pO0q6RbgRnAfcAuEXFERNzWzHUzM7NVDe97ljUXEUuB04DTJL0TeDUiXpK0P/AjSaNzLD8E7iFdGzlD0rqk7rFDSd1j5+V5BfwoIp6V9Bfg0Ii4dyDWxczM6huQpFIUEbMLw3cBe9WY5y5gtxqLT6oxr5OJmVmb8P93mJlZaZxUzMysNE4qZmZWGicVMzMrjZOKmZmVxknFzMxK46RiZmalcVIxM7PSOKmYmVlpnFTMzKw0TipmZlYaJxUzMytN2ycVSRtIOioPT5Z0ZatjMjOz2to+qQAbAEe1OggzM+vbgN/6vh9OAraWdBfwMvCCpIuBHYFu4OCICEkTgVOB9YCngEMiYkmrgjYzG4o6IakcC+wYETtLmgxcDuwALCY9NngPSbcBPwb2jYgnJU0BTgQO663i7m6Qmhq7mQ1hEa2OYOB1QlKpNjsiHgXIZy8TgGdJZy7XKmWJYUDNsxRJU4GpAOPHj2fhwgGI2MxsiOjEpLKiMPwqaR0E3BMRu/e1cERMB6YDdHV1DcHvEWZmzdMJF+qXAev3Mc8CYJyk3QEkjZC0Q9MjMzOzHtr+TCUilkq6SdJ84C/AEzXmeUnS/sCPJI0mrdcPgXsGNlozs6Gt7ZMKQER8qk75FwrDdwF7DVhQZma2ik7o/jIzsw7hpGJmZqVxUjEzs9I4qZiZWWmcVMzMrDROKmZmVhonFTMzK42TipmZlcZJxczMSuOkYmZmpXFSMTOz0jipmJlZaZxUzMysNB2TVCRNyLe/NzOzNjWgSUXS2pJGlVznKEkjyqzTzMz6Z0CSiqTtJJ1CekLjNrnsEUlj83CXpJl5eJqkn0uaKekhSUfXqG8rSXdKekeu74+SfiBpu4FYHzMzq61pD+nKZySfBA7PRWcD0yJiWQOLbwu8l/QY4QWSflqo96+BXwGHRMTcXPZ2YAowQ1IAZwEXRcQLNeKaCkxNY+OR+rV6ZmYdK6J5dTfzyY9LgHnAERFx32ou+7uIWAGskPRnYONcPg64HNgvIv5QmTknqhmkpLIdKamcBryxuuKImA5MB+jq6oo5c1YzMjMzq6uZ3V/7A48Bl0j6jqQtqqa/Unj/kVXTVhSGX+X15Pcc8CdgUvWb5Qv53wUuBRbl9zczswHUtKQSEddExBRgT1IyuFzSdZIm5FkeASbm4b9vsNqXgI8Dn5H0KViZTK4DLgOeBfaIiCkRcU0pK2JmZg1rZvcXABGxlNQVdZqkd5LOPACOB86S9K/AzNWo7wVJfwdcK2k5cCfwrYiYXW7kZma2uhTNvGLT5tI1FV9UMTNbHZK6I6Kr1rSO+edHMzNrf04qZmZWGicVMzMrjZOKmZmVxknFzMxKM6R//SVpGel+ZJ1gLPBUq4NokGNtDsdavk6JE9or1i0iYlytCU3/P5U2t6Dez+LajaQ5jrV8jrU5OiXWTokTOidWd3+ZmVlpnFTMzKw0Qz2pTG91AKvBsTaHY22OTom1U+KEDol1SF+oNzOzcg31MxUzMyuRk4qZmZVmyCYVSftIWiDpAUnHtuD9N5d0vaQ/SLpH0pdy+YaSrpV0f/47JpdL0o9yvPMk7Vqo67N5/vslfbaJMQ+TdKekK/P4lpJuyzFdKGntXL5OHn8gT59QqOO4XL5A0gebFOcGki6WdJ+keyXt3q7tKukrefvPl3SBpJHt0q6Sfi7pz5LmF8pKa0dJEyXdnZf5kdT/h3vXifXkvA/Mk3SppA0K02q2V73jQr1tUlashWlfkxSSxubxlrZrv0TEkHsBw4AHga2AtYG5wPYDHMMmwK55eH3gj8D2wPeBY3P5scD38vCHgKsAAbsBt+XyDYGH8t8xeXhMk2L+KvCfwJV5/CLggDx8BvD5PHwUcEYePgC4MA9vn9t6HWDLvA2GNSHOc0mPsSZv3w3asV2BNwMPA28otOch7dKuwF7ArsD8Qllp7QjMzvMqL/u3Jcf6AWB4Hv5eIdaa7UUvx4V626SsWHP55sDVwEJgbDu0a7/WbyDfrF1ewO7A1YXx44DjWhzT5cD7Sf/hv0ku24T0D5oAZwIHFuZfkKcfCJxZKO8xX4nxbQb8N/A+4Mq8wz5V+NCubNP8wdg9Dw/P86m6nYvzlRjnaNKBWlXlbdeupKSyKB8Yhud2/WA7tSswgZ4H6lLaMU+7r1DeY74yYq2a9nHg/Dxcs72oc1zobV8vM1bgYmAn0lNxK0ml5e26uq+h2v1V+TBXPJrLWiJ3Y+wC3AZsHBFL8qTHgY3zcL2YB2pdfggcA7yWxzcCno2IV2q878qY8vTn8vwDEeuWwJPA2UpddTMkjaIN2zUiHgN+APwJWEJqp27as10rymrHN+fh6vJmOYz0rZ0+YqpV3tu+XgpJ+wKPRcTcqknt3q6rGKpJpW1IWg/4DfDliHi+OC3SV42W/+Zb6fHNf46I7lbH0oDhpK6Fn0bELsALpG6aldqoXccA+5IS4abAKGCflga1GtqlHfsi6dvAK8D5rY6lFknrAt8CvtPqWMowVJPKY6T+y4rNctmAkjSClFDOj4hLcvETkjbJ0zcB/pzL68U8EOuyB/BRSY8AvyJ1gZ0GbCCpcv+44vuujClPHw0sHaBYHwUejYjb8vjFpCTTju26N/BwRDwZES8Dl5Dauh3btaKsdnwsDzc1ZkmHAH8HHJSTYH9iXUr9bVKGrUlfLObmz9hmwB2S3tSPWAekXXs1kH1t7fIifZt9iLQhKxfkdhjgGAT8AvhhVfnJ9LwQ+v08/GF6XrCbncs3JF1DGJNfDwMbNjHuybx+of7X9Lx4eVQe/kd6XlC+KA/vQM8LpA/RnAv1NwJ/nYen5TZtu3YF3gXcA6yb3/9c4Ivt1K6sek2ltHZk1QvKHyo51n2APwDjquar2V70clyot03KirVq2iO8fk2l5e262us2kG/WTi/Sryr+SPq1x7db8P6TSF0H84C78utDpP7b/wbuB64r7CgCfpLjvRvoKtR1GPBAfh3a5Lgn83pS2SrvwA/kD906uXxkHn8gT9+qsPy38zosoEm/SgF2Bubktr0sf+jasl2B44H7gPnAL/OBri3aFbiAdK3nZdIZ4OFltiPQldf7QeB0qn5cUUKsD5CuO1Q+X2f01V7UOS7U2yZlxVo1/RFeTyotbdf+vHybFjMzK81QvaZiZmZN4KRiZmalcVIxM7PSOKmYmVlpnFTMzKw0TirWUfIdXE8pjH9d0rSS6j5H0v5l1NXH+3xC6e7J1zf7vVpB0n8V7whsQ4uTinWaFcB+lVuDt4vCf1s34nDgyIh4b7Pi6YukYb2Nr4mI+FBEPFtWfdZZnFSs07xCelb3V6onVJ9pSFqe/06WNEvS5ZIeknSSpIMkzc7Pndi6UM3ekuZI+mO+51nlOTInS7o9P9PiHwr13ijpCtJ/blfHc2Cuf76k7+Wy75D+8fUsSSdXza/8PvPzclMK076Zy+ZKOimXvUXSdbnsDklb55iuLCx3er5VCZIekfQ9SXcAn6gx/gFJt+S6fp3vS1dZ7iVNOPoAAAN4SURBVPhcfrekbXP5epLOzmXzJP19Yf7K80AOzu18l6Qzc1sOy9uqsp6rbEvrXKvz7cqsXfwEmCfp+6uxzE7AdsDTpFtxzIiIdyo9HO2LwJfzfBOAd5Lux3S9pLcAnwGei4h3SFoHuEnSNXn+XYEdI+Lh4ptJ2pT0DI+JwDPANZI+FhH/Iul9wNcjYk5VjPuR7gawEzAWuF3SDblsX+BdEfGipA3z/OcDJ0XEpZJGkr4kbk7vlkbErjnGkyrjOQlcAuwdES9I+ibp+Tn/kpd7Ks93FPB14Ajgn3O7vC3XN6aqDbYDpgB7RMTLkv4DOIh0a5o3R8SOeT53lQ0iTirWcSLieUm/AI4G/tLgYrdHvmW7pAeBSlK4Gyh2Q10UEa8B90t6CNiW9LCntxfOgkYDbwVeIt2LqUdCyd4BzIyIJ/N7nk96ONNlvcQ4CbggIl4l3bhxVq7nPcDZEfFiXv+nJa1POjBfmsv+f36fvtrhwjrju5EeXnVTrmNt4JbCfJUbnnaTkh+kG2IeUJkhIp6pqvtvSEn19lznG0g3oPwtsJWkHwO/4/VtYYOAk4p1qh8CdwBnF8peIXfpSlqLdGCsWFEYfq0w/ho9PwfV9y0K0v2XvhgRVxcnSJpMurV+O1nZBtnIqunV8VbGBVwbEQfWqbfSXq/S+HFDwLkRcdwqE6SdSA8k+xzwSdJ9rGwQ8DUV60gR8TTpEa+HF4ofIX0zBvgoMKIfVX9C0lr5OstWpBsOXg18XulRBUjaRunBX72ZDbxH0th8EfxAYFYfy9wITMnXHMaRzmxmA9cChyo9dwNJG0bEMuBRSR/LZevk6QuB7fP4BqSzhUbcCuyRu/uQNErSNn0scy3pzsnkZcZUTf9vYH9Jf1WJW9IWuattrYj4DfBPpC5EGyScVKyTnUK69lDxM9KBfC7pka/9OYv4E+lAfhXwudytNIN0If4OSfNJj27t9dt67mo7FriedAv17oi4vI/3vpR0Z+W5wP8Ax0TE4xHxe+AKYI6ku0jXNAA+DRwtaR5wM/CmiFhESrbz8987G1np3E13CHBBru8WUtdfb04AxuQL7nPp2Y1IRPyBlDSuyXVeS3rc7ZuBmXldziM9stcGCd+l2MzMSuMzFTMzK42TipmZlcZJxczMSuOkYmZmpXFSMTOz0jipmJlZaZxUzMysNP8Hf1CHqLNEJ5wAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Sentences with the higher and lower values of the loss**"
      ],
      "metadata": {
        "id": "PDHdrrM6cZed"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Words with the lowest frequency still predicted**"
      ],
      "metadata": {
        "id": "3aFjePR8ikcW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we want to observe what is the minimum frequency with which a word in the training set is able to be predicted by the neural network. Output 8 shows that words that appear less than 8 times are not even considered."
      ],
      "metadata": {
        "id": "fOVUKtxDdoI7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize count to zero\n",
        "count = 0\n",
        "\n",
        "# Create an empty dictionary to store the frequency of words in train_words\n",
        "train_freq_words = {}\n",
        "\n",
        "# Iterate through the train_words\n",
        "for key in train_words:\n",
        "\n",
        "    # Count the number of occurrences of key in train_words_list\n",
        "    count = train_words_list.count(key)\n",
        "\n",
        "    # Add the word and its frequency to the dictionary\n",
        "    train_freq_words[vocab.word2id[key]] = count\n",
        "\n",
        "# Initialize min_occur to a large number\n",
        "min_occur = 1e10\n",
        "\n",
        "# Iterate through the predict_words\n",
        "for word in predict_words:\n",
        "\n",
        "    # Get the number of occurrences of the word in train_words\n",
        "    occur = train_freq_words[word]\n",
        "\n",
        "    # Update min_occur if the current word has fewer occurrences than the current min_occur\n",
        "    if occur < min_occur:\n",
        "        min_occur = occur\n",
        "\n",
        "# Print the minimum number of occurrences\n",
        "print(min_occur)"
      ],
      "metadata": {
        "id": "K2wCNJLI3Sly",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a42b51b-b4c4-487b-ea6d-8ccb98e96a3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Sentences with the lowest values of the loss**"
      ],
      "metadata": {
        "id": "l6l5_JNGGgkQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code prints the first 5 examples with the lowest loss.\n",
        "\n",
        "It iterates through the range of 5, for each example it prints the loss, the first word of the input data, the target labels, and the predicted labels. It also uses the vocab.id2word[value] to display the actual words from the vocabulary instead of their index values."
      ],
      "metadata": {
        "id": "HdcpBuq3cY-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort the test_stat by loss\n",
        "test_stat = sorted(test_stat, key = lambda x: x['loss'], reverse=True)\n",
        "sorted_train_freq_words = sorted(train_freq_words.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "\n",
        "# Print the first 5 examples with the Highest loss\n",
        "for i in range(5):\n",
        "    \"\"\"\n",
        "    Print the first 5 examples with the Highest loss\n",
        "    \n",
        "    Parameters:\n",
        "        test_stat (list): a list of dictionaries containing information about the predictions\n",
        "    \n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "   \n",
        "    print('Loss: {:3f}'.format(test_stat[i]['loss']))\n",
        "    print('Sent: ' + vocab.id2word[test_stat[i]['1stw']], end=' ')\n",
        "    for value in test_stat[i]['target']:\n",
        "        print(vocab.id2word[value], end=' ')\n",
        "    print('\\nTarget :', end=' ')\n",
        "    for value in test_stat[i]['target']:\n",
        "        print(vocab.id2word[value], end=' ')\n",
        "    print('\\nPredict:', end=' ')\n",
        "    for value in test_stat[i]['predict']:\n",
        "        print(vocab.id2word[value], end=' ')\n",
        "    for value in test_stat[i]['target']:\n",
        "        print('\\n word: '+ '{:<20}'.format(vocab.id2word[value]), end='')\n",
        "        print('{:>30}'.format('frequency in training set:' ), end='')\n",
        "        searched_word = [(key, val) for key, val in sorted_train_freq_words if key == value]\n",
        "        print(' {:<7}'.format(searched_word[0][1]), end =' ')\n",
        "        print('\\trank: {:d} / {:d}'.format(sorted_train_freq_words.index(searched_word[0]), vocab_size), end='')\n",
        "    \n",
        "    print('\\n***********************************************************************************')"
      ],
      "metadata": {
        "id": "injtMlgOprmY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69c6a48b-f5a9-488e-dc76-306b228253f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 9.236802\n",
            "Sent: as commonly understood service implies sacrifice <eos> \n",
            "Target : commonly understood service implies sacrifice <eos> \n",
            "Predict: a as the the the to \n",
            " word: commonly                frequency in training set: 8       \trank: 7240 / 10001\n",
            " word: understood              frequency in training set: 16      \trank: 4824 / 10001\n",
            " word: service                 frequency in training set: 309     \trank: 336 / 10001\n",
            " word: implies                 frequency in training set: 11      \trank: 5909 / 10001\n",
            " word: sacrifice               frequency in training set: 7       \trank: 8064 / 10001\n",
            " word: <eos>                   frequency in training set: 42068   \trank: 2 / 10001\n",
            "***********************************************************************************\n",
            "Loss: 8.834627\n",
            "Sent: fcc counsel joins firm <eos> \n",
            "Target : counsel joins firm <eos> \n",
            "Predict: officials <unk> the said \n",
            " word: counsel                 frequency in training set: 35      \trank: 2592 / 10001\n",
            " word: joins                   frequency in training set: 8       \trank: 7550 / 10001\n",
            " word: firm                    frequency in training set: 479     \trank: 191 / 10001\n",
            " word: <eos>                   frequency in training set: 42068   \trank: 2 / 10001\n",
            "***********************************************************************************\n",
            "Loss: 8.781332\n",
            "Sent: some say november <eos> \n",
            "Target : say november <eos> \n",
            "Predict: of they N \n",
            " word: say                     frequency in training set: 755     \trank: 117 / 10001\n",
            " word: november                frequency in training set: 71      \trank: 1449 / 10001\n",
            " word: <eos>                   frequency in training set: 42068   \trank: 2 / 10001\n",
            "***********************************************************************************\n",
            "Loss: 8.451649\n",
            "Sent: replied a justin salesman exactly <eos> \n",
            "Target : a justin salesman exactly <eos> \n",
            "Predict: the <unk> <unk> who the \n",
            " word: a                       frequency in training set: 21196   \trank: 6 / 10001\n",
            " word: justin                  frequency in training set: 2       \trank: 9965 / 10001\n",
            " word: salesman                frequency in training set: 15      \trank: 4913 / 10001\n",
            " word: exactly                 frequency in training set: 34      \trank: 2665 / 10001\n",
            " word: <eos>                   frequency in training set: 42068   \trank: 2 / 10001\n",
            "***********************************************************************************\n",
            "Loss: 8.150640\n",
            "Sent: what plunge <eos> \n",
            "Target : plunge <eos> \n",
            "Predict: 's is \n",
            " word: plunge                  frequency in training set: 86      \trank: 1225 / 10001\n",
            " word: <eos>                   frequency in training set: 42068   \trank: 2 / 10001\n",
            "***********************************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Sentences with the highest values of the loss**"
      ],
      "metadata": {
        "id": "TcAGf0T9GpK1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code works as the one above yet for the sentences with the highest value of the loss."
      ],
      "metadata": {
        "id": "sTxrmOy1JDE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort the test_stat by loss\n",
        "test_stat = sorted(test_stat, key = lambda x: x['loss'], )\n",
        "\n",
        "# Print the first 5 examples with the lowest loss\n",
        "for i in range(5):\n",
        "    \n",
        "    \"\"\"\n",
        "    Print the first 5 examples with the lowest loss\n",
        "    \n",
        "    Parameters:\n",
        "        test_stat (list): a list of dictionaries containing information about the predictions\n",
        "    \n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    \n",
        "    print('Loss: {:3f}'.format(test_stat[i]['loss']))\n",
        "    print('Sent: ' + vocab.id2word[test_stat[i]['1stw']], end=' ')\n",
        "    for value in test_stat[i]['target']:\n",
        "        print(vocab.id2word[value], end=' ')\n",
        "    print('\\nTarget :', end=' ')\n",
        "    for value in test_stat[i]['target']:\n",
        "        print(vocab.id2word[value], end=' ')\n",
        "    print('\\nPredict:', end=' ')\n",
        "    for value in test_stat[i]['predict']:\n",
        "        print(vocab.id2word[value], end=' ')\n",
        "    for value in test_stat[i]['target']:\n",
        "        print('\\n word: '+ '{:<20}'.format(vocab.id2word[value]), end='')\n",
        "        print('{:>30}'.format('frequency in training set:' ), end='')\n",
        "        searched_word = [(key, val) for key, val in sorted_train_freq_words if key == value]\n",
        "        print(' {:<7}'.format(searched_word[0][1]), end =' ')\n",
        "        print('\\trank: {:d} / {:d}'.format(sorted_train_freq_words.index(searched_word[0]), vocab_size), end='')\n",
        "    \n",
        "    print('\\n***********************************************************************************')"
      ],
      "metadata": {
        "id": "9UH2PdWz2Z8B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdc4bbdc-714e-4151-a57e-bb250edc7738"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.167244\n",
            "Sent: revenue rose N N to $ N billion from $ N billion <eos> \n",
            "Target : rose N N to $ N billion from $ N billion <eos> \n",
            "Predict: rose N N to $ N billion from $ N billion <eos> \n",
            " word: rose                    frequency in training set: 747     \trank: 119 / 10001\n",
            " word: N                       frequency in training set: 32481   \trank: 3 / 10001\n",
            " word: N                       frequency in training set: 32481   \trank: 3 / 10001\n",
            " word: to                      frequency in training set: 23638   \trank: 5 / 10001\n",
            " word: $                       frequency in training set: 7541    \trank: 12 / 10001\n",
            " word: N                       frequency in training set: 32481   \trank: 3 / 10001\n",
            " word: billion                 frequency in training set: 1881    \trank: 48 / 10001\n",
            " word: from                    frequency in training set: 4724    \trank: 20 / 10001\n",
            " word: $                       frequency in training set: 7541    \trank: 12 / 10001\n",
            " word: N                       frequency in training set: 32481   \trank: 3 / 10001\n",
            " word: billion                 frequency in training set: 1881    \trank: 48 / 10001\n",
            " word: <eos>                   frequency in training set: 42068   \trank: 2 / 10001\n",
            "***********************************************************************************\n",
            "Loss: 0.205894\n",
            "Sent: source telerate systems inc <eos> \n",
            "Target : telerate systems inc <eos> \n",
            "Predict: telerate systems inc <eos> \n",
            " word: telerate                frequency in training set: 47      \trank: 2071 / 10001\n",
            " word: systems                 frequency in training set: 252     \trank: 417 / 10001\n",
            " word: inc                     frequency in training set: 240     \trank: 445 / 10001\n",
            " word: <eos>                   frequency in training set: 42068   \trank: 2 / 10001\n",
            "***********************************************************************************\n",
            "Loss: 0.205894\n",
            "Sent: source telerate systems inc <eos> \n",
            "Target : telerate systems inc <eos> \n",
            "Predict: telerate systems inc <eos> \n",
            " word: telerate                frequency in training set: 47      \trank: 2071 / 10001\n",
            " word: systems                 frequency in training set: 252     \trank: 417 / 10001\n",
            " word: inc                     frequency in training set: 240     \trank: 445 / 10001\n",
            " word: <eos>                   frequency in training set: 42068   \trank: 2 / 10001\n",
            "***********************************************************************************\n",
            "Loss: 0.323741\n",
            "Sent: terms were n't disclosed <eos> \n",
            "Target : were n't disclosed <eos> \n",
            "Predict: were n't disclosed <eos> \n",
            " word: were                    frequency in training set: 2009    \trank: 46 / 10001\n",
            " word: n't                     frequency in training set: 3388    \trank: 32 / 10001\n",
            " word: disclosed               frequency in training set: 125     \trank: 867 / 10001\n",
            " word: <eos>                   frequency in training set: 42068   \trank: 2 / 10001\n",
            "***********************************************************************************\n",
            "Loss: 0.324605\n",
            "Sent: dow jones industrials N off N transportation N off N utilities N off N <eos> \n",
            "Target : jones industrials N off N transportation N off N utilities N off N <eos> \n",
            "Predict: jones industrials N up N transportation N off N utilities N off N <eos> \n",
            " word: jones                   frequency in training set: 233     \trank: 459 / 10001\n",
            " word: industrials             frequency in training set: 35      \trank: 2597 / 10001\n",
            " word: N                       frequency in training set: 32481   \trank: 3 / 10001\n",
            " word: off                     frequency in training set: 545     \trank: 170 / 10001\n",
            " word: N                       frequency in training set: 32481   \trank: 3 / 10001\n",
            " word: transportation          frequency in training set: 129     \trank: 844 / 10001\n",
            " word: N                       frequency in training set: 32481   \trank: 3 / 10001\n",
            " word: off                     frequency in training set: 545     \trank: 170 / 10001\n",
            " word: N                       frequency in training set: 32481   \trank: 3 / 10001\n",
            " word: utilities               frequency in training set: 59      \trank: 1716 / 10001\n",
            " word: N                       frequency in training set: 32481   \trank: 3 / 10001\n",
            " word: off                     frequency in training set: 545     \trank: 170 / 10001\n",
            " word: N                       frequency in training set: 32481   \trank: 3 / 10001\n",
            " word: <eos>                   frequency in training set: 42068   \trank: 2 / 10001\n",
            "***********************************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Correlation between lenght and correctness**"
      ],
      "metadata": {
        "id": "aFNQQijgiHOW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we want to try to relate the length of the predicted sentence to the actual correctness of the prediction. As can be seen, the two variables are inversely correlated (negative correlation index), but since the value is small they do not particularly affect each other.\n"
      ],
      "metadata": {
        "id": "AXfhXVJUdMxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort the test_stat by length\n",
        "test_stat = sorted(test_stat, key = lambda x: x['length'])\n",
        "\n",
        "# Create a dictionary to store the number of correct predictions per length\n",
        "length_correct_map = {}\n",
        "\n",
        "# Iterate through the test_stat and add the correct predictions to the dictionary\n",
        "for field in test_stat:\n",
        "    length = field['length']\n",
        "    correct = field['correct']\n",
        "    if length not in length_correct_map:\n",
        "        length_correct_map[length] = []\n",
        "    length_correct_map[length].append(correct)\n",
        "\n",
        "# Create lists for lengths and correct predictions\n",
        "lengths = []\n",
        "corrects = []\n",
        "\n",
        "# Iterate through the dictionary and add the values to the lists\n",
        "for length, correct_list in length_correct_map.items():\n",
        "    correct = sum(correct_list)\n",
        "    lengths.append(length)\n",
        "    corrects.append(correct)\n",
        "\n",
        "# Plot the lengths and correct predictions\n",
        "plt.plot(lengths, corrects)\n",
        "plt.xlabel('Length')\n",
        "plt.ylabel('Correct')\n",
        "plt.title('Length vs Correct')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qAqEndYMko-5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "d55a0263-41b2-4806-ceed-f8f4fd390a00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Correlation between lenght and correctness**"
      ],
      "metadata": {
        "id": "YWqxU2oriN3j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here the purpose is actually the same one of the code just described, yet in this case ww do not care that the word is located in the right position within the sentence. We only observe whether the word is actually contained in it."
      ],
      "metadata": {
        "id": "AkfWKnTjdV1R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort the test_stat by length\n",
        "test_stat = sorted(test_stat, key = lambda x: x['length'])\n",
        "\n",
        "# Create a dictionary to store the number of correct predictions per length\n",
        "length_in_set_map = {}\n",
        "\n",
        "# Iterate through the test_stat and add the in_set to the dictionary\n",
        "for field in test_stat:\n",
        "    length = field['length']\n",
        "    in_set = field['in_set']\n",
        "    if length not in length_in_set_map:\n",
        "        length_in_set_map[length] = []\n",
        "    length_in_set_map[length].append(in_set)\n",
        "\n",
        "# Create lists for lengths and in_set\n",
        "lengths = []\n",
        "in_sets = []\n",
        "\n",
        "# Iterate through the dictionary and add the values to the lists\n",
        "for length, in_set_list in length_in_set_map.items():\n",
        "    in_set = sum(in_set_list)\n",
        "    lengths.append(length)\n",
        "    in_sets.append(in_set)\n",
        "\n",
        "# Plot the lengths and in_set\n",
        "plt.plot(lengths, in_sets)\n",
        "plt.xlabel('Length')\n",
        "plt.ylabel('In_set')\n",
        "plt.title('Length vs In_set')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0MFCaGh3l294",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dc5db29-2028-4dd1-ebc3-d51909c4a362"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Correlation between the lenght and the loss**"
      ],
      "metadata": {
        "id": "mU9NXxp_jWCN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we correlate length with loss. Here the correlation index is positive (direct proportionality) but still has a very small value, which is why it can be inferred that these two variables do not particularly influence each other either."
      ],
      "metadata": {
        "id": "-v9xzCwWmTzm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort the test_stat by length\n",
        "test_stat = sorted(test_stat, key = lambda x: x['length'])\n",
        "\n",
        "# Create a dictionary to store the losses per length\n",
        "length_loss_map = {}\n",
        "\n",
        "# Iterate through the test_stat and add the loss to the dictionary\n",
        "for field in test_stat:\n",
        "    length = field['length']\n",
        "    loss = field['loss']\n",
        "    if length not in length_loss_map:\n",
        "        length_loss_map[length] = []\n",
        "    length_loss_map[length].append(loss)\n",
        "\n",
        "# Create lists for lengths and losses\n",
        "lengths = []\n",
        "losses = []\n",
        "\n",
        "# Iterate through the dictionary and calculate the average loss per length\n",
        "for length, loss_list in length_loss_map.items():\n",
        "    avg_loss = sum(loss_list) / len(loss_list)\n",
        "    lengths.append(length)\n",
        "    losses.append(avg_loss)\n",
        "    \n",
        "# Calculate the correlation coefficient \n",
        "correlation, _ = pearsonr(lengths, losses)\n",
        "print('Correlation ratio: ', correlation)\n",
        "\n",
        "# Plot the scatterplot of lengths and losses\n",
        "plt.scatter(lengths, losses)\n",
        "plt.xlabel('Length')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Length vs Loss')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jKUMAkhRw6i0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "49e3da3e-4e9c-47f7-9d5b-e117d2a863b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correlation ratio:  0.13489228987756366\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "YWqxU2oriN3j"
      ]
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}